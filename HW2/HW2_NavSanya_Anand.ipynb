{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qhbmTM6y1ElA"},"outputs":[],"source":["# ! pip install gensim"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fblJTZMnuZPG"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8236,"status":"ok","timestamp":1706664408388,"user":{"displayName":"Nav Sanya Anand","userId":"15108121630411136107"},"user_tz":480},"id":"7tXqdghCuk9J","outputId":"1367c069-7d88-47d5-ca9b-cac5afb34d34"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\navsa\\AppData\\Local\\Temp\\ipykernel_16584\\3298560112.py:1: DeprecationWarning: \n","Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n","(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n","but was not found to be installed on your system.\n","If this would cause problems for you,\n","please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n","        \n","  import pandas as pd\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import nltk\n","import re\n","from bs4 import BeautifulSoup\n","import os\n","import contractions\n","\n","# os.chdir('/content/drive/Shared drives/USC_CSCI544-Applied NLP/HWs/HW2') # where the files for this project are"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NJAtqst9upvr"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.linear_model import Perceptron, LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.svm import LinearSVC\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","import gensim\n","import gensim.downloader as api\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2005,"status":"ok","timestamp":1706664411777,"user":{"displayName":"Nav Sanya Anand","userId":"15108121630411136107"},"user_tz":480},"id":"8riaDzx6usdP","outputId":"4e77442a-402d-4da7-bcb3-c4516db8e873"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\navsa\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\navsa\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\navsa\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to\n","[nltk_data]     C:\\Users\\navsa\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     C:\\Users\\navsa\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]},{"data":{"text/plain":["True"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('wordnet')\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('omw-1.4')\n","nltk.download('averaged_perceptron_tagger')"]},{"cell_type":"markdown","metadata":{"id":"bfNdjSaTuyYP"},"source":["# **READ DATA**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41562,"status":"ok","timestamp":1706664453332,"user":{"displayName":"Nav Sanya Anand","userId":"15108121630411136107"},"user_tz":480},"id":"ok2MrfiguuwS","outputId":"af19f44e-ec1a-48fd-fe71-1ce020044d14"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\navsa\\AppData\\Local\\Temp\\ipykernel_16584\\1375649650.py:1: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df=pd.read_table('amazon_reviews_us_Office_Products_v1_00.tsv', on_bad_lines='skip')\n"]},{"name":"stdout","output_type":"stream","text":["                                               review_body  star_rating\n","0                                           Great product.          5.0\n","1        What's to say about this commodity item except...          5.0\n","2          Haven't used yet, but I am sure I will like it.          5.0\n","3        Although this was labeled as &#34;new&#34; the...          1.0\n","4                          Gorgeous colors and easy to use          4.0\n","...                                                    ...          ...\n","2640249  I can't live anymore whithout my Palm III. But...          4.0\n","2640250  Although the Palm Pilot is thin and compact it...          4.0\n","2640251  This book had a lot of great content without b...          4.0\n","2640252  I am teaching a course in Excel and am using t...          5.0\n","2640253  A very comprehensive layout of exactly how Vis...          5.0\n","\n","[2640080 rows x 2 columns]\n"]}],"source":["df=pd.read_table('amazon_reviews_us_Office_Products_v1_00.tsv', on_bad_lines='skip')\n","\n","df = df[['review_body', 'star_rating']]\n","\n","\n","df['star_rating'] = pd.to_numeric(df['star_rating'], errors='coerce')\n","\n","df = df.dropna(subset=['review_body'], how='all')\n","\n","print(df)\n"]},{"cell_type":"markdown","metadata":{"id":"3ndbN9pWztag"},"source":["# **CREATE BALANCED DATASET**"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":863,"status":"ok","timestamp":1706664454164,"user":{"displayName":"Nav Sanya Anand","userId":"15108121630411136107"},"user_tz":480},"id":"tBxdF2rbwtMM","outputId":"7ae10fb6-4ecb-4d3e-f7f8-d47988702f28"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                              review_body  star_rating\n","0       It was good at first. I loved how it could pri...          1.0\n","1       Did not work on my Samsung ML-2955ND. I will h...          1.0\n","2       I installed and uninstalled over and over and ...          1.0\n","3       Sometimes, you learn the hard way.<br /><br />...          1.0\n","4       I bought this in 2011 of october and as of Aug...          1.0\n","...                                                   ...          ...\n","249995  I bought these originally to hold overflow tra...          5.0\n","249996  Very good buy so far.  This is the first time ...          5.0\n","249997  This is a really good looking calendar. The ph...          5.0\n","249998  These are super cool! When I was a kid, we use...          5.0\n","249999  THIS PRODUCT IS AWESOME! PERFECT FOR YOUR CAR ...          5.0\n","\n","[250000 rows x 2 columns]\n"]}],"source":["def create_balanced_dataset(df, samples_per_rating=50000):\n","    balanced_df = pd.DataFrame()\n","    for rating in range(1, 6):\n","        rating_df = df[df['star_rating'] == rating].sample(samples_per_rating)\n","        balanced_df = pd.concat([balanced_df, rating_df], ignore_index=True)\n","    return balanced_df\n","\n","df = create_balanced_dataset(df)\n","\n","print(df)"]},{"cell_type":"markdown","metadata":{"id":"-JECD28uz0rE"},"source":["# **CREATE TERNARY LABELS**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":226,"status":"ok","timestamp":1706664454382,"user":{"displayName":"Nav Sanya Anand","userId":"15108121630411136107"},"user_tz":480},"id":"zxxdw66VzhwO","outputId":"075f9a65-9433-49f7-b338-0f9ca0f2bf82"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of positive reviews: 100000\n","Number of negative reviews: 100000\n","Number of neutral reviews: 50000\n","                                              review_body  star_rating  \\\n","0       It was good at first. I loved how it could pri...          1.0   \n","1       Did not work on my Samsung ML-2955ND. I will h...          1.0   \n","2       I installed and uninstalled over and over and ...          1.0   \n","3       Sometimes, you learn the hard way.<br /><br />...          1.0   \n","4       I bought this in 2011 of october and as of Aug...          1.0   \n","...                                                   ...          ...   \n","249995  I bought these originally to hold overflow tra...          5.0   \n","249996  Very good buy so far.  This is the first time ...          5.0   \n","249997  This is a really good looking calendar. The ph...          5.0   \n","249998  These are super cool! When I was a kid, we use...          5.0   \n","249999  THIS PRODUCT IS AWESOME! PERFECT FOR YOUR CAR ...          5.0   \n","\n","        sentiment  \n","0               2  \n","1               2  \n","2               2  \n","3               2  \n","4               2  \n","...           ...  \n","249995          1  \n","249996          1  \n","249997          1  \n","249998          1  \n","249999          1  \n","\n","[250000 rows x 3 columns]\n"]}],"source":["df['sentiment'] = np.where(df['star_rating'] <= 2, 2,  # Negative: 2\n","                          np.where(df['star_rating'] > 3, 1, 3))  # Positive: 1, Neutral: 3\n","\n","# df['sentiment'] = np.where(df['star_rating'] > 3, 1, 0)  # Positive: 1, Negative: 0\n","\n","# Print review counts per class\n","print(\"Number of positive reviews:\", df[df['sentiment'] == 1].shape[0])\n","print(\"Number of negative reviews:\", df[df['sentiment'] == 2].shape[0])\n","print(\"Number of neutral reviews:\", df[df['sentiment'] == 3].shape[0])\n","\n","print(df)"]},{"cell_type":"markdown","metadata":{"id":"KumPmpNJbzSH"},"source":["# **CREATE BINARY LABELS**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LmKTnNAs17OP","outputId":"7ac5691e-66bc-4bef-ce51-af7470fbef69"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\navsa\\AppData\\Local\\Temp\\ipykernel_16584\\668557797.py:9: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n","  soup = BeautifulSoup(text, 'html.parser')\n","C:\\Users\\navsa\\AppData\\Local\\Temp\\ipykernel_16584\\668557797.py:9: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n","  soup = BeautifulSoup(text, 'html.parser')\n"]},{"name":"stdout","output_type":"stream","text":["                                              review_body  star_rating  \\\n","0       It was good at first. I loved how it could pri...          1.0   \n","1       Did not work on my Samsung ML-2955ND. I will h...          1.0   \n","2       I installed and uninstalled over and over and ...          1.0   \n","3       Sometimes, you learn the hard way.<br /><br />...          1.0   \n","4       I bought this in 2011 of october and as of Aug...          1.0   \n","...                                                   ...          ...   \n","249995  I bought these originally to hold overflow tra...          5.0   \n","249996  Very good buy so far.  This is the first time ...          5.0   \n","249997  This is a really good looking calendar. The ph...          5.0   \n","249998  These are super cool! When I was a kid, we use...          5.0   \n","249999  THIS PRODUCT IS AWESOME! PERFECT FOR YOUR CAR ...          5.0   \n","\n","        sentiment                                       clean_review  \n","0               2  good first loved could print wirelessly le mon...  \n","1               2  work samsung mlnd paying higer price original ...  \n","2               2  installed uninstalled tv would work heard nois...  \n","3               2  sometimes learn hard wayi tried cool toner car...  \n","4               2  bought october august th opened box replace in...  \n","...           ...                                                ...  \n","249995          1  bought originally hold overflow trash baby sho...  \n","249996          1  good buy far first time replaced cordless phon...  \n","249997          1  really good looking calendar photo awesome wou...  \n","249998          1  super cool kid use paint holiday theme grandmo...  \n","249999          1  product awesome perfect car glovebox give gift...  \n","\n","[250000 rows x 4 columns]\n","Average length of cleaned reviews(TERTIARY): 201.449944\n"]}],"source":["# Data cleaning and preprocessing\n","stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","\n","# Data Cleaning\n","def clean_text(text):\n","    # Remove HTML tags\n","    if isinstance(text, str) and text:\n","      soup = BeautifulSoup(text, 'html.parser')\n","      text = soup.get_text()\n","\n","      # Expand contractions\n","      text = contractions.fix(text)\n","\n","      # Remove special characters and digits\n","      text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","\n","      # Remove URLs\n","      text = re.sub(r'https?://\\S+', '', text)\n","\n","      # Convert to lowercase\n","      text = text.lower()\n","\n","      # Tokenize\n","      words = nltk.word_tokenize(text)\n","\n","      # Remove stop words\n","      stop_words = set(nltk.corpus.stopwords.words('english'))\n","      words = [word for word in words if word not in stop_words]\n","\n","      # Lemmatize\n","      lemmatizer = WordNetLemmatizer()\n","      words = [lemmatizer.lemmatize(word) for word in words]\n","\n","      # Remove extra spaces\n","      text = ' '.join(words)\n","      text = re.sub(r'\\s+', ' ', text).strip()  # Replace multiple spaces with single space\n","\n","      return text\n","    else:\n","      return ''  # Return an empty string for empty or non-string inputs\n","\n","df['clean_review'] = df['review_body'].apply(clean_text)\n","# Print the average length of the cleaned reviews\n","print(df)\n","print(\"Average length of cleaned reviews(TERTIARY):\", df['clean_review'].str.len().mean())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cp0ceAXTbzSK","outputId":"a525fdbb-14a1-4ff1-e26b-4bbc6cb18e79"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of positive reviews: 100000\n","Number of negative reviews: 100000\n","Number of neutral reviews (discarded): 0\n","                                              review_body  star_rating  \\\n","0       It was good at first. I loved how it could pri...          1.0   \n","1       Did not work on my Samsung ML-2955ND. I will h...          1.0   \n","2       I installed and uninstalled over and over and ...          1.0   \n","3       Sometimes, you learn the hard way.<br /><br />...          1.0   \n","4       I bought this in 2011 of october and as of Aug...          1.0   \n","...                                                   ...          ...   \n","249995  I bought these originally to hold overflow tra...          5.0   \n","249996  Very good buy so far.  This is the first time ...          5.0   \n","249997  This is a really good looking calendar. The ph...          5.0   \n","249998  These are super cool! When I was a kid, we use...          5.0   \n","249999  THIS PRODUCT IS AWESOME! PERFECT FOR YOUR CAR ...          5.0   \n","\n","        sentiment                                       clean_review  \n","0               2  good first loved could print wirelessly le mon...  \n","1               2  work samsung mlnd paying higer price original ...  \n","2               2  installed uninstalled tv would work heard nois...  \n","3               2  sometimes learn hard wayi tried cool toner car...  \n","4               2  bought october august th opened box replace in...  \n","...           ...                                                ...  \n","249995          1  bought originally hold overflow trash baby sho...  \n","249996          1  good buy far first time replaced cordless phon...  \n","249997          1  really good looking calendar photo awesome wou...  \n","249998          1  super cool kid use paint holiday theme grandmo...  \n","249999          1  product awesome perfect car glovebox give gift...  \n","\n","[200000 rows x 4 columns]\n","Average length of cleaned reviews(BINARY): 198.71364\n"]}],"source":["\n","df_binary = df[df['star_rating'] != 3]  # Discard neutral reviews (rating 3)\n","\n","# Print review counts per class\n","print(\"Number of positive reviews:\", df_binary[df_binary['sentiment'] == 1].shape[0])\n","print(\"Number of negative reviews:\", df_binary[df_binary['sentiment'] == 2].shape[0])\n","print(\"Number of neutral reviews (discarded):\", len(df_binary[df_binary['star_rating'] == 3]))\n","print(df_binary)\n","print(\"Average length of cleaned reviews(BINARY):\", df_binary['clean_review'].str.len().mean())\n"]},{"cell_type":"markdown","metadata":{"id":"icfzjRb_z2FG"},"source":["# **SPLIT DATASET**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"41yz-5itzoiM"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(df['review_body'], df['sentiment'], test_size=0.2, random_state=42)\n","X_train_binary, X_test_binary, y_train_binary, y_test_binary = train_test_split(df_binary['review_body'], df_binary['sentiment'], test_size=0.2, random_state=42)\n"]},{"cell_type":"markdown","metadata":{"id":"2gFScquUbzSM"},"source":[]},{"cell_type":"markdown","metadata":{"id":"SZaktxgg0jdG"},"source":["# **WORD EMBEDDING**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VWnESNdtbzSN"},"outputs":[],"source":["import logging\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TbiPKUzP0ia9","outputId":"8e28e62a-ef19-443f-d89e-3b09845ee9b9"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-02-08 07:04:27,480 : INFO : loading projection weights from C:\\Users\\navsa/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n"]},{"name":"stderr","output_type":"stream","text":["2024-02-08 07:05:23,835 : INFO : KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from C:\\\\Users\\\\navsa/gensim-data\\\\word2vec-google-news-300\\\\word2vec-google-news-300.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-02-08T07:05:23.833849', 'gensim': '4.3.2', 'python': '3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.22621-SP0', 'event': 'load_word2vec_format'}\n"]}],"source":["# Load pretrained model\n","pretrained_model = api.load('word2vec-google-news-300')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yoebi8zVbzSO","outputId":"5a5e9f6f-0939-4d61-b055-4433287d932d"},"outputs":[{"name":"stdout","output_type":"stream","text":["PRETRAINED WORD2VEC\n","Semantic similarity between 'king - man + woman' and 'queen': [('queen', 0.46892058849334717)]\n","Semantic similarity between 'excellent' and 'outstanding': 0.55674857\n"]}],"source":["print(\"PRETRAINED WORD2VEC\")\n","king_minus_man_plus_woman = pretrained_model.most_similar(positive=['king', 'woman'], negative=['man', 'men'], topn=1)\n","excellent_similar_outstanding = pretrained_model.similarity('excellent', 'outstanding')\n","\n","print(\"Semantic similarity between 'king - man + woman' and 'queen':\", king_minus_man_plus_woman)\n","print(\"Semantic similarity between 'excellent' and 'outstanding':\", excellent_similar_outstanding)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JL8pFXNnbzSO","outputId":"5d08128e-8fe5-42f8-e6eb-afa269c5d4f0"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-02-08 07:09:17,610 : INFO : collecting all words and their counts\n","2024-02-08 07:09:17,611 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n","2024-02-08 07:09:17,914 : INFO : PROGRESS: at sentence #10000, processed 732015 words, keeping 26253 word types\n","2024-02-08 07:09:18,155 : INFO : PROGRESS: at sentence #20000, processed 1470902 words, keeping 38849 word types\n","2024-02-08 07:09:18,474 : INFO : PROGRESS: at sentence #30000, processed 2247079 words, keeping 49956 word types\n","2024-02-08 07:09:18,711 : INFO : PROGRESS: at sentence #40000, processed 2996889 words, keeping 58828 word types\n","2024-02-08 07:09:18,992 : INFO : PROGRESS: at sentence #50000, processed 3762337 words, keeping 66976 word types\n","2024-02-08 07:09:19,213 : INFO : PROGRESS: at sentence #60000, processed 4495574 words, keeping 74047 word types\n","2024-02-08 07:09:19,506 : INFO : PROGRESS: at sentence #70000, processed 5250328 words, keeping 80859 word types\n","2024-02-08 07:09:19,859 : INFO : PROGRESS: at sentence #80000, processed 6002297 words, keeping 87463 word types\n","2024-02-08 07:09:20,117 : INFO : PROGRESS: at sentence #90000, processed 6721724 words, keeping 93477 word types\n","2024-02-08 07:09:20,375 : INFO : PROGRESS: at sentence #100000, processed 7464120 words, keeping 99688 word types\n","2024-02-08 07:09:20,596 : INFO : PROGRESS: at sentence #110000, processed 8219026 words, keeping 105433 word types\n","2024-02-08 07:09:20,830 : INFO : PROGRESS: at sentence #120000, processed 8949845 words, keeping 110865 word types\n","2024-02-08 07:09:21,166 : INFO : PROGRESS: at sentence #130000, processed 9693717 words, keeping 116474 word types\n","2024-02-08 07:09:21,630 : INFO : PROGRESS: at sentence #140000, processed 10453626 words, keeping 121676 word types\n","2024-02-08 07:09:22,087 : INFO : PROGRESS: at sentence #150000, processed 11228092 words, keeping 127049 word types\n","2024-02-08 07:09:22,472 : INFO : PROGRESS: at sentence #160000, processed 11975844 words, keeping 131911 word types\n","2024-02-08 07:09:22,742 : INFO : PROGRESS: at sentence #170000, processed 12718160 words, keeping 136904 word types\n","2024-02-08 07:09:23,033 : INFO : PROGRESS: at sentence #180000, processed 13461095 words, keeping 142097 word types\n","2024-02-08 07:09:23,294 : INFO : PROGRESS: at sentence #190000, processed 14221585 words, keeping 146817 word types\n","2024-02-08 07:09:23,683 : INFO : collected 151301 word types from a corpus of 14949117 raw words and 200000 sentences\n","2024-02-08 07:09:23,684 : INFO : Creating a fresh vocabulary\n","2024-02-08 07:09:23,940 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 21395 unique words (14.14% of original 151301, drops 129906)', 'datetime': '2024-02-08T07:09:23.940596', 'gensim': '4.3.2', 'python': '3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.22621-SP0', 'event': 'prepare_vocab'}\n","2024-02-08 07:09:23,942 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 14704865 word corpus (98.37% of original 14949117, drops 244252)', 'datetime': '2024-02-08T07:09:23.942599', 'gensim': '4.3.2', 'python': '3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.22621-SP0', 'event': 'prepare_vocab'}\n","2024-02-08 07:09:24,243 : INFO : deleting the raw counts dictionary of 151301 items\n","2024-02-08 07:09:24,250 : INFO : sample=0.001 downsamples 54 most-common words\n","2024-02-08 07:09:24,300 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 10549222.36658601 word corpus (71.7%% of prior 14704865)', 'datetime': '2024-02-08T07:09:24.300565', 'gensim': '4.3.2', 'python': '3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.22621-SP0', 'event': 'prepare_vocab'}\n","2024-02-08 07:09:24,786 : INFO : estimated required memory for 21395 words and 300 dimensions: 62045500 bytes\n","2024-02-08 07:09:24,788 : INFO : resetting layer weights\n","2024-02-08 07:09:24,862 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-02-08T07:09:24.862290', 'gensim': '4.3.2', 'python': '3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.22621-SP0', 'event': 'build_vocab'}\n","2024-02-08 07:09:24,864 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 21395 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=11 shrink_windows=True', 'datetime': '2024-02-08T07:09:24.864306', 'gensim': '4.3.2', 'python': '3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.22621-SP0', 'event': 'train'}\n","2024-02-08 07:09:26,106 : INFO : EPOCH 0 - PROGRESS: at 3.19% examples, 332234 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:09:27,123 : INFO : EPOCH 0 - PROGRESS: at 6.29% examples, 323553 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:09:28,145 : INFO : EPOCH 0 - PROGRESS: at 9.59% examples, 327009 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:09:29,151 : INFO : EPOCH 0 - PROGRESS: at 13.84% examples, 360386 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:09:30,159 : INFO : EPOCH 0 - PROGRESS: at 16.38% examples, 343159 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:09:31,161 : INFO : EPOCH 0 - PROGRESS: at 19.77% examples, 345205 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:09:32,179 : INFO : EPOCH 0 - PROGRESS: at 23.21% examples, 348681 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:09:33,187 : INFO : EPOCH 0 - PROGRESS: at 26.66% examples, 349844 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:09:34,215 : INFO : EPOCH 0 - PROGRESS: at 29.99% examples, 348000 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:09:35,240 : INFO : EPOCH 0 - PROGRESS: at 32.80% examples, 342386 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:09:36,261 : INFO : EPOCH 0 - PROGRESS: at 34.96% examples, 331760 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:09:37,318 : INFO : EPOCH 0 - PROGRESS: at 36.06% examples, 312728 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:09:38,334 : INFO : EPOCH 0 - PROGRESS: at 37.29% examples, 298646 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:09:39,363 : INFO : EPOCH 0 - PROGRESS: at 39.42% examples, 292787 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:09:40,364 : INFO : EPOCH 0 - PROGRESS: at 41.98% examples, 290953 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:09:41,393 : INFO : EPOCH 0 - PROGRESS: at 44.76% examples, 289654 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:09:42,403 : INFO : EPOCH 0 - PROGRESS: at 47.58% examples, 290004 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:09:43,417 : INFO : EPOCH 0 - PROGRESS: at 51.48% examples, 296013 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:09:44,424 : INFO : EPOCH 0 - PROGRESS: at 54.84% examples, 299397 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:09:45,435 : INFO : EPOCH 0 - PROGRESS: at 58.72% examples, 304058 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:09:46,469 : INFO : EPOCH 0 - PROGRESS: at 62.76% examples, 309118 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:09:47,471 : INFO : EPOCH 0 - PROGRESS: at 66.36% examples, 312283 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:09:48,496 : INFO : EPOCH 0 - PROGRESS: at 70.10% examples, 315837 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:09:49,512 : INFO : EPOCH 0 - PROGRESS: at 73.23% examples, 316561 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:09:50,528 : INFO : EPOCH 0 - PROGRESS: at 76.34% examples, 317333 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:09:51,558 : INFO : EPOCH 0 - PROGRESS: at 79.65% examples, 318137 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:09:52,569 : INFO : EPOCH 0 - PROGRESS: at 82.51% examples, 316826 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:09:53,582 : INFO : EPOCH 0 - PROGRESS: at 85.65% examples, 317513 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:09:54,609 : INFO : EPOCH 0 - PROGRESS: at 89.08% examples, 318746 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:09:55,669 : INFO : EPOCH 0 - PROGRESS: at 91.13% examples, 314751 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:09:56,702 : INFO : EPOCH 0 - PROGRESS: at 94.31% examples, 315291 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:09:57,735 : INFO : EPOCH 0 - PROGRESS: at 97.32% examples, 314671 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:09:58,750 : INFO : EPOCH 0 - PROGRESS: at 99.80% examples, 312869 words/s, in_qsize 4, out_qsize 0\n","2024-02-08 07:09:58,801 : INFO : EPOCH 0: training on 14949117 raw words (10550167 effective words) took 33.7s, 313078 effective words/s\n","2024-02-08 07:09:59,821 : INFO : EPOCH 1 - PROGRESS: at 2.93% examples, 302236 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:00,854 : INFO : EPOCH 1 - PROGRESS: at 5.08% examples, 258313 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:01,866 : INFO : EPOCH 1 - PROGRESS: at 8.44% examples, 286642 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:02,871 : INFO : EPOCH 1 - PROGRESS: at 10.88% examples, 279239 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:03,879 : INFO : EPOCH 1 - PROGRESS: at 13.31% examples, 276625 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:04,898 : INFO : EPOCH 1 - PROGRESS: at 16.52% examples, 287174 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:05,913 : INFO : EPOCH 1 - PROGRESS: at 19.97% examples, 297327 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:06,916 : INFO : EPOCH 1 - PROGRESS: at 21.52% examples, 281285 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:07,918 : INFO : EPOCH 1 - PROGRESS: at 24.25% examples, 282806 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:10:08,933 : INFO : EPOCH 1 - PROGRESS: at 27.28% examples, 286195 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:09,965 : INFO : EPOCH 1 - PROGRESS: at 30.18% examples, 286021 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:10,969 : INFO : EPOCH 1 - PROGRESS: at 33.98% examples, 295648 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:11,984 : INFO : EPOCH 1 - PROGRESS: at 37.70% examples, 303013 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:13,016 : INFO : EPOCH 1 - PROGRESS: at 41.60% examples, 309513 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:10:14,034 : INFO : EPOCH 1 - PROGRESS: at 45.10% examples, 312175 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:15,035 : INFO : EPOCH 1 - PROGRESS: at 48.31% examples, 313866 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:16,048 : INFO : EPOCH 1 - PROGRESS: at 50.68% examples, 309603 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:17,049 : INFO : EPOCH 1 - PROGRESS: at 53.38% examples, 308620 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:10:18,064 : INFO : EPOCH 1 - PROGRESS: at 56.47% examples, 309090 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:19,074 : INFO : EPOCH 1 - PROGRESS: at 60.32% examples, 313228 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:10:20,108 : INFO : EPOCH 1 - PROGRESS: at 63.87% examples, 315629 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:21,116 : INFO : EPOCH 1 - PROGRESS: at 67.39% examples, 318064 words/s, in_qsize 4, out_qsize 1\n","2024-02-08 07:10:22,141 : INFO : EPOCH 1 - PROGRESS: at 70.70% examples, 319308 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:10:23,183 : INFO : EPOCH 1 - PROGRESS: at 73.17% examples, 316702 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:24,210 : INFO : EPOCH 1 - PROGRESS: at 74.98% examples, 311877 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:25,226 : INFO : EPOCH 1 - PROGRESS: at 77.70% examples, 310696 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:26,250 : INFO : EPOCH 1 - PROGRESS: at 80.41% examples, 309487 words/s, in_qsize 4, out_qsize 1\n","2024-02-08 07:10:27,303 : INFO : EPOCH 1 - PROGRESS: at 83.77% examples, 310270 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:10:28,320 : INFO : EPOCH 1 - PROGRESS: at 86.36% examples, 309027 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:29,346 : INFO : EPOCH 1 - PROGRESS: at 88.91% examples, 307528 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:30,357 : INFO : EPOCH 1 - PROGRESS: at 91.32% examples, 305614 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:31,378 : INFO : EPOCH 1 - PROGRESS: at 94.84% examples, 307603 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:32,393 : INFO : EPOCH 1 - PROGRESS: at 98.73% examples, 310132 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:32,901 : INFO : EPOCH 1: training on 14949117 raw words (10549515 effective words) took 34.1s, 309451 effective words/s\n","2024-02-08 07:10:33,922 : INFO : EPOCH 2 - PROGRESS: at 3.24% examples, 336040 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:34,941 : INFO : EPOCH 2 - PROGRESS: at 6.73% examples, 345806 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:35,951 : INFO : EPOCH 2 - PROGRESS: at 9.91% examples, 338576 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:36,953 : INFO : EPOCH 2 - PROGRESS: at 13.49% examples, 352099 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:37,970 : INFO : EPOCH 2 - PROGRESS: at 16.77% examples, 351223 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:39,012 : INFO : EPOCH 2 - PROGRESS: at 19.57% examples, 339304 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:40,027 : INFO : EPOCH 2 - PROGRESS: at 23.27% examples, 347627 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:41,049 : INFO : EPOCH 2 - PROGRESS: at 25.33% examples, 330326 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:42,074 : INFO : EPOCH 2 - PROGRESS: at 27.04% examples, 313240 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:43,097 : INFO : EPOCH 2 - PROGRESS: at 29.65% examples, 307851 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:44,121 : INFO : EPOCH 2 - PROGRESS: at 32.21% examples, 303944 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:45,127 : INFO : EPOCH 2 - PROGRESS: at 34.71% examples, 300707 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:46,129 : INFO : EPOCH 2 - PROGRESS: at 37.22% examples, 298380 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:47,142 : INFO : EPOCH 2 - PROGRESS: at 40.02% examples, 297731 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:48,163 : INFO : EPOCH 2 - PROGRESS: at 42.94% examples, 297545 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:49,208 : INFO : EPOCH 2 - PROGRESS: at 46.08% examples, 298454 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:50,220 : INFO : EPOCH 2 - PROGRESS: at 48.46% examples, 295093 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:51,242 : INFO : EPOCH 2 - PROGRESS: at 51.16% examples, 293840 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:10:52,328 : INFO : EPOCH 2 - PROGRESS: at 53.52% examples, 290685 words/s, in_qsize 6, out_qsize 1\n","2024-02-08 07:10:53,339 : INFO : EPOCH 2 - PROGRESS: at 55.54% examples, 286560 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:10:54,397 : INFO : EPOCH 2 - PROGRESS: at 57.91% examples, 283516 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:55,410 : INFO : EPOCH 2 - PROGRESS: at 60.32% examples, 282146 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:56,414 : INFO : EPOCH 2 - PROGRESS: at 62.53% examples, 279845 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:57,453 : INFO : EPOCH 2 - PROGRESS: at 65.57% examples, 281263 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:58,472 : INFO : EPOCH 2 - PROGRESS: at 68.03% examples, 280286 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:10:59,475 : INFO : EPOCH 2 - PROGRESS: at 70.50% examples, 279685 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:00,490 : INFO : EPOCH 2 - PROGRESS: at 73.28% examples, 280423 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:01,507 : INFO : EPOCH 2 - PROGRESS: at 76.04% examples, 280949 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:02,537 : INFO : EPOCH 2 - PROGRESS: at 79.24% examples, 282674 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:03,603 : INFO : EPOCH 2 - PROGRESS: at 82.31% examples, 282864 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:04,608 : INFO : EPOCH 2 - PROGRESS: at 84.49% examples, 281528 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:05,617 : INFO : EPOCH 2 - PROGRESS: at 87.35% examples, 281983 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:11:06,628 : INFO : EPOCH 2 - PROGRESS: at 89.83% examples, 281188 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:07,634 : INFO : EPOCH 2 - PROGRESS: at 91.99% examples, 279678 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:08,658 : INFO : EPOCH 2 - PROGRESS: at 95.14% examples, 281160 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:09,687 : INFO : EPOCH 2 - PROGRESS: at 98.29% examples, 282061 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:10,272 : INFO : EPOCH 2: training on 14949117 raw words (10549349 effective words) took 37.4s, 282357 effective words/s\n","2024-02-08 07:11:11,288 : INFO : EPOCH 3 - PROGRESS: at 2.93% examples, 303193 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:12,311 : INFO : EPOCH 3 - PROGRESS: at 5.57% examples, 284269 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:13,311 : INFO : EPOCH 3 - PROGRESS: at 8.51% examples, 291542 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:14,313 : INFO : EPOCH 3 - PROGRESS: at 11.49% examples, 298529 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:15,322 : INFO : EPOCH 3 - PROGRESS: at 14.27% examples, 299028 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:16,335 : INFO : EPOCH 3 - PROGRESS: at 17.03% examples, 298069 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:17,377 : INFO : EPOCH 3 - PROGRESS: at 20.11% examples, 299621 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:18,380 : INFO : EPOCH 3 - PROGRESS: at 22.89% examples, 300476 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:19,408 : INFO : EPOCH 3 - PROGRESS: at 24.57% examples, 286071 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:20,437 : INFO : EPOCH 3 - PROGRESS: at 27.28% examples, 285348 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:21,457 : INFO : EPOCH 3 - PROGRESS: at 30.12% examples, 284955 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:22,466 : INFO : EPOCH 3 - PROGRESS: at 32.34% examples, 280751 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:23,468 : INFO : EPOCH 3 - PROGRESS: at 34.18% examples, 274328 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:24,511 : INFO : EPOCH 3 - PROGRESS: at 36.55% examples, 272234 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:11:25,513 : INFO : EPOCH 3 - PROGRESS: at 39.29% examples, 273176 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:26,544 : INFO : EPOCH 3 - PROGRESS: at 42.20% examples, 274321 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:27,592 : INFO : EPOCH 3 - PROGRESS: at 44.76% examples, 272555 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:28,616 : INFO : EPOCH 3 - PROGRESS: at 47.40% examples, 272513 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:29,637 : INFO : EPOCH 3 - PROGRESS: at 50.68% examples, 275761 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:30,650 : INFO : EPOCH 3 - PROGRESS: at 53.58% examples, 277417 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:31,656 : INFO : EPOCH 3 - PROGRESS: at 55.84% examples, 275498 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:32,659 : INFO : EPOCH 3 - PROGRESS: at 58.67% examples, 275931 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:33,688 : INFO : EPOCH 3 - PROGRESS: at 61.46% examples, 276255 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:34,697 : INFO : EPOCH 3 - PROGRESS: at 64.61% examples, 278481 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:11:35,721 : INFO : EPOCH 3 - PROGRESS: at 67.46% examples, 279145 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:36,740 : INFO : EPOCH 3 - PROGRESS: at 69.58% examples, 277093 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:37,781 : INFO : EPOCH 3 - PROGRESS: at 72.56% examples, 278174 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:38,788 : INFO : EPOCH 3 - PROGRESS: at 75.76% examples, 280801 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:39,790 : INFO : EPOCH 3 - PROGRESS: at 78.88% examples, 282347 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:40,804 : INFO : EPOCH 3 - PROGRESS: at 81.63% examples, 282107 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:41,809 : INFO : EPOCH 3 - PROGRESS: at 84.61% examples, 283469 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:42,835 : INFO : EPOCH 3 - PROGRESS: at 88.00% examples, 285443 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:43,835 : INFO : EPOCH 3 - PROGRESS: at 91.19% examples, 286897 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:44,835 : INFO : EPOCH 3 - PROGRESS: at 94.96% examples, 290298 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:45,868 : INFO : EPOCH 3 - PROGRESS: at 98.79% examples, 292861 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:46,460 : INFO : EPOCH 3: training on 14949117 raw words (10549155 effective words) took 36.2s, 291589 effective words/s\n","2024-02-08 07:11:47,479 : INFO : EPOCH 4 - PROGRESS: at 2.66% examples, 275871 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:48,484 : INFO : EPOCH 4 - PROGRESS: at 4.83% examples, 249029 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:49,497 : INFO : EPOCH 4 - PROGRESS: at 7.60% examples, 262122 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:50,543 : INFO : EPOCH 4 - PROGRESS: at 10.03% examples, 256607 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:51,553 : INFO : EPOCH 4 - PROGRESS: at 12.65% examples, 262808 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:52,598 : INFO : EPOCH 4 - PROGRESS: at 15.06% examples, 259899 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:53,605 : INFO : EPOCH 4 - PROGRESS: at 18.07% examples, 267734 words/s, in_qsize 6, out_qsize 1\n","2024-02-08 07:11:54,635 : INFO : EPOCH 4 - PROGRESS: at 20.90% examples, 270931 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:11:55,637 : INFO : EPOCH 4 - PROGRESS: at 23.46% examples, 272226 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:56,645 : INFO : EPOCH 4 - PROGRESS: at 25.94% examples, 270450 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:57,649 : INFO : EPOCH 4 - PROGRESS: at 27.93% examples, 264989 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:58,661 : INFO : EPOCH 4 - PROGRESS: at 30.50% examples, 264689 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:11:59,669 : INFO : EPOCH 4 - PROGRESS: at 32.80% examples, 262945 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:00,677 : INFO : EPOCH 4 - PROGRESS: at 35.02% examples, 261062 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:01,701 : INFO : EPOCH 4 - PROGRESS: at 37.35% examples, 259866 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:02,707 : INFO : EPOCH 4 - PROGRESS: at 40.49% examples, 263953 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:03,713 : INFO : EPOCH 4 - PROGRESS: at 43.78% examples, 268051 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:04,718 : INFO : EPOCH 4 - PROGRESS: at 47.25% examples, 273030 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:05,748 : INFO : EPOCH 4 - PROGRESS: at 50.16% examples, 274032 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:06,767 : INFO : EPOCH 4 - PROGRESS: at 52.19% examples, 270887 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:07,834 : INFO : EPOCH 4 - PROGRESS: at 54.33% examples, 268139 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:08,859 : INFO : EPOCH 4 - PROGRESS: at 58.11% examples, 273036 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:09,897 : INFO : EPOCH 4 - PROGRESS: at 61.81% examples, 277531 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:10,915 : INFO : EPOCH 4 - PROGRESS: at 65.75% examples, 283220 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:11,922 : INFO : EPOCH 4 - PROGRESS: at 69.49% examples, 287561 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:12,927 : INFO : EPOCH 4 - PROGRESS: at 73.10% examples, 291521 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:13,949 : INFO : EPOCH 4 - PROGRESS: at 76.28% examples, 293362 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:14,967 : INFO : EPOCH 4 - PROGRESS: at 79.31% examples, 294121 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:15,989 : INFO : EPOCH 4 - PROGRESS: at 82.67% examples, 295473 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:16,994 : INFO : EPOCH 4 - PROGRESS: at 85.70% examples, 296453 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:18,012 : INFO : EPOCH 4 - PROGRESS: at 88.53% examples, 296371 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:19,018 : INFO : EPOCH 4 - PROGRESS: at 91.85% examples, 297921 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:20,031 : INFO : EPOCH 4 - PROGRESS: at 94.58% examples, 297654 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:21,036 : INFO : EPOCH 4 - PROGRESS: at 97.04% examples, 296218 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:22,054 : INFO : EPOCH 4 - PROGRESS: at 99.88% examples, 296009 words/s, in_qsize 3, out_qsize 0\n","2024-02-08 07:12:22,079 : INFO : EPOCH 4: training on 14949117 raw words (10547537 effective words) took 35.6s, 296242 effective words/s\n","2024-02-08 07:12:22,081 : INFO : Word2Vec lifecycle event {'msg': 'training on 74745585 raw words (52745723 effective words) took 177.2s, 297636 effective words/s', 'datetime': '2024-02-08T07:12:22.080994', 'gensim': '4.3.2', 'python': '3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.22621-SP0', 'event': 'train'}\n","2024-02-08 07:12:22,082 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=21395, vector_size=300, alpha=0.025>', 'datetime': '2024-02-08T07:12:22.082992', 'gensim': '4.3.2', 'python': '3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.22621-SP0', 'event': 'created'}\n","2024-02-08 07:12:22,087 : INFO : collecting all words and their counts\n","2024-02-08 07:12:22,089 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n","2024-02-08 07:12:22,537 : INFO : PROGRESS: at sentence #10000, processed 730187 words, keeping 26495 word types\n","2024-02-08 07:12:22,911 : INFO : PROGRESS: at sentence #20000, processed 1458200 words, keeping 39085 word types\n","2024-02-08 07:12:23,341 : INFO : PROGRESS: at sentence #30000, processed 2188654 words, keeping 49173 word types\n","2024-02-08 07:12:23,718 : INFO : PROGRESS: at sentence #40000, processed 2923590 words, keeping 58102 word types\n","2024-02-08 07:12:23,986 : INFO : PROGRESS: at sentence #50000, processed 3670238 words, keeping 66378 word types\n","2024-02-08 07:12:24,376 : INFO : PROGRESS: at sentence #60000, processed 4407774 words, keeping 73750 word types\n","2024-02-08 07:12:24,918 : INFO : PROGRESS: at sentence #70000, processed 5128251 words, keeping 80515 word types\n","2024-02-08 07:12:25,307 : INFO : PROGRESS: at sentence #80000, processed 5853368 words, keeping 87113 word types\n","2024-02-08 07:12:25,689 : INFO : PROGRESS: at sentence #90000, processed 6588375 words, keeping 93284 word types\n","2024-02-08 07:12:25,999 : INFO : PROGRESS: at sentence #100000, processed 7322752 words, keeping 99864 word types\n","2024-02-08 07:12:26,275 : INFO : PROGRESS: at sentence #110000, processed 8055248 words, keeping 105543 word types\n","2024-02-08 07:12:26,510 : INFO : PROGRESS: at sentence #120000, processed 8802287 words, keeping 111160 word types\n","2024-02-08 07:12:26,832 : INFO : PROGRESS: at sentence #130000, processed 9520929 words, keeping 116245 word types\n","2024-02-08 07:12:27,062 : INFO : PROGRESS: at sentence #140000, processed 10267682 words, keeping 121801 word types\n","2024-02-08 07:12:27,371 : INFO : PROGRESS: at sentence #150000, processed 11002756 words, keeping 126768 word types\n","2024-02-08 07:12:27,624 : INFO : collected 131601 word types from a corpus of 11732180 raw words and 160000 sentences\n","2024-02-08 07:12:27,625 : INFO : Creating a fresh vocabulary\n","2024-02-08 07:12:27,844 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 18755 unique words (14.25% of original 131601, drops 112846)', 'datetime': '2024-02-08T07:12:27.844629', 'gensim': '4.3.2', 'python': '3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.22621-SP0', 'event': 'prepare_vocab'}\n","2024-02-08 07:12:27,845 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 11518795 word corpus (98.18% of original 11732180, drops 213385)', 'datetime': '2024-02-08T07:12:27.845666', 'gensim': '4.3.2', 'python': '3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.22621-SP0', 'event': 'prepare_vocab'}\n","2024-02-08 07:12:28,033 : INFO : deleting the raw counts dictionary of 131601 items\n","2024-02-08 07:12:28,038 : INFO : sample=0.001 downsamples 55 most-common words\n","2024-02-08 07:12:28,039 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 8269299.884821603 word corpus (71.8%% of prior 11518795)', 'datetime': '2024-02-08T07:12:28.039351', 'gensim': '4.3.2', 'python': '3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.22621-SP0', 'event': 'prepare_vocab'}\n","2024-02-08 07:12:28,355 : INFO : estimated required memory for 18755 words and 300 dimensions: 54389500 bytes\n","2024-02-08 07:12:28,356 : INFO : resetting layer weights\n","2024-02-08 07:12:28,416 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-02-08T07:12:28.416538', 'gensim': '4.3.2', 'python': '3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.22621-SP0', 'event': 'build_vocab'}\n","2024-02-08 07:12:28,417 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 18755 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=11 shrink_windows=True', 'datetime': '2024-02-08T07:12:28.417529', 'gensim': '4.3.2', 'python': '3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.22621-SP0', 'event': 'train'}\n","2024-02-08 07:12:29,480 : INFO : EPOCH 0 - PROGRESS: at 4.32% examples, 357762 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:30,488 : INFO : EPOCH 0 - PROGRESS: at 9.29% examples, 378758 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:31,509 : INFO : EPOCH 0 - PROGRESS: at 13.82% examples, 372626 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:32,524 : INFO : EPOCH 0 - PROGRESS: at 16.72% examples, 339699 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:33,526 : INFO : EPOCH 0 - PROGRESS: at 19.19% examples, 312367 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:34,532 : INFO : EPOCH 0 - PROGRESS: at 23.38% examples, 316829 words/s, in_qsize 6, out_qsize 1\n","2024-02-08 07:12:35,538 : INFO : EPOCH 0 - PROGRESS: at 27.39% examples, 319242 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:36,571 : INFO : EPOCH 0 - PROGRESS: at 30.80% examples, 314624 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:37,591 : INFO : EPOCH 0 - PROGRESS: at 34.76% examples, 315814 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:38,617 : INFO : EPOCH 0 - PROGRESS: at 39.12% examples, 319035 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:39,625 : INFO : EPOCH 0 - PROGRESS: at 43.58% examples, 322893 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:40,676 : INFO : EPOCH 0 - PROGRESS: at 47.60% examples, 321885 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:12:41,707 : INFO : EPOCH 0 - PROGRESS: at 50.72% examples, 316326 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:42,718 : INFO : EPOCH 0 - PROGRESS: at 55.14% examples, 319705 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:43,728 : INFO : EPOCH 0 - PROGRESS: at 59.63% examples, 322626 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:44,736 : INFO : EPOCH 0 - PROGRESS: at 64.11% examples, 325328 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:45,741 : INFO : EPOCH 0 - PROGRESS: at 67.65% examples, 323394 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:46,752 : INFO : EPOCH 0 - PROGRESS: at 71.25% examples, 322144 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:47,761 : INFO : EPOCH 0 - PROGRESS: at 76.16% examples, 326632 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:12:48,767 : INFO : EPOCH 0 - PROGRESS: at 80.94% examples, 329356 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:49,791 : INFO : EPOCH 0 - PROGRESS: at 85.38% examples, 331055 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:50,799 : INFO : EPOCH 0 - PROGRESS: at 89.13% examples, 330428 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:51,802 : INFO : EPOCH 0 - PROGRESS: at 92.89% examples, 329379 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:52,812 : INFO : EPOCH 0 - PROGRESS: at 97.20% examples, 329989 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:12:53,467 : INFO : EPOCH 0: training on 11732180 raw words (8269252 effective words) took 25.0s, 330815 effective words/s\n","2024-02-08 07:12:54,483 : INFO : EPOCH 1 - PROGRESS: at 3.41% examples, 290790 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:55,492 : INFO : EPOCH 1 - PROGRESS: at 7.44% examples, 303871 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:56,528 : INFO : EPOCH 1 - PROGRESS: at 10.32% examples, 279752 words/s, in_qsize 4, out_qsize 1\n","2024-02-08 07:12:57,596 : INFO : EPOCH 1 - PROGRESS: at 13.10% examples, 261495 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:12:58,602 : INFO : EPOCH 1 - PROGRESS: at 16.64% examples, 267295 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:12:59,613 : INFO : EPOCH 1 - PROGRESS: at 19.98% examples, 267589 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:00,622 : INFO : EPOCH 1 - PROGRESS: at 22.65% examples, 260066 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:01,626 : INFO : EPOCH 1 - PROGRESS: at 27.56% examples, 278587 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:13:02,645 : INFO : EPOCH 1 - PROGRESS: at 33.48% examples, 302146 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:03,666 : INFO : EPOCH 1 - PROGRESS: at 38.06% examples, 309630 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:04,669 : INFO : EPOCH 1 - PROGRESS: at 43.68% examples, 322508 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:13:05,685 : INFO : EPOCH 1 - PROGRESS: at 48.94% examples, 331013 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:06,722 : INFO : EPOCH 1 - PROGRESS: at 53.18% examples, 331293 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:07,729 : INFO : EPOCH 1 - PROGRESS: at 57.30% examples, 331436 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:08,731 : INFO : EPOCH 1 - PROGRESS: at 60.85% examples, 329135 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:09,756 : INFO : EPOCH 1 - PROGRESS: at 65.98% examples, 334531 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:10,798 : INFO : EPOCH 1 - PROGRESS: at 70.57% examples, 336536 words/s, in_qsize 4, out_qsize 1\n","2024-02-08 07:13:11,800 : INFO : EPOCH 1 - PROGRESS: at 74.82% examples, 337815 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:12,803 : INFO : EPOCH 1 - PROGRESS: at 78.62% examples, 336507 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:13,809 : INFO : EPOCH 1 - PROGRESS: at 83.16% examples, 337724 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:14,857 : INFO : EPOCH 1 - PROGRESS: at 87.18% examples, 337299 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:15,893 : INFO : EPOCH 1 - PROGRESS: at 89.49% examples, 330352 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:16,899 : INFO : EPOCH 1 - PROGRESS: at 92.00% examples, 324814 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:17,912 : INFO : EPOCH 1 - PROGRESS: at 95.76% examples, 323848 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:18,927 : INFO : EPOCH 1 - PROGRESS: at 98.43% examples, 319668 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:19,292 : INFO : EPOCH 1: training on 11732180 raw words (8268550 effective words) took 25.8s, 320297 effective words/s\n","2024-02-08 07:13:20,305 : INFO : EPOCH 2 - PROGRESS: at 5.35% examples, 441392 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:21,308 : INFO : EPOCH 2 - PROGRESS: at 9.74% examples, 400609 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:13:22,319 : INFO : EPOCH 2 - PROGRESS: at 13.91% examples, 377285 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:23,340 : INFO : EPOCH 2 - PROGRESS: at 17.51% examples, 357972 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:24,355 : INFO : EPOCH 2 - PROGRESS: at 22.06% examples, 357822 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:25,359 : INFO : EPOCH 2 - PROGRESS: at 26.87% examples, 365455 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:26,364 : INFO : EPOCH 2 - PROGRESS: at 31.71% examples, 371604 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:27,378 : INFO : EPOCH 2 - PROGRESS: at 36.02% examples, 369572 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:13:28,395 : INFO : EPOCH 2 - PROGRESS: at 41.61% examples, 378184 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:13:29,401 : INFO : EPOCH 2 - PROGRESS: at 46.77% examples, 382631 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:30,409 : INFO : EPOCH 2 - PROGRESS: at 52.74% examples, 391685 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:31,434 : INFO : EPOCH 2 - PROGRESS: at 59.07% examples, 401583 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:32,440 : INFO : EPOCH 2 - PROGRESS: at 63.93% examples, 401570 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:13:33,443 : INFO : EPOCH 2 - PROGRESS: at 67.17% examples, 391874 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:34,464 : INFO : EPOCH 2 - PROGRESS: at 70.38% examples, 383399 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:35,470 : INFO : EPOCH 2 - PROGRESS: at 75.43% examples, 385738 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:36,473 : INFO : EPOCH 2 - PROGRESS: at 79.65% examples, 383497 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:37,473 : INFO : EPOCH 2 - PROGRESS: at 83.57% examples, 379695 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:38,488 : INFO : EPOCH 2 - PROGRESS: at 88.98% examples, 383707 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:39,494 : INFO : EPOCH 2 - PROGRESS: at 93.56% examples, 383279 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:40,516 : INFO : EPOCH 2 - PROGRESS: at 97.70% examples, 380523 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:40,991 : INFO : EPOCH 2: training on 11732180 raw words (8268986 effective words) took 21.7s, 381188 effective words/s\n","2024-02-08 07:13:42,011 : INFO : EPOCH 3 - PROGRESS: at 3.74% examples, 315862 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:43,045 : INFO : EPOCH 3 - PROGRESS: at 9.48% examples, 383237 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:44,058 : INFO : EPOCH 3 - PROGRESS: at 14.79% examples, 397775 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:13:45,060 : INFO : EPOCH 3 - PROGRESS: at 19.45% examples, 393942 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:46,077 : INFO : EPOCH 3 - PROGRESS: at 23.58% examples, 380990 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:47,079 : INFO : EPOCH 3 - PROGRESS: at 28.25% examples, 382638 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:48,079 : INFO : EPOCH 3 - PROGRESS: at 33.80% examples, 395153 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:13:49,108 : INFO : EPOCH 3 - PROGRESS: at 39.97% examples, 407928 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:50,117 : INFO : EPOCH 3 - PROGRESS: at 43.42% examples, 393565 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:51,123 : INFO : EPOCH 3 - PROGRESS: at 47.95% examples, 390911 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:52,136 : INFO : EPOCH 3 - PROGRESS: at 53.27% examples, 394671 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:53,152 : INFO : EPOCH 3 - PROGRESS: at 58.82% examples, 399419 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:54,169 : INFO : EPOCH 3 - PROGRESS: at 64.19% examples, 402413 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:55,180 : INFO : EPOCH 3 - PROGRESS: at 68.82% examples, 400817 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:56,196 : INFO : EPOCH 3 - PROGRESS: at 74.04% examples, 402816 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:13:57,197 : INFO : EPOCH 3 - PROGRESS: at 77.42% examples, 395555 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:13:58,221 : INFO : EPOCH 3 - PROGRESS: at 81.41% examples, 390676 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:13:59,247 : INFO : EPOCH 3 - PROGRESS: at 86.12% examples, 390291 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:14:00,255 : INFO : EPOCH 3 - PROGRESS: at 91.90% examples, 394802 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:14:01,269 : INFO : EPOCH 3 - PROGRESS: at 97.28% examples, 396661 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:14:01,902 : INFO : EPOCH 3: training on 11732180 raw words (8271188 effective words) took 20.9s, 395643 effective words/s\n","2024-02-08 07:14:02,932 : INFO : EPOCH 4 - PROGRESS: at 4.93% examples, 401162 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:14:03,935 : INFO : EPOCH 4 - PROGRESS: at 8.96% examples, 363520 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:14:04,973 : INFO : EPOCH 4 - PROGRESS: at 13.28% examples, 355949 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:14:05,974 : INFO : EPOCH 4 - PROGRESS: at 18.51% examples, 374864 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:14:07,008 : INFO : EPOCH 4 - PROGRESS: at 22.90% examples, 368586 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:14:08,011 : INFO : EPOCH 4 - PROGRESS: at 28.31% examples, 382425 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:14:09,014 : INFO : EPOCH 4 - PROGRESS: at 33.27% examples, 387150 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:14:10,022 : INFO : EPOCH 4 - PROGRESS: at 36.75% examples, 375893 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:14:11,042 : INFO : EPOCH 4 - PROGRESS: at 39.97% examples, 362217 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:14:12,049 : INFO : EPOCH 4 - PROGRESS: at 42.75% examples, 348364 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:14:13,063 : INFO : EPOCH 4 - PROGRESS: at 45.13% examples, 334245 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:14:14,069 : INFO : EPOCH 4 - PROGRESS: at 49.21% examples, 334049 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:14:15,079 : INFO : EPOCH 4 - PROGRESS: at 54.76% examples, 343279 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:14:16,125 : INFO : EPOCH 4 - PROGRESS: at 58.33% examples, 338488 words/s, in_qsize 6, out_qsize 1\n","2024-02-08 07:14:17,129 : INFO : EPOCH 4 - PROGRESS: at 61.34% examples, 332638 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:14:18,143 : INFO : EPOCH 4 - PROGRESS: at 64.34% examples, 327258 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:14:19,187 : INFO : EPOCH 4 - PROGRESS: at 67.83% examples, 324089 words/s, in_qsize 6, out_qsize 0\n","2024-02-08 07:14:20,197 : INFO : EPOCH 4 - PROGRESS: at 71.75% examples, 324354 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:14:21,205 : INFO : EPOCH 4 - PROGRESS: at 76.16% examples, 326563 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:14:22,218 : INFO : EPOCH 4 - PROGRESS: at 80.20% examples, 326401 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:14:23,219 : INFO : EPOCH 4 - PROGRESS: at 83.99% examples, 325476 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:14:24,224 : INFO : EPOCH 4 - PROGRESS: at 87.18% examples, 323193 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:14:25,258 : INFO : EPOCH 4 - PROGRESS: at 91.29% examples, 323462 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:14:26,289 : INFO : EPOCH 4 - PROGRESS: at 95.94% examples, 325169 words/s, in_qsize 5, out_qsize 0\n","2024-02-08 07:14:27,232 : INFO : EPOCH 4: training on 11732180 raw words (8269714 effective words) took 25.3s, 326548 effective words/s\n","2024-02-08 07:14:27,233 : INFO : Word2Vec lifecycle event {'msg': 'training on 58660900 raw words (41347690 effective words) took 118.8s, 348001 effective words/s', 'datetime': '2024-02-08T07:14:27.233796', 'gensim': '4.3.2', 'python': '3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.22621-SP0', 'event': 'train'}\n","2024-02-08 07:14:27,234 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=18755, vector_size=300, alpha=0.025>', 'datetime': '2024-02-08T07:14:27.234819', 'gensim': '4.3.2', 'python': '3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.22621-SP0', 'event': 'created'}\n"]}],"source":["# Tokenize the reviews for training the Word2Vec model\n","tokenized_reviews = [nltk.word_tokenize(review) for review in X_train]\n","tokenized_reviews_binary = [nltk.word_tokenize(review) for review in X_train_binary]\n","\n","# Train a Word2Vec model using the tokenized reviews\n","word2vec_model = gensim.models.Word2Vec(sentences=tokenized_reviews, vector_size=300, window=11, min_count=10)\n","word2vec_model_binary = gensim.models.Word2Vec(sentences=tokenized_reviews_binary, vector_size=300, window=11, min_count=10)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fP3iMsJ3bzSP","outputId":"4314a911-54a8-4776-f65d-beb1eb18d23a"},"outputs":[{"name":"stdout","output_type":"stream","text":["TRAINED WORD2VEC\n","Semantic similarity between 'king - man + woman' and 'queen' (custom): [('Binders', 0.471794992685318)]\n","Semantic similarity between 'excellent' and 'outstanding' (custom): 0.869806\n"]}],"source":["print(\"TRAINED WORD2VEC\")\n","\n","# Demonstrate semantic similarities for the trained Word2Vec model\n","king_minus_man_plus_woman_custom = word2vec_model.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)\n","excellent_similar_outstanding_custom = word2vec_model.wv.similarity('excellent', 'outstanding')\n","\n","print(\"Semantic similarity between 'king - man + woman' and 'queen' (custom):\", king_minus_man_plus_woman_custom)\n","print(\"Semantic similarity between 'excellent' and 'outstanding' (custom):\", excellent_similar_outstanding_custom)\n"]},{"cell_type":"markdown","metadata":{"id":"bnYp92B-bzSQ"},"source":["# **SIMPLE MODELS**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2K4z2ZeUbzSQ","outputId":"a8e479bc-61e4-4efa-d196-7082fa29bfbf"},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train:\n","75381     not good... after printing 10 pages the black ...\n","65569                                          out of date.\n","163473    Ok phone for the price. It did what I needed i...\n","90518     I just set up my printer.  However, while gett...\n","138866    It is super cute, but I am not sure of its las...\n","                                ...                        \n","119879    Easy to install unfortunately for me I would n...\n","103694    the negatives first:<br />- Big ink eater.<br ...\n","131932    This product only lasts one month once you ope...\n","146867    The ink is on the dry side.  All in all, it is...\n","121958    I got the inks in a timely manner however ever...\n","Name: review_body, Length: 200000, dtype: object\n","X_train_word2vec:\n","[[ 0.2017009  -0.07907066 -0.5368596  ... -0.02009328  0.18456513\n","  -0.6514082 ]\n"," [-0.18326068 -0.27163714 -0.15210694 ...  0.4901157   0.1256411\n","  -0.591722  ]\n"," [ 0.2116916  -0.7891235   0.08931109 ...  0.40844712  0.0389001\n","  -0.28632304]\n"," ...\n"," [ 0.38853553 -0.13969746 -0.20579696 ... -0.161258   -0.25687334\n","  -0.19186014]\n"," [-0.64858884 -0.2711212  -0.23436244 ...  0.45585138  0.31138873\n","   0.11617168]\n"," [ 0.10837158 -0.03350762  0.1950251  ... -0.09390983  0.04972094\n","   0.18334861]]\n","X_train_binary:\n","203248    the first one did not work and I called the co...\n","67802                                          Did not owrk\n","198889    Have not installed these print cartridges as y...\n","153093    I'm no stranger to using small notebooks--I te...\n","154681                 great ink that always costs too much\n","                                ...                        \n","169879                                        Good product!\n","153694                               Cute for couple gifts.\n","181932    for delivery it was quick<br />a little hard f...\n","196867    Great, just as expected! Arrived quickly, and ...\n","171958    The only negative comment I have on these fold...\n","Name: review_body, Length: 160000, dtype: object\n","X_train_word2vec:\n","[[ 0.31096187 -0.6091808   0.42416167 ...  0.09640834  0.3426072\n","  -0.6899506 ]\n"," [-0.7746662  -0.80873775  0.6279652  ... -0.6703824   0.78104913\n","  -0.95146084]\n"," [-0.48868233  0.3614831  -0.535585   ... -0.40747473  0.42555365\n","   0.32634395]\n"," ...\n"," [ 0.13285238 -0.54224074 -0.30347514 ... -0.20487067 -0.32228625\n","  -0.19547915]\n"," [-0.08551348  0.01734277 -0.22523554 ...  0.25071156 -0.3336795\n","  -0.0644362 ]\n"," [-0.45002365 -0.19304007 -0.39098364 ...  0.00859469  0.34466815\n","  -0.26334968]]\n"]}],"source":["# Define a function to represent each example with Word2Vec features\n","def get_word2vec_features(review_text):\n","    feature_vector = np.zeros((word2vec_model.vector_size,), dtype=\"float32\")\n","    num_words = 0\n","    for word in review_text:\n","        if word in word2vec_model.wv:\n","            feature_vector = np.add(feature_vector, word2vec_model.wv[word])\n","            num_words += 1\n","    if num_words != 0:\n","        feature_vector = np.divide(feature_vector, num_words)\n","    return feature_vector\n","\n","# Represent each example with Word2Vec features\n","X_train_word2vec = np.array([get_word2vec_features(review) for review in tokenized_reviews])\n","print(\"X_train:\")\n","print(X_train)\n","print('X_train_word2vec:')\n","print(X_train_word2vec)\n","\n","X_train_word2vec_binary = np.array([get_word2vec_features(review) for review in tokenized_reviews_binary])\n","print(\"X_train_binary:\")\n","print(X_train_binary)\n","print('X_train_word2vec:')\n","print(X_train_word2vec_binary)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ofwPYwXebzSR","outputId":"ee26689f-3f74-4536-87da-0d201aa9ec3e"},"outputs":[{"name":"stdout","output_type":"stream","text":["X_test:\n","38683     Product is not what it seems. drivers are all ...\n","64939     I have no problem with the comfort of this sty...\n","3954      The ink never worked.  I guess the printer rec...\n","120374    This is a printer for the more serious photo e...\n","172861    I ordered these cartridges Nov 2012 despite so...\n","                                ...                        \n","179545    Yes, it only does a few pages at a time, but i...\n","222647    My son loves this lunch box. Plenty of room in...\n","171823    UPDATE: Just loaded the replacement system on ...\n","135782    It is a nice pen and it writes pretty smoothly...\n","208380                                                 fine\n","Name: review_body, Length: 50000, dtype: object\n","X_test_word2vec:\n","[[ 0.2107049  -0.03894419 -0.12380177 ...  0.4152071  -0.01419189\n","  -0.4963122 ]\n"," [ 0.2319088   0.07627508  0.02765747 ...  0.40654182  0.00452649\n","  -0.30042556]\n"," [ 0.01375213 -0.22278257  0.16280225 ...  0.38947886  0.27099377\n","  -0.23333038]\n"," ...\n"," [ 0.30687958  0.01539032 -0.01106659 ...  0.0930704  -0.03212216\n","  -0.2339327 ]\n"," [-0.2953721  -0.24762636 -0.25611266 ... -0.09011092 -0.10960484\n","   0.12806235]\n"," [ 0.6992302   0.56315106 -1.4366542  ... -0.6593002   0.7092717\n","  -2.316879  ]]\n","X_test_binary:\n","169737    These toner units arrived as expected, and my ...\n","72272     I know this was a used product but it should h...\n","208154    I love this sleeve. Perfect fit for my tablet....\n","65426     I keep getting emails from Amazon marketplace ...\n","30074     I have had this printer for about two years no...\n","                                ...                        \n","4174      I bought this phone because we owned an old SW...\n","91537     I was looking for some new pens because I got ...\n","206449                      Just as described. Works great!\n","234376    I was afraid that the card was bad when I init...\n","6584      I used this for my Epson Workforce 633 all in ...\n","Name: review_body, Length: 40000, dtype: object\n","X_test_word2vec:\n","[[ 3.26966017e-01 -1.11894116e-01 -1.86730698e-01 ... -4.94685322e-02\n","  -6.25153184e-02 -4.11460012e-01]\n"," [ 5.73072314e-01 -4.15823519e-01 -1.58854481e-02 ...  3.03850248e-02\n","   2.16844633e-01 -3.74767601e-01]\n"," [ 1.22771490e+00 -4.08821702e-01 -2.84862012e-01 ...  1.43464044e-01\n","  -2.90960353e-02  2.07666323e-01]\n"," ...\n"," [ 9.30196494e-02  6.78098127e-02 -2.31968835e-01 ...  2.70754248e-01\n","  -4.89973754e-01 -3.79799992e-01]\n"," [ 7.72652042e-04 -3.02741945e-01 -8.62538889e-02 ...  4.50719684e-01\n","   7.29737133e-02  2.81247236e-02]\n"," [ 2.27113888e-01 -1.94789991e-01 -8.69283751e-02 ...  1.49957001e-01\n","   2.06025258e-01 -1.44690439e-01]]\n"]}],"source":["# Tokenize the reviews for training the Word2Vec model\n","tokenized_reviews_test = [nltk.word_tokenize(review) for review in X_test]\n","tokenized_reviews_test_binary = [nltk.word_tokenize(review) for review in X_test_binary]\n","\n","# Represent each example with Word2Vec features\n","X_test_word2vec = np.array([get_word2vec_features(review) for review in tokenized_reviews_test])\n","print(\"X_test:\")\n","print(X_test)\n","print('X_test_word2vec:')\n","print(X_test_word2vec)\n","\n","X_test_word2vec_binary = np.array([get_word2vec_features(review) for review in tokenized_reviews_test_binary])\n","print(\"X_test_binary:\")\n","print(X_test_binary)\n","print('X_test_word2vec:')\n","print(X_test_word2vec_binary)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AVJKBuYkbzSR","outputId":"1064f8c1-9d13-43a8-b176-67ee7f32267b"},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train:\n","75381     not good... after printing 10 pages the black ...\n","65569                                          out of date.\n","163473    Ok phone for the price. It did what I needed i...\n","90518     I just set up my printer.  However, while gett...\n","138866    It is super cute, but I am not sure of its las...\n","                                ...                        \n","119879    Easy to install unfortunately for me I would n...\n","103694    the negatives first:<br />- Big ink eater.<br ...\n","131932    This product only lasts one month once you ope...\n","146867    The ink is on the dry side.  All in all, it is...\n","121958    I got the inks in a timely manner however ever...\n","Name: review_body, Length: 200000, dtype: object\n","X_train_pretrained:\n","[[ 2.00366974e-02  7.23719001e-02 -8.65077972e-03 ...  7.31647015e-03\n","   2.55513191e-03 -3.31020355e-02]\n"," [ 6.95800781e-02 -6.98242188e-02  1.13891602e-01 ...  5.51757812e-02\n","  -1.22802734e-01 -1.23901367e-01]\n"," [ 4.91333008e-02  3.28812599e-02  6.25712052e-02 ... -8.61612987e-03\n","   4.92045097e-02 -9.64151993e-02]\n"," ...\n"," [ 3.07371076e-02 -3.80854435e-05  1.91473197e-02 ... -2.06987932e-02\n","   3.11773978e-02 -3.84303667e-02]\n"," [ 9.39941406e-03  5.75222224e-02  4.59241383e-02 ... -4.33462039e-02\n","   2.60941349e-02 -5.69168888e-02]\n"," [ 3.21655273e-02  7.10173473e-02 -2.61804909e-02 ...  1.30890114e-02\n","  -3.46634798e-02 -4.46956865e-02]]\n","X_train_binary:\n","203248    the first one did not work and I called the co...\n","67802                                          Did not owrk\n","198889    Have not installed these print cartridges as y...\n","153093    I'm no stranger to using small notebooks--I te...\n","154681                 great ink that always costs too much\n","                                ...                        \n","169879                                        Good product!\n","153694                               Cute for couple gifts.\n","181932    for delivery it was quick<br />a little hard f...\n","196867    Great, just as expected! Arrived quickly, and ...\n","171958    The only negative comment I have on these fold...\n","Name: review_body, Length: 160000, dtype: object\n","X_train_pretrained_binary:\n","[[ 0.05831427  0.01119031  0.07898591 ... -0.04217369  0.02300704\n","  -0.07814347]\n"," [ 0.23095703  0.0402832   0.14453125 ... -0.2097168  -0.12011719\n","  -0.06481552]\n"," [-0.03130231  0.0554548   0.02392578 ... -0.06044224  0.00124468\n","  -0.06059047]\n"," ...\n"," [ 0.00028522  0.03813636  0.01627448 ...  0.00038816  0.01891151\n","  -0.0498579 ]\n"," [ 0.08214188  0.10991287 -0.03590393 ... -0.02081299  0.00827026\n","  -0.08468628]\n"," [ 0.00210089  0.04958223  0.00985959 ... -0.05374306  0.00385365\n","  -0.01479761]]\n"]}],"source":["def get_pretrained_features(review_text):\n","    feature_vector = np.zeros((pretrained_model.vector_size,), dtype=\"float32\")\n","    num_words = 0\n","    for word in review_text:\n","        if word in pretrained_model:\n","            feature_vector += pretrained_model[word]\n","            num_words += 1\n","    if num_words != 0:\n","        feature_vector /= num_words\n","    return feature_vector\n","\n","# Represent each example with Word2Vec features\n","X_train_pretrained = np.array([get_pretrained_features(review) for review in tokenized_reviews])\n","print(\"X_train:\")\n","print(X_train)\n","print('X_train_pretrained:')\n","print(X_train_pretrained)\n","\n","X_train_pretrained_binary = np.array([get_pretrained_features(review) for review in tokenized_reviews_binary])\n","print(\"X_train_binary:\")\n","print(X_train_binary)\n","print('X_train_pretrained_binary:')\n","print(X_train_pretrained_binary)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DK2il-QsbzSS","outputId":"941890ef-093e-47ae-d283-4e5d32fed178"},"outputs":[{"name":"stdout","output_type":"stream","text":["X_test:\n","38683     Product is not what it seems. drivers are all ...\n","64939     I have no problem with the comfort of this sty...\n","3954      The ink never worked.  I guess the printer rec...\n","120374    This is a printer for the more serious photo e...\n","172861    I ordered these cartridges Nov 2012 despite so...\n","                                ...                        \n","179545    Yes, it only does a few pages at a time, but i...\n","222647    My son loves this lunch box. Plenty of room in...\n","171823    UPDATE: Just loaded the replacement system on ...\n","135782    It is a nice pen and it writes pretty smoothly...\n","208380                                                 fine\n","Name: review_body, Length: 50000, dtype: object\n","X_test_word2vec:\n","[[ 0.0372862  -0.00168436  0.05948415 ... -0.04786951  0.01604865\n","  -0.04362757]\n"," [ 0.03357273  0.00863713  0.0109748  ... -0.04012519  0.00613607\n","   0.01166263]\n"," [ 0.06132334  0.03399953  0.04939803 ... -0.05105566  0.02355414\n","  -0.06073438]\n"," ...\n"," [ 0.02403376  0.0442563   0.01952958 ... -0.02775795  0.01947324\n","  -0.05326013]\n"," [ 0.05141469  0.05772183 -0.01482659 ... -0.02143613 -0.00704606\n","  -0.0586152 ]\n"," [ 0.24316406 -0.02770996  0.16113281 ... -0.05786133  0.06396484\n","  -0.15527344]]\n","X_train_binary:\n","169737    These toner units arrived as expected, and my ...\n","72272     I know this was a used product but it should h...\n","208154    I love this sleeve. Perfect fit for my tablet....\n","65426     I keep getting emails from Amazon marketplace ...\n","30074     I have had this printer for about two years no...\n","                                ...                        \n","4174      I bought this phone because we owned an old SW...\n","91537     I was looking for some new pens because I got ...\n","206449                      Just as described. Works great!\n","234376    I was afraid that the card was bad when I init...\n","6584      I used this for my Epson Workforce 633 all in ...\n","Name: review_body, Length: 40000, dtype: object\n","X_train_word2vec:\n","[[ 0.05048644  0.04953517  0.02170318 ... -0.03716278  0.04053267\n","  -0.03790456]\n"," [ 0.05233256  0.03170214  0.05355167 ... -0.03378471  0.01800187\n","  -0.06788865]\n"," [ 0.03572083  0.01698971  0.00825402 ... -0.05144968  0.01558119\n","  -0.03912944]\n"," ...\n"," [ 0.04291992  0.08842774  0.03728027 ... -0.02246094  0.05861817\n","  -0.04296875]\n"," [ 0.03939318  0.02414313  0.05656319 ... -0.04614804  0.01034227\n","  -0.02155179]\n"," [ 0.05767284  0.04606729  0.03031287 ... -0.01336824  0.00937646\n","  -0.07723463]]\n"]}],"source":["# Represent each example with Pretrained features\n","X_test_pretrained = np.array([get_pretrained_features(review) for review in tokenized_reviews_test])\n","print(\"X_test:\")\n","print(X_test)\n","print('X_test_word2vec:')\n","print(X_test_pretrained)\n","\n","X_test_pretrained_binary = np.array([get_pretrained_features(review) for review in tokenized_reviews_test_binary])\n","print(\"X_train_binary:\")\n","print(X_test_binary)\n","print('X_train_word2vec:')\n","print(X_test_pretrained_binary)"]},{"cell_type":"markdown","source":["#### **TDIDF**"],"metadata":{"id":"WIGz2FGpcsz2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"CednTeUibzST","outputId":"35a98ac1-ea79-46f1-a867-22dc6d48fd8a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of feature matrix (X): (200000, 130597)\n"]}],"source":["tfidf_vectorizer = TfidfVectorizer()\n","X = tfidf_vectorizer.fit_transform(df_binary['clean_review'])\n","y = df_binary['sentiment']\n","\n","# shape of the feature matrix\n","print(\"Shape of feature matrix (X):\", X.shape)\n","\n","# Split the data into training and testing sets\n","X_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned = train_test_split(X, y, test_size=0.2, random_state=42)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XJHEeJ4IbzST","outputId":"fe76d40d-4f0e-48d9-ee7c-9284865e9d6e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Perceptron Accuracy (TF-IDF): 0.816475\n"]}],"source":["perceptron_classifier = Perceptron(max_iter=1000)\n","perceptron_classifier.fit(X_train_cleaned, y_train_cleaned)\n","y_pred_perceptron = perceptron_classifier.predict(X_test_cleaned)\n","print(\"Perceptron Accuracy (TF-IDF):\", accuracy_score(y_test_cleaned, y_pred_perceptron))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AVh_4afkbzSU","outputId":"abb0b9b6-3402-49b7-a89a-aa437cb63856"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["SVM Accuracy (TF-IDF): 0.8651\n"]}],"source":["svm_classifier = LinearSVC()\n","svm_classifier.fit(X_train_cleaned, y_train_cleaned)\n","y_pred_svm = svm_classifier.predict(X_test_cleaned)\n","print(\"SVM Accuracy (TF-IDF):\", accuracy_score(y_test_cleaned, y_pred_svm))"]},{"cell_type":"markdown","source":["#### **PRE-TRAINED**"],"metadata":{"id":"zLz0QbhQczpX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MQcty_U8bzSV","outputId":"8d8fe23a-4897-46ec-87e8-36bae45ac318"},"outputs":[{"name":"stdout","output_type":"stream","text":["Perceptron Accuracy (Word2Vec): 0.7555\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["SVM Accuracy (Word2Vec): 0.83135\n"]}],"source":["# Train Perceptron and SVM models using pretrained features\n","perceptron_classifier_pretrained = Perceptron(max_iter=1000)\n","perceptron_classifier_pretrained.fit(X_train_pretrained_binary, y_train_binary)\n","y_pred_perceptron_pretrained = perceptron_classifier_pretrained.predict(X_test_pretrained_binary)\n","print(\"Perceptron Accuracy (Word2Vec):\", accuracy_score(y_test_binary, y_pred_perceptron_pretrained))\n","\n","svm_classifier_pretrained =  LinearSVC()\n","svm_classifier_pretrained.fit(X_train_pretrained_binary, y_train_binary)\n","y_pred_svm_pretrained = svm_classifier_pretrained.predict(X_test_pretrained_binary)\n","print(\"SVM Accuracy (Word2Vec):\", accuracy_score(y_test_binary, y_pred_svm_pretrained))"]},{"cell_type":"markdown","source":["#### **WORD2VEC**"],"metadata":{"id":"8WGB347Nc7DP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Y4UZgvwbzSW","outputId":"fabe1c94-c735-421a-c91c-cb5cad1c74da"},"outputs":[{"name":"stdout","output_type":"stream","text":["Perceptron Accuracy (Word2Vec): 0.7956\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["SVM Accuracy (Word2Vec): 0.8656\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n"]}],"source":["# Train Perceptron and SVM models using Word2Vec features\n","perceptron_classifier_word2vec = Perceptron(max_iter=1000)\n","perceptron_classifier_word2vec.fit(X_train_word2vec_binary, y_train_binary)\n","y_pred_perceptron_word2vec = perceptron_classifier_word2vec.predict(X_test_word2vec_binary)\n","print(\"Perceptron Accuracy (Word2Vec):\", accuracy_score(y_test_binary, y_pred_perceptron_word2vec))\n","\n","svm_classifier_word2vec =  LinearSVC()\n","svm_classifier_word2vec.fit(X_train_word2vec_binary, y_train_binary)\n","y_pred_svm_word2vec = svm_classifier_word2vec.predict(X_test_word2vec_binary)\n","print(\"SVM Accuracy (Word2Vec):\", accuracy_score(y_test_binary, y_pred_svm_word2vec))"]},{"cell_type":"markdown","metadata":{"id":"9XnbZmGeJREd"},"source":["# **Feedforward Neural Networks**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IfNCxsJ0bzSX"},"outputs":[],"source":["\n","# Define the MLP model\n","class MLP(nn.Module):\n","    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n","        super(MLP, self).__init__()\n","        self.fc1 = nn.Linear(input_size, hidden_size1)\n","        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n","        self.fc3 = nn.Linear(hidden_size2, output_size)\n","        self.relu = nn.ReLU()\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, x):\n","        x = self.relu(self.fc1(x))\n","        x = self.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return self.softmax(x)"]},{"cell_type":"markdown","metadata":{"id":"5LiUrGhAbzSX"},"source":["### **AVG METHOD**"]},{"cell_type":"markdown","metadata":{"id":"1bnBQOH_bzSY"},"source":["##### **WORD2VEC BINARY**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mo7vAapfbzSY"},"outputs":[],"source":["# Convert the Word2Vec features to PyTorch tensors\n","X_train_tensor_binary = torch.tensor(X_train_word2vec_binary, dtype=torch.float32)\n","y_train_tensor_binary = torch.tensor(y_train_binary-1, dtype=torch.long)  # Assuming y_train_binary contains binary labels\n","X_test_tensor_binary = torch.tensor(X_test_word2vec_binary, dtype=torch.float32)\n","y_test_tensor_binary = torch.tensor(y_test_binary.values-1, dtype=torch.long)  # Convert Series to numpy array with .values attribute\n","\n","# Define hyperparameters\n","input_size = X_train_tensor_binary.shape[1]\n","hidden_size1 = 50\n","hidden_size2 = 10\n","output_size = 2  # Binary classification\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IZzaEn_NbzSZ","outputId":"8b0cdec6-bd51-4a42-ceb5-589ec44482c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([0, 1, 0,  ..., 0, 0, 0])\n"]}],"source":["print(y_train_tensor_binary)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gegL7O-NbzSg"},"outputs":[],"source":["# Create DataLoader for training\n","train_dataset_binary = TensorDataset(X_train_tensor_binary, y_train_tensor_binary)\n","train_loader_binary = DataLoader(train_dataset_binary, batch_size=64, shuffle=True)\n","\n","# Initialize the model, loss function, and optimizer\n","model = MLP(input_size, hidden_size1, hidden_size2, output_size)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w9p3OqgrbzSh","outputId":"68b92d93-760e-432d-94f3-dd891b3498fb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 0.6967177987098694\n","Epoch 2, Loss: 0.694940447807312\n","Epoch 3, Loss: 0.6932215094566345\n","Epoch 4, Loss: 0.6913893818855286\n","Epoch 5, Loss: 0.6894699931144714\n","Epoch 6, Loss: 0.6874707341194153\n","Epoch 7, Loss: 0.6853677034378052\n","Epoch 8, Loss: 0.6831188201904297\n","Epoch 9, Loss: 0.6806842088699341\n","Epoch 10, Loss: 0.6780391931533813\n","Epoch 11, Loss: 0.6751617789268494\n","Epoch 12, Loss: 0.6720513105392456\n","Epoch 13, Loss: 0.6687220335006714\n","Epoch 14, Loss: 0.6651893258094788\n","Epoch 15, Loss: 0.66147381067276\n","Epoch 16, Loss: 0.6575876474380493\n","Epoch 17, Loss: 0.6535401344299316\n","Epoch 18, Loss: 0.6493402719497681\n","Epoch 19, Loss: 0.6449918746948242\n","Epoch 20, Loss: 0.6405007839202881\n","Epoch 21, Loss: 0.6358786225318909\n","Epoch 22, Loss: 0.6311385631561279\n","Epoch 23, Loss: 0.6262966990470886\n","Epoch 24, Loss: 0.6213709712028503\n","Epoch 25, Loss: 0.6163766980171204\n","Epoch 26, Loss: 0.6113384962081909\n","Epoch 27, Loss: 0.6062882542610168\n","Epoch 28, Loss: 0.6012606620788574\n","Epoch 29, Loss: 0.5962892770767212\n","Epoch 30, Loss: 0.5914019346237183\n","Epoch 31, Loss: 0.5866128206253052\n","Epoch 32, Loss: 0.5819260478019714\n","Epoch 33, Loss: 0.5773425102233887\n","Epoch 34, Loss: 0.5728665590286255\n","Epoch 35, Loss: 0.5685064792633057\n","Epoch 36, Loss: 0.564272403717041\n","Epoch 37, Loss: 0.5601673722267151\n","Epoch 38, Loss: 0.5561916828155518\n","Epoch 39, Loss: 0.5523435473442078\n","Epoch 40, Loss: 0.5486234426498413\n","Epoch 41, Loss: 0.5450335741043091\n","Epoch 42, Loss: 0.5415758490562439\n","Epoch 43, Loss: 0.5382502675056458\n","Epoch 44, Loss: 0.535053551197052\n","Epoch 45, Loss: 0.5319804549217224\n","Epoch 46, Loss: 0.5290269255638123\n","Epoch 47, Loss: 0.5261919498443604\n","Epoch 48, Loss: 0.5234744548797607\n","Epoch 49, Loss: 0.5208732485771179\n","Epoch 50, Loss: 0.5183826684951782\n","Epoch 51, Loss: 0.5159963369369507\n","Epoch 52, Loss: 0.513709306716919\n","Epoch 53, Loss: 0.5115179419517517\n","Epoch 54, Loss: 0.5094189643859863\n","Epoch 55, Loss: 0.5074063539505005\n","Epoch 56, Loss: 0.5054739117622375\n","Epoch 57, Loss: 0.5036162734031677\n","Epoch 58, Loss: 0.5018291473388672\n","Epoch 59, Loss: 0.5001081824302673\n","Epoch 60, Loss: 0.49844956398010254\n","Epoch 61, Loss: 0.49684759974479675\n","Epoch 62, Loss: 0.4952980577945709\n","Epoch 63, Loss: 0.4937981963157654\n","Epoch 64, Loss: 0.49234604835510254\n","Epoch 65, Loss: 0.49093887209892273\n","Epoch 66, Loss: 0.4895747900009155\n","Epoch 67, Loss: 0.4882524311542511\n","Epoch 68, Loss: 0.4869706630706787\n","Epoch 69, Loss: 0.485728919506073\n","Epoch 70, Loss: 0.48452553153038025\n","Epoch 71, Loss: 0.4833589494228363\n","Epoch 72, Loss: 0.48222869634628296\n","Epoch 73, Loss: 0.4811336398124695\n","Epoch 74, Loss: 0.48007240891456604\n","Epoch 75, Loss: 0.47904449701309204\n","Epoch 76, Loss: 0.47804898023605347\n","Epoch 77, Loss: 0.4770859479904175\n","Epoch 78, Loss: 0.4761550724506378\n","Epoch 79, Loss: 0.47525522112846375\n","Epoch 80, Loss: 0.47438526153564453\n","Epoch 81, Loss: 0.47354409098625183\n","Epoch 82, Loss: 0.47273018956184387\n","Epoch 83, Loss: 0.47194233536720276\n","Epoch 84, Loss: 0.47117915749549866\n","Epoch 85, Loss: 0.4704402685165405\n","Epoch 86, Loss: 0.4697254002094269\n","Epoch 87, Loss: 0.4690329134464264\n","Epoch 88, Loss: 0.46836143732070923\n","Epoch 89, Loss: 0.4677097797393799\n","Epoch 90, Loss: 0.4670770466327667\n","Epoch 91, Loss: 0.4664621949195862\n","Epoch 92, Loss: 0.4658643901348114\n","Epoch 93, Loss: 0.46528300642967224\n","Epoch 94, Loss: 0.46471723914146423\n","Epoch 95, Loss: 0.4641666114330292\n","Epoch 96, Loss: 0.4636305272579193\n","Epoch 97, Loss: 0.46310821175575256\n","Epoch 98, Loss: 0.4625994563102722\n","Epoch 99, Loss: 0.46210357546806335\n","Epoch 100, Loss: 0.4616205096244812\n"]}],"source":["# Training loop\n","num_epochs = 100\n","for epoch in range(num_epochs):\n","    model.train()\n","    optimizer.zero_grad()\n","    outputs = model(X_train_tensor_binary)\n","    loss = criterion(outputs, y_train_tensor_binary)\n","    loss.backward()\n","    optimizer.step()\n","    print(f\"Epoch {epoch+1}, Loss: {loss}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6_Sp8An4bzSh","outputId":"0c3c65e5-fad2-4097-aa99-98f3d1530b1b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy (WORD2VEC BINARY): 0.852975\n"]}],"source":["# Evaluate the model on the test set\n","model.eval()\n","with torch.no_grad():\n","    outputs = model(X_test_tensor_binary)\n","    _, predicted = torch.max(outputs, 1)\n","    accuracy = (predicted == y_test_tensor_binary).sum().item() / len(y_test_tensor_binary)\n","    print(\"Accuracy (WORD2VEC BINARY):\", accuracy)"]},{"cell_type":"markdown","metadata":{"id":"xWFEoVE6bzSi"},"source":["#####  **WORD2VEC TERTIARY**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VNeHDqG-bzSi"},"outputs":[],"source":["# Convert the Word2Vec features to PyTorch tensors\n","X_train_tensor = torch.tensor(X_train_word2vec, dtype=torch.float32)\n","y_train_tensor = torch.tensor(y_train-1, dtype=torch.long)  # Assuming y_train_binary contains binary labels\n","X_test_tensor = torch.tensor(X_test_word2vec, dtype=torch.float32)\n","y_test_tensor = torch.tensor(y_test.values-1, dtype=torch.long)  # Convert Series to numpy array with .values attribute\n","\n","# Define hyperparameters\n","input_size = X_train_tensor.shape[1]\n","hidden_size1 = 50\n","hidden_size2 = 10\n","output_size = 3  # Tertiary classification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"74zYxSg7bzSi","outputId":"d02f84a4-1d6c-4b4d-88de-9aae021136d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([1, 1, 0,  ..., 2, 2, 2])\n"]}],"source":["print(y_train_tensor)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m9MihUtvbzSi"},"outputs":[],"source":["# Create DataLoader for training\n","train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","\n","# Initialize the model, loss function, and optimizer\n","model = MLP(input_size, hidden_size1, hidden_size2, output_size)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gwiMEGoGbzSj","outputId":"7a206561-6c10-407f-b30f-550988989494"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 1.1127711534500122\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, Loss: 1.1096194982528687\n","Epoch 3, Loss: 1.1063947677612305\n","Epoch 4, Loss: 1.10308837890625\n","Epoch 5, Loss: 1.0996514558792114\n","Epoch 6, Loss: 1.0960378646850586\n","Epoch 7, Loss: 1.0922117233276367\n","Epoch 8, Loss: 1.0881465673446655\n","Epoch 9, Loss: 1.083852767944336\n","Epoch 10, Loss: 1.0793362855911255\n","Epoch 11, Loss: 1.074615716934204\n","Epoch 12, Loss: 1.069711446762085\n","Epoch 13, Loss: 1.0646355152130127\n","Epoch 14, Loss: 1.0593911409378052\n","Epoch 15, Loss: 1.0539799928665161\n","Epoch 16, Loss: 1.048409104347229\n","Epoch 17, Loss: 1.0427024364471436\n","Epoch 18, Loss: 1.0369000434875488\n","Epoch 19, Loss: 1.031054139137268\n","Epoch 20, Loss: 1.0252203941345215\n","Epoch 21, Loss: 1.0194531679153442\n","Epoch 22, Loss: 1.0137931108474731\n","Epoch 23, Loss: 1.0082675218582153\n","Epoch 24, Loss: 1.0028938055038452\n","Epoch 25, Loss: 0.9976795315742493\n","Epoch 26, Loss: 0.9926290512084961\n","Epoch 27, Loss: 0.987740695476532\n","Epoch 28, Loss: 0.9830119609832764\n","Epoch 29, Loss: 0.9784449934959412\n","Epoch 30, Loss: 0.9740492105484009\n","Epoch 31, Loss: 0.9698386192321777\n","Epoch 32, Loss: 0.9658202528953552\n","Epoch 33, Loss: 0.9619792103767395\n","Epoch 34, Loss: 0.9582827091217041\n","Epoch 35, Loss: 0.9546897411346436\n","Epoch 36, Loss: 0.9511716961860657\n","Epoch 37, Loss: 0.9477365016937256\n","Epoch 38, Loss: 0.9444355368614197\n","Epoch 39, Loss: 0.9413365721702576\n","Epoch 40, Loss: 0.9384547472000122\n","Epoch 41, Loss: 0.9357389211654663\n","Epoch 42, Loss: 0.9331312775611877\n","Epoch 43, Loss: 0.9306162595748901\n","Epoch 44, Loss: 0.9282079935073853\n","Epoch 45, Loss: 0.9259200096130371\n","Epoch 46, Loss: 0.9237464666366577\n","Epoch 47, Loss: 0.9216598272323608\n","Epoch 48, Loss: 0.9196363091468811\n","Epoch 49, Loss: 0.9176711440086365\n","Epoch 50, Loss: 0.9157791137695312\n","Epoch 51, Loss: 0.9139764904975891\n","Epoch 52, Loss: 0.912263035774231\n","Epoch 53, Loss: 0.9106231927871704\n","Epoch 54, Loss: 0.9090389609336853\n","Epoch 55, Loss: 0.9075061082839966\n","Epoch 56, Loss: 0.9060324430465698\n","Epoch 57, Loss: 0.9046252369880676\n","Epoch 58, Loss: 0.9032800197601318\n","Epoch 59, Loss: 0.9019821286201477\n","Epoch 60, Loss: 0.900719404220581\n","Epoch 61, Loss: 0.8994911909103394\n","Epoch 62, Loss: 0.8983030319213867\n","Epoch 63, Loss: 0.8971557021141052\n","Epoch 64, Loss: 0.8960423469543457\n","Epoch 65, Loss: 0.8949539661407471\n","Epoch 66, Loss: 0.8938878774642944\n","Epoch 67, Loss: 0.8928479552268982\n","Epoch 68, Loss: 0.8918364644050598\n","Epoch 69, Loss: 0.8908512592315674\n","Epoch 70, Loss: 0.8898867964744568\n","Epoch 71, Loss: 0.8889415860176086\n","Epoch 72, Loss: 0.888017475605011\n","Epoch 73, Loss: 0.8871172070503235\n","Epoch 74, Loss: 0.8862389922142029\n","Epoch 75, Loss: 0.885379433631897\n","Epoch 76, Loss: 0.8845376372337341\n","Epoch 77, Loss: 0.8837149143218994\n","Epoch 78, Loss: 0.8829135894775391\n","Epoch 79, Loss: 0.8821322917938232\n","Epoch 80, Loss: 0.8813684582710266\n","Epoch 81, Loss: 0.880621612071991\n","Epoch 82, Loss: 0.8798927068710327\n","Epoch 83, Loss: 0.8791822791099548\n","Epoch 84, Loss: 0.8784886598587036\n","Epoch 85, Loss: 0.8778102993965149\n","Epoch 86, Loss: 0.8771481513977051\n","Epoch 87, Loss: 0.876503586769104\n","Epoch 88, Loss: 0.87587571144104\n","Epoch 89, Loss: 0.8752632141113281\n","Epoch 90, Loss: 0.8746656179428101\n","Epoch 91, Loss: 0.8740838766098022\n","Epoch 92, Loss: 0.8735173344612122\n","Epoch 93, Loss: 0.8729644417762756\n","Epoch 94, Loss: 0.8724247813224792\n","Epoch 95, Loss: 0.8718981742858887\n","Epoch 96, Loss: 0.8713844418525696\n","Epoch 97, Loss: 0.8708820343017578\n","Epoch 98, Loss: 0.8703904747962952\n","Epoch 99, Loss: 0.8699095845222473\n","Epoch 100, Loss: 0.8694393634796143\n"]}],"source":["# Training loop\n","num_epochs = 100\n","for epoch in range(num_epochs):\n","    model.train()\n","    optimizer.zero_grad()\n","    outputs = model(X_train_tensor)\n","    loss = criterion(outputs, y_train_tensor)\n","    loss.backward()\n","    optimizer.step()\n","    print(f\"Epoch {epoch+1}, Loss: {loss}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fziyD5uTbzSj","outputId":"ddf454ca-57e7-461a-c3a1-10657c7efd4d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy (WORD2VEC TERTIARY): 0.67886\n"]}],"source":["# Evaluate the model on the test set\n","model.eval()\n","with torch.no_grad():\n","    outputs = model(X_test_tensor)\n","    _, predicted = torch.max(outputs, 1)\n","    accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)\n","    print(\"Accuracy (WORD2VEC TERTIARY):\", accuracy)"]},{"cell_type":"markdown","metadata":{"id":"S00OotMMbzSj"},"source":["#####  **PRE_TRAINED BINARY**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vluuRb1sbzSj"},"outputs":[],"source":["# Convert the Word2Vec features to PyTorch tensors\n","X_train_tensor_binary = torch.tensor(X_train_pretrained_binary, dtype=torch.float32)\n","y_train_tensor_binary = torch.tensor(y_train_binary-1, dtype=torch.long)  # Assuming y_train_binary contains binary labels\n","X_test_tensor_binary = torch.tensor(X_test_pretrained_binary, dtype=torch.float32)\n","y_test_tensor_binary = torch.tensor(y_test_binary.values-1, dtype=torch.long)  # Convert Series to numpy array with .values attribute\n","\n","# Define hyperparameters\n","input_size = X_train_tensor_binary.shape[1]\n","hidden_size1 = 50\n","hidden_size2 = 10\n","output_size = 2  # Binary classification\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1YVyJc1QbzSk","outputId":"0b980985-4213-4bf3-c5de-e245cee37614"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([0, 1, 0,  ..., 0, 0, 0])\n"]}],"source":["print(y_train_tensor_binary)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fh2a3W46bzSk"},"outputs":[],"source":["# Create DataLoader for training\n","train_dataset_binary = TensorDataset(X_train_tensor_binary, y_train_tensor_binary)\n","train_loader_binary = DataLoader(train_dataset_binary, batch_size=64, shuffle=True)\n","\n","# Initialize the model, loss function, and optimizer\n","model = MLP(input_size, hidden_size1, hidden_size2, output_size)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-K6EwtdEbzSk","outputId":"4bbe5fa2-cc8c-450c-b560-fa23da294bf9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 0.6963824033737183\n","Epoch 2, Loss: 0.6959983706474304\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, Loss: 0.6956819891929626\n","Epoch 4, Loss: 0.695370078086853\n","Epoch 5, Loss: 0.6950533986091614\n","Epoch 6, Loss: 0.6947672963142395\n","Epoch 7, Loss: 0.6945233345031738\n","Epoch 8, Loss: 0.6942496299743652\n","Epoch 9, Loss: 0.6939220428466797\n","Epoch 10, Loss: 0.6935703754425049\n","Epoch 11, Loss: 0.6932117938995361\n","Epoch 12, Loss: 0.6928437948226929\n","Epoch 13, Loss: 0.692457914352417\n","Epoch 14, Loss: 0.6920475363731384\n","Epoch 15, Loss: 0.6916114687919617\n","Epoch 16, Loss: 0.6911475658416748\n","Epoch 17, Loss: 0.6906532645225525\n","Epoch 18, Loss: 0.6901236772537231\n","Epoch 19, Loss: 0.689552903175354\n","Epoch 20, Loss: 0.6889275312423706\n","Epoch 21, Loss: 0.6882386803627014\n","Epoch 22, Loss: 0.6874949932098389\n","Epoch 23, Loss: 0.6866989731788635\n","Epoch 24, Loss: 0.685836911201477\n","Epoch 25, Loss: 0.6848797798156738\n","Epoch 26, Loss: 0.6837737560272217\n","Epoch 27, Loss: 0.6825416088104248\n","Epoch 28, Loss: 0.6813512444496155\n","Epoch 29, Loss: 0.6801514029502869\n","Epoch 30, Loss: 0.678846538066864\n","Epoch 31, Loss: 0.6774090528488159\n","Epoch 32, Loss: 0.6758975386619568\n","Epoch 33, Loss: 0.6743893623352051\n","Epoch 34, Loss: 0.6728538870811462\n","Epoch 35, Loss: 0.6712594032287598\n","Epoch 36, Loss: 0.6695950627326965\n","Epoch 37, Loss: 0.6678584218025208\n","Epoch 38, Loss: 0.6660496592521667\n","Epoch 39, Loss: 0.664168119430542\n","Epoch 40, Loss: 0.6622152924537659\n","Epoch 41, Loss: 0.6601929664611816\n","Epoch 42, Loss: 0.6581037044525146\n","Epoch 43, Loss: 0.6559494137763977\n","Epoch 44, Loss: 0.6537325978279114\n","Epoch 45, Loss: 0.6514565348625183\n","Epoch 46, Loss: 0.6491228342056274\n","Epoch 47, Loss: 0.6467311978340149\n","Epoch 48, Loss: 0.6442824602127075\n","Epoch 49, Loss: 0.6417815685272217\n","Epoch 50, Loss: 0.6392343640327454\n","Epoch 51, Loss: 0.6366410255432129\n","Epoch 52, Loss: 0.6340004205703735\n","Epoch 53, Loss: 0.6313220262527466\n","Epoch 54, Loss: 0.6286217570304871\n","Epoch 55, Loss: 0.6259087920188904\n","Epoch 56, Loss: 0.6231865882873535\n","Epoch 57, Loss: 0.6204555630683899\n","Epoch 58, Loss: 0.6177158951759338\n","Epoch 59, Loss: 0.6149677038192749\n","Epoch 60, Loss: 0.6122135519981384\n","Epoch 61, Loss: 0.609459638595581\n","Epoch 62, Loss: 0.6067186594009399\n","Epoch 63, Loss: 0.6039997339248657\n","Epoch 64, Loss: 0.6013016700744629\n","Epoch 65, Loss: 0.5986260771751404\n","Epoch 66, Loss: 0.5959789752960205\n","Epoch 67, Loss: 0.593363881111145\n","Epoch 68, Loss: 0.5907707810401917\n","Epoch 69, Loss: 0.5881720781326294\n","Epoch 70, Loss: 0.5855847597122192\n","Epoch 71, Loss: 0.5830959677696228\n","Epoch 72, Loss: 0.580665647983551\n","Epoch 73, Loss: 0.5782763361930847\n","Epoch 74, Loss: 0.5759308934211731\n","Epoch 75, Loss: 0.5736322999000549\n","Epoch 76, Loss: 0.5713825225830078\n","Epoch 77, Loss: 0.5691856741905212\n","Epoch 78, Loss: 0.5670433640480042\n","Epoch 79, Loss: 0.5649531483650208\n","Epoch 80, Loss: 0.5629155039787292\n","Epoch 81, Loss: 0.5609331130981445\n","Epoch 82, Loss: 0.5590060353279114\n","Epoch 83, Loss: 0.557131290435791\n","Epoch 84, Loss: 0.5553088188171387\n","Epoch 85, Loss: 0.5535395741462708\n","Epoch 86, Loss: 0.5518217086791992\n","Epoch 87, Loss: 0.5501528978347778\n","Epoch 88, Loss: 0.548533022403717\n","Epoch 89, Loss: 0.5469614267349243\n","Epoch 90, Loss: 0.5454354882240295\n","Epoch 91, Loss: 0.5439538955688477\n","Epoch 92, Loss: 0.5425161719322205\n","Epoch 93, Loss: 0.5411204695701599\n","Epoch 94, Loss: 0.5397648215293884\n","Epoch 95, Loss: 0.5384487509727478\n","Epoch 96, Loss: 0.5371708273887634\n","Epoch 97, Loss: 0.5359287858009338\n","Epoch 98, Loss: 0.5347222089767456\n","Epoch 99, Loss: 0.5335499048233032\n","Epoch 100, Loss: 0.5324098467826843\n"]}],"source":["# Training loop\n","num_epochs = 100\n","for epoch in range(num_epochs):\n","    model.train()\n","    optimizer.zero_grad()\n","    outputs = model(X_train_tensor_binary)\n","    loss = criterion(outputs, y_train_tensor_binary)\n","    loss.backward()\n","    optimizer.step()\n","    print(f\"Epoch {epoch+1}, Loss: {loss}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KxxVyJXdbzSk","outputId":"b994b641-9edc-4691-9aa1-fce446d4a07b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy (WORD2VEC BINARY): 0.792575\n"]}],"source":["# Evaluate the model on the test set\n","model.eval()\n","with torch.no_grad():\n","    outputs = model(X_test_tensor_binary)\n","    _, predicted = torch.max(outputs, 1)\n","    accuracy = (predicted == y_test_tensor_binary).sum().item() / len(y_test_tensor_binary)\n","    print(\"Accuracy (WORD2VEC BINARY):\", accuracy)"]},{"cell_type":"markdown","metadata":{"id":"oHvzLR5qbzSl"},"source":["#####  **PRE_TRAINED TERTIARY**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TPhhv5habzSl"},"outputs":[],"source":["# Convert the Word2Vec features to PyTorch tensors\n","X_train_tensor = torch.tensor(X_train_pretrained, dtype=torch.float32)\n","y_train_tensor = torch.tensor(y_train-1, dtype=torch.long)  # Assuming y_train_binary contains binary labels\n","X_test_tensor = torch.tensor(X_test_pretrained, dtype=torch.float32)\n","y_test_tensor = torch.tensor(y_test.values-1, dtype=torch.long)  # Convert Series to numpy array with .values attribute\n","\n","# Define hyperparameters\n","input_size = X_train_tensor.shape[1]\n","hidden_size1 = 50\n","hidden_size2 = 10\n","output_size = 3  # Binary classification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FKLGnfr6bzSl","outputId":"3d4da02d-7978-41f6-e4e5-5d9afebd8f91"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0.0125,  0.0380,  0.0217,  ..., -0.0304,  0.0146, -0.0478],\n","        [-0.0041,  0.0431,  0.0086,  ..., -0.0256,  0.0155, -0.0352],\n","        [ 0.0405,  0.0625, -0.0175,  ..., -0.1562,  0.0019, -0.0698],\n","        ...,\n","        [ 0.0073,  0.0507,  0.0160,  ..., -0.0472,  0.0499,  0.0199],\n","        [ 0.0491,  0.0376,  0.0664,  ..., -0.0399,  0.0535, -0.0417],\n","        [ 0.0251,  0.0570, -0.0098,  ..., -0.0155,  0.0093, -0.0694]])\n","tensor([1, 1, 0,  ..., 2, 2, 2])\n"]}],"source":["print(X_train_tensor)\n","print(y_train_tensor)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rNUqxMdmbzSm"},"outputs":[],"source":["# Create DataLoader for training\n","train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","\n","# Initialize the model, loss function, and optimizer\n","model = MLP(input_size, hidden_size1, hidden_size2, output_size)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jtqkD4p1bzSm","outputId":"fc6e712e-5b79-4e44-db34-0c2719f5e5d6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 1.1097946166992188\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, Loss: 1.1086381673812866\n","Epoch 3, Loss: 1.1074906587600708\n","Epoch 4, Loss: 1.1063374280929565\n","Epoch 5, Loss: 1.1051570177078247\n","Epoch 6, Loss: 1.1039496660232544\n","Epoch 7, Loss: 1.102838158607483\n","Epoch 8, Loss: 1.1017757654190063\n","Epoch 9, Loss: 1.1006669998168945\n","Epoch 10, Loss: 1.0995147228240967\n","Epoch 11, Loss: 1.0983113050460815\n","Epoch 12, Loss: 1.0970594882965088\n","Epoch 13, Loss: 1.0957598686218262\n","Epoch 14, Loss: 1.0944137573242188\n","Epoch 15, Loss: 1.0930252075195312\n","Epoch 16, Loss: 1.0916029214859009\n","Epoch 17, Loss: 1.0901508331298828\n","Epoch 18, Loss: 1.0886669158935547\n","Epoch 19, Loss: 1.087151050567627\n","Epoch 20, Loss: 1.0856043100357056\n","Epoch 21, Loss: 1.0840296745300293\n","Epoch 22, Loss: 1.0824295282363892\n","Epoch 23, Loss: 1.0808048248291016\n","Epoch 24, Loss: 1.0791511535644531\n","Epoch 25, Loss: 1.0774600505828857\n","Epoch 26, Loss: 1.0757488012313843\n","Epoch 27, Loss: 1.074116826057434\n","Epoch 28, Loss: 1.0725889205932617\n","Epoch 29, Loss: 1.0711016654968262\n","Epoch 30, Loss: 1.0696390867233276\n","Epoch 31, Loss: 1.068204402923584\n","Epoch 32, Loss: 1.066801905632019\n","Epoch 33, Loss: 1.0654345750808716\n","Epoch 34, Loss: 1.0641052722930908\n","Epoch 35, Loss: 1.0628209114074707\n","Epoch 36, Loss: 1.0615869760513306\n","Epoch 37, Loss: 1.060400366783142\n","Epoch 38, Loss: 1.0592615604400635\n","Epoch 39, Loss: 1.058176040649414\n","Epoch 40, Loss: 1.057146668434143\n","Epoch 41, Loss: 1.056153416633606\n","Epoch 42, Loss: 1.055159568786621\n","Epoch 43, Loss: 1.0541553497314453\n","Epoch 44, Loss: 1.0531699657440186\n","Epoch 45, Loss: 1.052223563194275\n","Epoch 46, Loss: 1.0512994527816772\n","Epoch 47, Loss: 1.0503714084625244\n","Epoch 48, Loss: 1.049422264099121\n","Epoch 49, Loss: 1.048440933227539\n","Epoch 50, Loss: 1.04742431640625\n","Epoch 51, Loss: 1.046389102935791\n","Epoch 52, Loss: 1.0453542470932007\n","Epoch 53, Loss: 1.0443214178085327\n","Epoch 54, Loss: 1.0432815551757812\n","Epoch 55, Loss: 1.0422247648239136\n","Epoch 56, Loss: 1.041145920753479\n","Epoch 57, Loss: 1.0400452613830566\n","Epoch 58, Loss: 1.038927674293518\n","Epoch 59, Loss: 1.0377963781356812\n","Epoch 60, Loss: 1.0366452932357788\n","Epoch 61, Loss: 1.035465121269226\n","Epoch 62, Loss: 1.0342587232589722\n","Epoch 63, Loss: 1.0330517292022705\n","Epoch 64, Loss: 1.0318396091461182\n","Epoch 65, Loss: 1.0306047201156616\n","Epoch 66, Loss: 1.0293430089950562\n","Epoch 67, Loss: 1.0280547142028809\n","Epoch 68, Loss: 1.0267415046691895\n","Epoch 69, Loss: 1.0254050493240356\n","Epoch 70, Loss: 1.024046540260315\n","Epoch 71, Loss: 1.0226666927337646\n","Epoch 72, Loss: 1.0212644338607788\n","Epoch 73, Loss: 1.0198378562927246\n","Epoch 74, Loss: 1.0183855295181274\n","Epoch 75, Loss: 1.016910433769226\n","Epoch 76, Loss: 1.0154184103012085\n","Epoch 77, Loss: 1.0139113664627075\n","Epoch 78, Loss: 1.0123887062072754\n","Epoch 79, Loss: 1.010850429534912\n","Epoch 80, Loss: 1.0092965364456177\n","Epoch 81, Loss: 1.0077276229858398\n","Epoch 82, Loss: 1.0061450004577637\n","Epoch 83, Loss: 1.0045493841171265\n","Epoch 84, Loss: 1.0029422044754028\n","Epoch 85, Loss: 1.0013248920440674\n","Epoch 86, Loss: 0.9996989965438843\n","Epoch 87, Loss: 0.9980658888816833\n","Epoch 88, Loss: 0.9964264035224915\n","Epoch 89, Loss: 0.9947822093963623\n","Epoch 90, Loss: 0.9931347370147705\n","Epoch 91, Loss: 0.9914857149124146\n","Epoch 92, Loss: 0.989837110042572\n","Epoch 93, Loss: 0.9881906509399414\n","Epoch 94, Loss: 0.9865477085113525\n","Epoch 95, Loss: 0.984910249710083\n","Epoch 96, Loss: 0.9832792282104492\n","Epoch 97, Loss: 0.9816566109657288\n","Epoch 98, Loss: 0.9800436496734619\n","Epoch 99, Loss: 0.978442370891571\n","Epoch 100, Loss: 0.9768534898757935\n"]}],"source":["# Training loop\n","num_epochs = 100\n","for epoch in range(num_epochs):\n","    model.train()\n","    optimizer.zero_grad()\n","    outputs = model(X_train_tensor)\n","    loss = criterion(outputs, y_train_tensor)\n","    loss.backward()\n","    optimizer.step()\n","    print(f\"Epoch {epoch+1}, Loss: {loss}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BRF_VRXKbzSm","outputId":"8a240d9e-8e50-4479-e027-b770d78ea633"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy (WORD2VEC TERTIARY): 0.61564\n"]}],"source":["# Evaluate the model on the test set\n","model.eval()\n","with torch.no_grad():\n","    outputs = model(X_test_tensor)\n","    _, predicted = torch.max(outputs, 1)\n","    accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)\n","    print(\"Accuracy (WORD2VEC TERTIARY):\", accuracy)"]},{"cell_type":"markdown","metadata":{"id":"6fYpE0TvbzSn"},"source":["### **CONCAT METHOD**"]},{"cell_type":"markdown","metadata":{"id":"CZoDx9YBbzSn"},"source":["#### **WORD2VEC BINARY**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1CdvCCFpbzSn","outputId":"48c79dcc-5fe5-4e8a-f950-018ce6b97a15"},"outputs":[{"name":"stdout","output_type":"stream","text":["(1, 10, 300)\n"]}],"source":["x = np.array([X_train_word2vec_binary[:10]])\n","# x = X_train_word2vec_binary\n","print(x.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"atmiW2oZbzSn","outputId":"d3d4e1a0-9dd9-43b9-8a93-26a0a474f9e7"},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train:\n","75381     Good for printing \\\\\"internal\\\\\" documents tha...\n","65569                    These sticky notes are very small.\n","163473    Good quality for the price. Holds quite a bit ...\n","90518     two colors leaked, wouldn't work and one feels...\n","138866    As usual, when you get a bargain product, you ...\n","                                ...                        \n","119879    Tape cracks too easily even when unrolling tap...\n","103694    Ink cannot last 2 months. Original can use for...\n","131932    The pen is great I just wish it had normal col...\n","146867    IT was very easy to install and I got it up an...\n","121958    These cartridges are quite inexpensive compare...\n","Name: review_body, Length: 200000, dtype: object\n","X_train_word2vec_concat:\n","[[-0.75934184 -0.26071328 -0.1153479  ...  0.60092944 -0.6419013\n","  -0.50474095]\n"," [ 0.25292343 -0.63952917  0.024776   ... -0.22854434  0.8567886\n","  -0.76089936]\n"," [-0.06805354  0.4369895  -0.29323617 ...  0.2717982  -0.17278934\n","  -0.33616853]\n"," ...\n"," [-0.4412589   0.36281526  0.9165349  ... -0.44698372 -0.20175739\n","   0.2245328 ]\n"," [-0.70799077 -0.26589832  0.76164436 ... -0.03134464 -0.24354856\n","  -0.4309112 ]\n"," [ 0.39295706 -0.3895449   0.05441349 ...  0.1497356   0.07423248\n","  -0.5706169 ]]\n","X_train_binary:\n","203248    This calendar rocks. Have had one the last cou...\n","67802     Color ink was dried out a pone arrival. though...\n","198889    I mistakenly opted for the higher milliAmp hou...\n","153093    I've tried several lesson plan books and grade...\n","154681    [[VIDEOID:mo3TKS7DZ1NAZY9]]UPDATE Oct 2010: I ...\n","                                ...                        \n","169879     these cloths are good for glasses,and sunglasses\n","153694    This is a great voice recorder. However, it wo...\n","181932    I unpacked this product, inserted the ink cart...\n","196867    I really like these little clips (they are ver...\n","171958    good audio.<br />Easy link to our cell phones ...\n","Name: review_body, Length: 160000, dtype: object\n","X_train_word2vec_concat_binary:\n","[[ 0.11160541  1.0992686  -0.29012793 ...  0.06769264  0.451941\n","  -0.23758169]\n"," [-0.02508275 -0.03989909 -0.33501947 ... -0.2923721  -0.4933513\n","  -0.2611604 ]\n"," [-0.7024362   0.5855692  -0.23958656 ...  0.11012936 -0.45363152\n","   0.04325382]\n"," ...\n"," [-0.44593668  0.5151983   0.54883856 ...  0.86591226 -0.9474575\n","  -0.11500883]\n"," [ 0.3429993  -0.13824537  0.5819689  ... -0.29405493  0.36854145\n","   0.09087314]\n"," [ 0.16054232 -0.37476406 -0.71712273 ...  0.55348164  0.02152144\n","  -0.57505345]]\n"]}],"source":["def get_word2vec_features_concat(review_text):\n","  feature_vector = np.zeros((word2vec_model.vector_size,), dtype=\"float32\")\n","  num_words = 0\n","  for i, word in enumerate(review_text):\n","    if word in word2vec_model.wv and i < 10:  # Limit to first 10 words\n","      feature_vector = np.add(feature_vector, word2vec_model.wv[word])\n","      num_words += 1\n","  if num_words != 0:\n","    feature_vector = np.divide(feature_vector, num_words)\n","  return feature_vector\n","\n","# Represent each example with Word2Vec features\n","X_train_word2vec_concat = np.array([get_word2vec_features_concat(review) for review in tokenized_reviews])\n","print(\"X_train:\")\n","print(X_train)\n","print('X_train_word2vec_concat:')\n","print(X_train_word2vec_concat)\n","\n","X_train_word2vec_concat_binary = np.array([get_word2vec_features_concat(review) for review in tokenized_reviews_binary])\n","print(\"X_train_binary:\")\n","print(X_train_binary)\n","print('X_train_word2vec_concat_binary:')\n","print(X_train_word2vec_concat_binary)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bPdAiUzMbzSo","outputId":"0eb237ef-c355-41d6-80a7-e3c6782b5b52"},"outputs":[{"data":{"text/plain":["(200000, 300)"]},"execution_count":78,"metadata":{},"output_type":"execute_result"}],"source":["X_train_word2vec_concat.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pp9lIFDFbzSo","outputId":"5fa3af7b-7cce-4f47-91ef-d309efd3d9c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["X_test:\n","38683     I bought this printer for my Business, and it ...\n","64939     ...if it lasted. I'm glad I waited to write th...\n","3954      We purchased and installed this printer for a ...\n","120374    Everything that I have put into this machine, ...\n","172861    ordered as gift. recipient said it was great f...\n","                                ...                        \n","179545    the way you see those folders looking on the p...\n","222647                                  i'll buy again soon\n","171823    staff was helpful and tried very hard to make ...\n","135782                                       It's a fun pen\n","208380    This twine is thin but strong and is perfect f...\n","Name: review_body, Length: 50000, dtype: object\n","X_test_word2vec_concat:\n","[[-0.49564967  0.6754187   0.7017226  ...  0.5581657  -0.21785626\n","  -0.79002684]\n"," [-0.01967601  0.2927174   0.4976541  ...  0.09664098 -0.84558785\n","  -0.52363837]\n"," [-0.64603937  0.1970729   0.2222267  ...  0.5787168  -0.18215156\n","  -0.8590026 ]\n"," ...\n"," [-0.44005466 -1.0092087   0.45311767 ... -0.26107606  0.14324935\n","  -0.41561994]\n"," [-0.5998161  -0.2791256   1.1577679  ...  0.47636938 -0.25175697\n","  -0.17114687]\n"," [-0.4111878  -1.1172986   0.29387027 ...  0.21670623  0.6403555\n","   0.502069  ]]\n","X_test_binary:\n","169737                                               Thanks\n","72272     I had my hp for about 7 years until that broke...\n","208154                  Worked for my phone.  Much cheaper!\n","65426     A very good phone, but the handset freezes and...\n","30074     First, the scanner is fine, enough to scan a 8...\n","                                ...                        \n","4174      Horrible phone.  It's very lightweight and fee...\n","91537     As you can see from other reviews, this HP Mul...\n","206449    Undeniably an attractive  feature for my store...\n","234376    This mouse pad is better in person!  It is hig...\n","6584      Cheap is a great word for these. They are abso...\n","Name: review_body, Length: 40000, dtype: object\n","X_test_word2vec_concat_binary:\n","[[-1.2108998   0.5108104   1.5523907  ... -0.5143207   1.1485553\n","  -1.5499436 ]\n"," [ 0.01802554  0.7395651  -0.15074003 ... -0.15693423 -0.2744637\n","  -0.02262963]\n"," [-0.01103263  0.20817432 -0.35120422 ...  0.26778355  0.2200028\n","  -0.9391627 ]\n"," ...\n"," [-0.32780966 -0.35901725 -0.32526678 ...  0.89415956 -0.22289875\n","  -0.8779451 ]\n"," [ 0.2755072  -0.13127977  1.1359109  ... -0.13104723  0.32742313\n","   0.27027172]\n"," [ 0.28005925 -0.371653    0.09708144 ... -0.17396727  0.40888518\n","  -0.54067236]]\n"]}],"source":["tokenized_reviews_test = [nltk.word_tokenize(review) for review in X_test]\n","tokenized_reviews_test_binary = [nltk.word_tokenize(review) for review in X_test_binary]\n","\n","# Represent each example with Word2Vec features\n","X_test_word2vec_concat = np.array([get_word2vec_features_concat(review) for review in tokenized_reviews_test])\n","print(\"X_test:\")\n","print(X_test)\n","print('X_test_word2vec_concat:')\n","print(X_test_word2vec_concat)\n","\n","X_test_word2vec_concat_binary = np.array([get_word2vec_features_concat(review) for review in tokenized_reviews_test_binary])\n","print(\"X_test_binary:\")\n","print(X_test_binary)\n","print('X_test_word2vec_concat_binary:')\n","print(X_test_word2vec_concat_binary)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EkRkpNBtbzSo","outputId":"c5635fbb-1c62-4bc9-a3f6-9623682bce38"},"outputs":[{"data":{"text/plain":["(40000, 300)"]},"execution_count":80,"metadata":{},"output_type":"execute_result"}],"source":["X_test_word2vec_concat_binary.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-DqAFjFhbzSp","outputId":"2fd0b5fd-e799-409f-c044-1ccc2f5bd89c"},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train:\n","75381     Good for printing \\\\\"internal\\\\\" documents tha...\n","65569                    These sticky notes are very small.\n","163473    Good quality for the price. Holds quite a bit ...\n","90518     two colors leaked, wouldn't work and one feels...\n","138866    As usual, when you get a bargain product, you ...\n","                                ...                        \n","119879    Tape cracks too easily even when unrolling tap...\n","103694    Ink cannot last 2 months. Original can use for...\n","131932    The pen is great I just wish it had normal col...\n","146867    IT was very easy to install and I got it up an...\n","121958    These cartridges are quite inexpensive compare...\n","Name: review_body, Length: 200000, dtype: object\n","X_train_pretrained_concat:\n","[[ 0.04692586  0.01741537 -0.02351888 ... -0.1127065  -0.02482096\n","   0.02710978]\n"," [-0.01839193  0.08548991 -0.02026367 ...  0.08823649  0.06197103\n","   0.03828939]\n"," [ 0.08663177  0.04251099 -0.06643677 ... -0.07038116  0.07638216\n","   0.03092194]\n"," ...\n"," [-0.0145752   0.08721447  0.02330322 ...  0.00130005 -0.0020752\n","  -0.04778137]\n"," [ 0.0586853   0.02579284 -0.01300049 ... -0.02672577  0.06678772\n","  -0.0072937 ]\n"," [-0.04066298  0.11420356  0.05430434 ... -0.00880263  0.05834961\n","   0.08483887]]\n","X_train_binary:\n","203248    This calendar rocks. Have had one the last cou...\n","67802     Color ink was dried out a pone arrival. though...\n","198889    I mistakenly opted for the higher milliAmp hou...\n","153093    I've tried several lesson plan books and grade...\n","154681    [[VIDEOID:mo3TKS7DZ1NAZY9]]UPDATE Oct 2010: I ...\n","                                ...                        \n","169879     these cloths are good for glasses,and sunglasses\n","153694    This is a great voice recorder. However, it wo...\n","181932    I unpacked this product, inserted the ink cart...\n","196867    I really like these little clips (they are ver...\n","171958    good audio.<br />Easy link to our cell phones ...\n","Name: review_body, Length: 160000, dtype: object\n","X_train_pretrained_concat_binary:\n","[[ 0.00838216  0.03869968 -0.01325141 ... -0.12575954 -0.03513591\n","  -0.07570987]\n"," [ 0.03896332  0.11447144 -0.02932739 ... -0.00567627 -0.02555847\n","  -0.01315308]\n"," [ 0.00169754  0.0132637   0.04131317 ...  0.0124588  -0.02471924\n","   0.03318787]\n"," ...\n"," [-0.02640533  0.20225143  0.02890015 ...  0.03025436 -0.03327942\n","  -0.08462667]\n"," [ 0.04100206  0.08048163  0.00455221 ... -0.00067139  0.06385633\n","  -0.01171875]\n"," [-0.11617025  0.02766927 -0.05857341 ... -0.13081868 -0.02482478\n","  -0.0625    ]]\n"]}],"source":["def get_pretrained_features_concat(review_text):\n","    feature_vector = np.zeros((pretrained_model.vector_size,), dtype=\"float32\")\n","    num_words = 0\n","    for i, word in enumerate(review_text):\n","        if word in pretrained_model and i < 10: # limit to first 10\n","            feature_vector += pretrained_model[word]\n","            num_words += 1\n","    if num_words != 0:\n","        feature_vector /= num_words\n","    return feature_vector\n","\n","# Represent each example with Word2Vec features\n","X_train_pretrained_concat = np.array([get_pretrained_features_concat(review) for review in tokenized_reviews])\n","print(\"X_train:\")\n","print(X_train)\n","print('X_train_pretrained_concat:')\n","print(X_train_pretrained_concat)\n","\n","X_train_pretrained_concat_binary = np.array([get_pretrained_features_concat(review) for review in tokenized_reviews_binary])\n","print(\"X_train_binary:\")\n","print(X_train_binary)\n","print('X_train_pretrained_concat_binary:')\n","print(X_train_pretrained_concat_binary)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9A1oKViYbzSp","outputId":"e1c76f74-aebf-42d9-e5f3-dde3608fb808"},"outputs":[{"name":"stdout","output_type":"stream","text":["X_test:\n","38683     I bought this printer for my Business, and it ...\n","64939     ...if it lasted. I'm glad I waited to write th...\n","3954      We purchased and installed this printer for a ...\n","120374    Everything that I have put into this machine, ...\n","172861    ordered as gift. recipient said it was great f...\n","                                ...                        \n","179545    the way you see those folders looking on the p...\n","222647                                  i'll buy again soon\n","171823    staff was helpful and tried very hard to make ...\n","135782                                       It's a fun pen\n","208380    This twine is thin but strong and is perfect f...\n","Name: review_body, Length: 50000, dtype: object\n","X_test_pretrained_concat:\n","[[ 0.06853485 -0.01130891 -0.00944519 ... -0.00183487  0.0769043\n","  -0.07565308]\n"," [ 0.05960083  0.02112746  0.07992554 ... -0.03105164  0.07575226\n","  -0.0617981 ]\n"," [-0.03662981  0.06673758 -0.03023856 ... -0.04982648  0.01924133\n","  -0.15809849]\n"," ...\n"," [ 0.04202652  0.03329372  0.03141022 ... -0.00953674  0.06111145\n","  -0.01318359]\n"," [-0.02977498  0.04752604 -0.03792318 ...  0.05305989 -0.09895834\n","  -0.03369141]\n"," [-0.03634983  0.05224609  0.03599718 ... -0.04201932 -0.04842122\n","  -0.03770616]]\n","X_train_binary:\n","169737                                               Thanks\n","72272     I had my hp for about 7 years until that broke...\n","208154                  Worked for my phone.  Much cheaper!\n","65426     A very good phone, but the handset freezes and...\n","30074     First, the scanner is fine, enough to scan a 8...\n","                                ...                        \n","4174      Horrible phone.  It's very lightweight and fee...\n","91537     As you can see from other reviews, this HP Mul...\n","206449    Undeniably an attractive  feature for my store...\n","234376    This mouse pad is better in person!  It is hig...\n","6584      Cheap is a great word for these. They are abso...\n","Name: review_body, Length: 40000, dtype: object\n","X_test_pretrained_concat_binary:\n","[[-0.3046875   0.20703125 -0.00390625 ...  0.08886719  0.04125977\n","   0.19238281]\n"," [ 0.07800293 -0.00306091  0.05956421 ... -0.00079041 -0.01992836\n","  -0.07976075]\n"," [ 0.02268473  0.00467936 -0.01866659 ... -0.09284464  0.02290853\n","   0.02429199]\n"," ...\n"," [-0.00289578  0.09198676 -0.0223253  ... -0.0625746   0.08059353\n","   0.01929389]\n"," [-0.04454888  0.01077779  0.04248047 ...  0.03143989 -0.02861192\n","  -0.01737467]\n"," [ 0.0125618   0.03167725  0.0436554  ... -0.04179382  0.07652283\n","   0.02340698]]\n"]}],"source":["# Represent each example with Pretrained features\n","X_test_pretrained_concat = np.array([get_pretrained_features_concat(review) for review in tokenized_reviews_test])\n","print(\"X_test:\")\n","print(X_test)\n","print('X_test_pretrained_concat:')\n","print(X_test_pretrained_concat)\n","\n","X_test_pretrained_concat_binary = np.array([get_pretrained_features_concat(review) for review in tokenized_reviews_test_binary])\n","print(\"X_train_binary:\")\n","print(X_test_binary)\n","print('X_test_pretrained_concat_binary:')\n","print(X_test_pretrained_concat_binary)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jKsi79H-bzSp","outputId":"8208e005-0000-4bf0-eb2d-2820c941a893"},"outputs":[{"data":{"text/plain":["(160000, 300)"]},"execution_count":84,"metadata":{},"output_type":"execute_result"}],"source":["X_train_pretrained_concat_binary.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o1tetgVUbzSq"},"outputs":[],"source":["# Convert the Word2Vec features to PyTorch tensors\n","X_train_tensor_binary = torch.tensor(X_train_word2vec_concat_binary, dtype=torch.float32)\n","y_train_tensor_binary = torch.tensor(y_train_binary.values - 1, dtype=torch.long)\n","X_test_tensor_binary = torch.tensor(X_test_word2vec_concat_binary, dtype=torch.float32)\n","y_test_tensor_binary = torch.tensor(y_test_binary.values - 1, dtype=torch.long)\n","\n","# Define hyperparameters\n","input_size_binary = X_train_tensor_binary.shape[1]\n","hidden_size1 = 50\n","hidden_size2 = 10\n","output_size = 2  # Binary classification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2qYbFV2ZbzSq","outputId":"f3626aac-5b46-4787-d40c-34d65cdfa890"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([160000])\n","torch.Size([160000, 300])\n"]}],"source":["print(y_train_tensor_binary.shape)\n","print(X_train_tensor_binary.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ek5lpGSbbzSq"},"outputs":[],"source":["# Create DataLoader for training\n","train_dataset_binary = TensorDataset(X_train_tensor_binary, y_train_tensor_binary)\n","train_loader_binary = DataLoader(train_dataset_binary, batch_size=64, shuffle=True)\n","\n","# Initialize the model, loss function, and optimizer\n","model = MLP(input_size_binary, hidden_size1, hidden_size2, output_size)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pqLCXa5gbzSq","outputId":"d9cae0d2-86b3-417b-b810-5676d918e34c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 0.6937284469604492\n","Epoch 2, Loss: 0.6905949711799622\n","Epoch 3, Loss: 0.6874700784683228\n","Epoch 4, Loss: 0.6843757033348083\n","Epoch 5, Loss: 0.6812653541564941\n","Epoch 6, Loss: 0.6780884265899658\n","Epoch 7, Loss: 0.6748496294021606\n","Epoch 8, Loss: 0.6715579032897949\n","Epoch 9, Loss: 0.6682333946228027\n","Epoch 10, Loss: 0.6648876667022705\n","Epoch 11, Loss: 0.6615219116210938\n","Epoch 12, Loss: 0.6581389307975769\n","Epoch 13, Loss: 0.6547418236732483\n","Epoch 14, Loss: 0.6513332724571228\n","Epoch 15, Loss: 0.647918701171875\n","Epoch 16, Loss: 0.6444942355155945\n","Epoch 17, Loss: 0.6410764455795288\n","Epoch 18, Loss: 0.6376779079437256\n","Epoch 19, Loss: 0.6343069076538086\n","Epoch 20, Loss: 0.6309748888015747\n","Epoch 21, Loss: 0.6276821494102478\n","Epoch 22, Loss: 0.6244305372238159\n","Epoch 23, Loss: 0.6212145090103149\n","Epoch 24, Loss: 0.6180214881896973\n","Epoch 25, Loss: 0.6148437261581421\n","Epoch 26, Loss: 0.6116703152656555\n","Epoch 27, Loss: 0.6085010170936584\n","Epoch 28, Loss: 0.6053378582000732\n","Epoch 29, Loss: 0.6021886467933655\n","Epoch 30, Loss: 0.5990654826164246\n","Epoch 31, Loss: 0.5959776043891907\n","Epoch 32, Loss: 0.592939019203186\n","Epoch 33, Loss: 0.5899701714515686\n","Epoch 34, Loss: 0.5870846509933472\n","Epoch 35, Loss: 0.5842989087104797\n","Epoch 36, Loss: 0.5816235542297363\n","Epoch 37, Loss: 0.5790597796440125\n","Epoch 38, Loss: 0.5766086578369141\n","Epoch 39, Loss: 0.5742701888084412\n","Epoch 40, Loss: 0.5720463991165161\n","Epoch 41, Loss: 0.5699340105056763\n","Epoch 42, Loss: 0.5679319500923157\n","Epoch 43, Loss: 0.5660403966903687\n","Epoch 44, Loss: 0.5642569661140442\n","Epoch 45, Loss: 0.5625784397125244\n","Epoch 46, Loss: 0.5610013008117676\n","Epoch 47, Loss: 0.5595192909240723\n","Epoch 48, Loss: 0.5581275820732117\n","Epoch 49, Loss: 0.5568227767944336\n","Epoch 50, Loss: 0.5556015372276306\n","Epoch 51, Loss: 0.5544593930244446\n","Epoch 52, Loss: 0.5533916354179382\n","Epoch 53, Loss: 0.5523939728736877\n","Epoch 54, Loss: 0.5514612793922424\n","Epoch 55, Loss: 0.5505896806716919\n","Epoch 56, Loss: 0.5497756600379944\n","Epoch 57, Loss: 0.5490143299102783\n","Epoch 58, Loss: 0.5483006834983826\n","Epoch 59, Loss: 0.5476303696632385\n","Epoch 60, Loss: 0.5469997525215149\n","Epoch 61, Loss: 0.5464050769805908\n","Epoch 62, Loss: 0.5458424687385559\n","Epoch 63, Loss: 0.5453077554702759\n","Epoch 64, Loss: 0.5447977781295776\n","Epoch 65, Loss: 0.5443097949028015\n","Epoch 66, Loss: 0.5438421368598938\n","Epoch 67, Loss: 0.5433927774429321\n","Epoch 68, Loss: 0.5429602861404419\n","Epoch 69, Loss: 0.5425432920455933\n","Epoch 70, Loss: 0.5421412587165833\n","Epoch 71, Loss: 0.5417540073394775\n","Epoch 72, Loss: 0.5413804054260254\n","Epoch 73, Loss: 0.5410203337669373\n","Epoch 74, Loss: 0.5406739711761475\n","Epoch 75, Loss: 0.5403406023979187\n","Epoch 76, Loss: 0.5400200486183167\n","Epoch 77, Loss: 0.539711594581604\n","Epoch 78, Loss: 0.53941410779953\n","Epoch 79, Loss: 0.5391265153884888\n","Epoch 80, Loss: 0.5388479232788086\n","Epoch 81, Loss: 0.5385780334472656\n","Epoch 82, Loss: 0.5383154153823853\n","Epoch 83, Loss: 0.5380592942237854\n","Epoch 84, Loss: 0.5378094911575317\n","Epoch 85, Loss: 0.5375656485557556\n","Epoch 86, Loss: 0.5373274683952332\n","Epoch 87, Loss: 0.5370944142341614\n","Epoch 88, Loss: 0.5368661880493164\n","Epoch 89, Loss: 0.5366424918174744\n","Epoch 90, Loss: 0.5364231467247009\n","Epoch 91, Loss: 0.5362074971199036\n","Epoch 92, Loss: 0.535995364189148\n","Epoch 93, Loss: 0.5357861518859863\n","Epoch 94, Loss: 0.5355798006057739\n","Epoch 95, Loss: 0.5353761911392212\n","Epoch 96, Loss: 0.5351747274398804\n","Epoch 97, Loss: 0.534974992275238\n","Epoch 98, Loss: 0.5347766876220703\n","Epoch 99, Loss: 0.5345800518989563\n","Epoch 100, Loss: 0.5343846082687378\n"]}],"source":["#Training Loop\n","num_epochs = 100\n","for epoch in range(num_epochs):\n","    model.train()\n","    optimizer.zero_grad()\n","    outputs = model(X_train_tensor_binary)\n","    loss = criterion(outputs, y_train_tensor_binary)\n","    loss.backward()\n","    optimizer.step()\n","    print(f\"Epoch {epoch+1}, Loss: {loss}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YgbM-BmvbzSr","outputId":"05bf5fec-b975-4af8-a80f-8b28520a2935"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy (WORD2VEC BINARY): 0.76315\n"]}],"source":["# Evaluate the model on the test set\n","model.eval()\n","with torch.no_grad():\n","    outputs = model(X_test_tensor_binary)\n","    _, predicted = torch.max(outputs, 1)\n","    accuracy = (predicted == y_test_tensor_binary).sum().item() / len(y_test_tensor_binary)\n","    print(\"Accuracy (WORD2VEC BINARY):\", accuracy)"]},{"cell_type":"markdown","metadata":{"id":"FeKCovmLbzSr"},"source":["#### **PRE TRAINED BINARY**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9jUZpepFbzSr"},"outputs":[],"source":["# Convert the Word2Vec features to PyTorch tensors\n","X_train_tensor_binary = torch.tensor(X_train_pretrained_concat_binary, dtype=torch.float32)\n","y_train_tensor_binary = torch.tensor(y_train_binary.values - 1, dtype=torch.long)\n","X_test_tensor_binary = torch.tensor(X_test_pretrained_concat_binary, dtype=torch.float32)\n","y_test_tensor_binary = torch.tensor(y_test_binary.values - 1, dtype=torch.long)\n","\n","# Define hyperparameters\n","input_size_binary = X_train_tensor_binary.shape[1]\n","hidden_size1 = 50\n","hidden_size2 = 10\n","output_size = 2  # Binary classification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PF_oSMwebzSr"},"outputs":[],"source":["# Create DataLoader for training\n","train_dataset_binary = TensorDataset(X_train_tensor_binary, y_train_tensor_binary)\n","train_loader_binary = DataLoader(train_dataset_binary, batch_size=64, shuffle=True)\n","\n","# Initialize the model, loss function, and optimizer\n","model = MLP(input_size_binary, hidden_size1, hidden_size2, output_size)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rr709Q_lbzSs","outputId":"c1c60d81-6fc1-4d59-9781-ec82d280cba3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 0.6937354207038879\n","Epoch 2, Loss: 0.6933442950248718\n","Epoch 3, Loss: 0.6929532885551453\n","Epoch 4, Loss: 0.6925543546676636\n","Epoch 5, Loss: 0.6921371817588806\n","Epoch 6, Loss: 0.6917063593864441\n","Epoch 7, Loss: 0.6912907958030701\n","Epoch 8, Loss: 0.6908850073814392\n","Epoch 9, Loss: 0.6904476284980774\n","Epoch 10, Loss: 0.6899695992469788\n","Epoch 11, Loss: 0.689461350440979\n","Epoch 12, Loss: 0.688934862613678\n","Epoch 13, Loss: 0.6883927583694458\n","Epoch 14, Loss: 0.6878263354301453\n","Epoch 15, Loss: 0.6872265338897705\n","Epoch 16, Loss: 0.6865859031677246\n","Epoch 17, Loss: 0.6859007477760315\n","Epoch 18, Loss: 0.6851728558540344\n","Epoch 19, Loss: 0.6844083666801453\n","Epoch 20, Loss: 0.6836192607879639\n","Epoch 21, Loss: 0.6828117966651917\n","Epoch 22, Loss: 0.6819773316383362\n","Epoch 23, Loss: 0.6811069846153259\n","Epoch 24, Loss: 0.6801964640617371\n","Epoch 25, Loss: 0.6792457103729248\n","Epoch 26, Loss: 0.6782565116882324\n","Epoch 27, Loss: 0.6772310137748718\n","Epoch 28, Loss: 0.6761707663536072\n","Epoch 29, Loss: 0.6750749945640564\n","Epoch 30, Loss: 0.6739424467086792\n","Epoch 31, Loss: 0.6727706789970398\n","Epoch 32, Loss: 0.6715576648712158\n","Epoch 33, Loss: 0.6703025102615356\n","Epoch 34, Loss: 0.6690050363540649\n","Epoch 35, Loss: 0.6676653027534485\n","Epoch 36, Loss: 0.6662836670875549\n","Epoch 37, Loss: 0.6648601293563843\n","Epoch 38, Loss: 0.6633954048156738\n","Epoch 39, Loss: 0.6618901491165161\n","Epoch 40, Loss: 0.6603457927703857\n","Epoch 41, Loss: 0.6587641835212708\n","Epoch 42, Loss: 0.6571465730667114\n","Epoch 43, Loss: 0.6554944515228271\n","Epoch 44, Loss: 0.6538097262382507\n","Epoch 45, Loss: 0.6520951390266418\n","Epoch 46, Loss: 0.650353729724884\n","Epoch 47, Loss: 0.6485872268676758\n","Epoch 48, Loss: 0.6467968821525574\n","Epoch 49, Loss: 0.644984245300293\n","Epoch 50, Loss: 0.6431515216827393\n","Epoch 51, Loss: 0.6413019299507141\n","Epoch 52, Loss: 0.6394389867782593\n","Epoch 53, Loss: 0.6375664472579956\n","Epoch 54, Loss: 0.635688304901123\n","Epoch 55, Loss: 0.6338077783584595\n","Epoch 56, Loss: 0.6319279074668884\n","Epoch 57, Loss: 0.6300516128540039\n","Epoch 58, Loss: 0.628182053565979\n","Epoch 59, Loss: 0.6263221502304077\n","Epoch 60, Loss: 0.6244751811027527\n","Epoch 61, Loss: 0.6226440668106079\n","Epoch 62, Loss: 0.6208323240280151\n","Epoch 63, Loss: 0.6190425157546997\n","Epoch 64, Loss: 0.6172780990600586\n","Epoch 65, Loss: 0.6155413389205933\n","Epoch 66, Loss: 0.61383455991745\n","Epoch 67, Loss: 0.612159788608551\n","Epoch 68, Loss: 0.6105185747146606\n","Epoch 69, Loss: 0.6089125275611877\n","Epoch 70, Loss: 0.6073428988456726\n","Epoch 71, Loss: 0.6058103442192078\n","Epoch 72, Loss: 0.604315996170044\n","Epoch 73, Loss: 0.6028597950935364\n","Epoch 74, Loss: 0.6014421582221985\n","Epoch 75, Loss: 0.6000629663467407\n","Epoch 76, Loss: 0.5987221002578735\n","Epoch 77, Loss: 0.5974189043045044\n","Epoch 78, Loss: 0.5961539149284363\n","Epoch 79, Loss: 0.5949276089668274\n","Epoch 80, Loss: 0.5937389135360718\n","Epoch 81, Loss: 0.5925867557525635\n","Epoch 82, Loss: 0.5914707779884338\n","Epoch 83, Loss: 0.5903893113136292\n","Epoch 84, Loss: 0.5893418192863464\n","Epoch 85, Loss: 0.5883274674415588\n","Epoch 86, Loss: 0.5873456597328186\n","Epoch 87, Loss: 0.5863953828811646\n","Epoch 88, Loss: 0.5854759812355042\n","Epoch 89, Loss: 0.5845860838890076\n","Epoch 90, Loss: 0.5837249755859375\n","Epoch 91, Loss: 0.5828912854194641\n","Epoch 92, Loss: 0.5820841193199158\n","Epoch 93, Loss: 0.5813024044036865\n","Epoch 94, Loss: 0.5805452466011047\n","Epoch 95, Loss: 0.5798118114471436\n","Epoch 96, Loss: 0.5791012644767761\n","Epoch 97, Loss: 0.5784127116203308\n","Epoch 98, Loss: 0.5777451992034912\n","Epoch 99, Loss: 0.5770978927612305\n","Epoch 100, Loss: 0.5764699578285217\n"]}],"source":["#Training Loop\n","num_epochs = 100\n","for epoch in range(num_epochs):\n","    model.train()\n","    optimizer.zero_grad()\n","    outputs = model(X_train_tensor_binary)\n","    loss = criterion(outputs, y_train_tensor_binary)\n","    loss.backward()\n","    optimizer.step()\n","    print(f\"Epoch {epoch+1}, Loss: {loss}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1YH4u3OsbzSs","outputId":"a917b8e2-92c7-4bfa-a008-6e4ca9687851"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy (WORD2VEC BINARY): 0.7259\n"]}],"source":["# Evaluate the model on the test set\n","model.eval()\n","with torch.no_grad():\n","    outputs = model(X_test_tensor_binary)\n","    _, predicted = torch.max(outputs, 1)\n","    accuracy = (predicted == y_test_tensor_binary).sum().item() / len(y_test_tensor_binary)\n","    print(\"Accuracy (WORD2VEC BINARY):\", accuracy)"]},{"cell_type":"markdown","metadata":{"id":"i4zmeIjsbzSs"},"source":["#### **WORD2VEC TERTIARY**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X8rSkWJebzSs"},"outputs":[],"source":["# Convert the Word2Vec features to PyTorch tensors\n","X_train_tensor = torch.tensor(X_train_word2vec_concat, dtype=torch.float32)\n","y_train_tensor = torch.tensor(y_train.values - 1, dtype=torch.long)\n","X_test_tensor = torch.tensor(X_test_word2vec_concat, dtype=torch.float32)\n","y_test_tensor = torch.tensor(y_test.values - 1, dtype=torch.long)\n","\n","# Define hyperparameters\n","input_size = X_train_tensor.shape[1]\n","hidden_size1 = 50\n","hidden_size2 = 10\n","output_size = 3  # Tertiary classification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CYJJSH7_bzSt","outputId":"df380387-0b9a-4246-bf99-3ca9140c953b"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([200000])\n","torch.Size([200000, 300])\n"]}],"source":["print(y_train_tensor.shape)\n","print(X_train_tensor.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bvZkNXPabzSt"},"outputs":[],"source":["# Create DataLoader for training\n","train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","\n","# Initialize the model, loss function, and optimizer\n","model = MLP(input_size, hidden_size1, hidden_size2, output_size)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XkQ5XpcFbzSt","outputId":"6a345753-844b-4452-f9f5-c971db1d3150"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 1.0911445617675781\n","Epoch 2, Loss: 1.0901182889938354\n","Epoch 3, Loss: 1.0887724161148071\n","Epoch 4, Loss: 1.0871578454971313\n","Epoch 5, Loss: 1.0853055715560913\n","Epoch 6, Loss: 1.083208680152893\n","Epoch 7, Loss: 1.0808581113815308\n","Epoch 8, Loss: 1.0782570838928223\n","Epoch 9, Loss: 1.0754159688949585\n","Epoch 10, Loss: 1.0723522901535034\n","Epoch 11, Loss: 1.0690921545028687\n","Epoch 12, Loss: 1.065659999847412\n","Epoch 13, Loss: 1.0620765686035156\n","Epoch 14, Loss: 1.0583627223968506\n","Epoch 15, Loss: 1.0545334815979004\n","Epoch 16, Loss: 1.0506020784378052\n","Epoch 17, Loss: 1.0465855598449707\n","Epoch 18, Loss: 1.0424928665161133\n","Epoch 19, Loss: 1.0383297204971313\n","Epoch 20, Loss: 1.0341094732284546\n","Epoch 21, Loss: 1.029843807220459\n","Epoch 22, Loss: 1.025551438331604\n","Epoch 23, Loss: 1.0212509632110596\n","Epoch 24, Loss: 1.0169700384140015\n","Epoch 25, Loss: 1.0127335786819458\n","Epoch 26, Loss: 1.008561372756958\n","Epoch 27, Loss: 1.004474401473999\n","Epoch 28, Loss: 1.000489592552185\n","Epoch 29, Loss: 0.9966208338737488\n","Epoch 30, Loss: 0.9928826689720154\n","Epoch 31, Loss: 0.9892885684967041\n","Epoch 32, Loss: 0.9858514070510864\n","Epoch 33, Loss: 0.9825777411460876\n","Epoch 34, Loss: 0.9794723987579346\n","Epoch 35, Loss: 0.9765387773513794\n","Epoch 36, Loss: 0.9737770557403564\n","Epoch 37, Loss: 0.971183180809021\n","Epoch 38, Loss: 0.9687512516975403\n","Epoch 39, Loss: 0.966474711894989\n","Epoch 40, Loss: 0.9643478989601135\n","Epoch 41, Loss: 0.96236252784729\n","Epoch 42, Loss: 0.960506796836853\n","Epoch 43, Loss: 0.958770215511322\n","Epoch 44, Loss: 0.9571413397789001\n","Epoch 45, Loss: 0.9556089043617249\n","Epoch 46, Loss: 0.9541624784469604\n","Epoch 47, Loss: 0.9527939558029175\n","Epoch 48, Loss: 0.951496422290802\n","Epoch 49, Loss: 0.9502634406089783\n","Epoch 50, Loss: 0.9490907788276672\n","Epoch 51, Loss: 0.9479753971099854\n","Epoch 52, Loss: 0.9469134211540222\n","Epoch 53, Loss: 0.9459021687507629\n","Epoch 54, Loss: 0.9449389576911926\n","Epoch 55, Loss: 0.9440220594406128\n","Epoch 56, Loss: 0.9431495070457458\n","Epoch 57, Loss: 0.94231778383255\n","Epoch 58, Loss: 0.9415258765220642\n","Epoch 59, Loss: 0.9407719373703003\n","Epoch 60, Loss: 0.9400538802146912\n","Epoch 61, Loss: 0.9393709301948547\n","Epoch 62, Loss: 0.9387214779853821\n","Epoch 63, Loss: 0.9381038546562195\n","Epoch 64, Loss: 0.9375169277191162\n","Epoch 65, Loss: 0.9369591474533081\n","Epoch 66, Loss: 0.9364258646965027\n","Epoch 67, Loss: 0.9359143972396851\n","Epoch 68, Loss: 0.9354228377342224\n","Epoch 69, Loss: 0.934948205947876\n","Epoch 70, Loss: 0.9344879984855652\n","Epoch 71, Loss: 0.9340405464172363\n","Epoch 72, Loss: 0.9336036443710327\n","Epoch 73, Loss: 0.9331758618354797\n","Epoch 74, Loss: 0.932755708694458\n","Epoch 75, Loss: 0.9323418736457825\n","Epoch 76, Loss: 0.9319337010383606\n","Epoch 77, Loss: 0.9315304756164551\n","Epoch 78, Loss: 0.9311326742172241\n","Epoch 79, Loss: 0.930739164352417\n","Epoch 80, Loss: 0.9303507804870605\n","Epoch 81, Loss: 0.9299678206443787\n","Epoch 82, Loss: 0.9295898675918579\n","Epoch 83, Loss: 0.9292165637016296\n","Epoch 84, Loss: 0.9288485646247864\n","Epoch 85, Loss: 0.9284852147102356\n","Epoch 86, Loss: 0.9281245470046997\n","Epoch 87, Loss: 0.9277648329734802\n","Epoch 88, Loss: 0.927405834197998\n","Epoch 89, Loss: 0.9270472526550293\n","Epoch 90, Loss: 0.9266864657402039\n","Epoch 91, Loss: 0.9263196587562561\n","Epoch 92, Loss: 0.9259469509124756\n","Epoch 93, Loss: 0.9255675077438354\n","Epoch 94, Loss: 0.9251808524131775\n","Epoch 95, Loss: 0.9247881174087524\n","Epoch 96, Loss: 0.9243878722190857\n","Epoch 97, Loss: 0.9239804148674011\n","Epoch 98, Loss: 0.9235717058181763\n","Epoch 99, Loss: 0.9231617450714111\n","Epoch 100, Loss: 0.9227511882781982\n"]}],"source":["#Training Loop\n","num_epochs = 100\n","for epoch in range(num_epochs):\n","    model.train()\n","    optimizer.zero_grad()\n","    outputs = model(X_train_tensor)\n","    loss = criterion(outputs, y_train_tensor)\n","    loss.backward()\n","    optimizer.step()\n","    print(f\"Epoch {epoch+1}, Loss: {loss}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s4O2OfIvbzSt","outputId":"b8e0314d-3ce3-412c-e939-55e711120bfd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy (WORD2VEC TERTIARY): 0.61114\n"]}],"source":["# Evaluate the model on the test set\n","model.eval()\n","with torch.no_grad():\n","    outputs = model(X_test_tensor)\n","    _, predicted = torch.max(outputs, 1)\n","    accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)\n","    print(\"Accuracy (WORD2VEC TERTIARY):\", accuracy)"]},{"cell_type":"markdown","source":["#### **WORD2VEC TERTIARY**"],"metadata":{"id":"RG0ge3XHcc_s"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UXw7ksFTbzSu"},"outputs":[],"source":["# Convert the Word2Vec features to PyTorch tensors\n","X_train_tensor = torch.tensor(X_train_pretrained_concat, dtype=torch.float32)\n","y_train_tensor = torch.tensor(y_train.values - 1, dtype=torch.long)\n","X_test_tensor = torch.tensor(X_test_pretrained_concat, dtype=torch.float32)\n","y_test_tensor = torch.tensor(y_test.values - 1, dtype=torch.long)\n","\n","# Define hyperparameters\n","input_size = X_train_tensor.shape[1]\n","hidden_size1 = 50\n","hidden_size2 = 10\n","output_size = 3  # Tertiary classification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8oRaXXR6bzSu"},"outputs":[],"source":["# Create DataLoader for training\n","train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","\n","# Initialize the model, loss function, and optimizer\n","model = MLP(input_size, hidden_size1, hidden_size2, output_size)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PXdc1Xn7bzSu","outputId":"5c4c529d-69ce-4dba-9deb-1eb225379ca6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 1.1017768383026123\n","Epoch 2, Loss: 1.1008857488632202\n","Epoch 3, Loss: 1.1000282764434814\n","Epoch 4, Loss: 1.0991886854171753\n","Epoch 5, Loss: 1.0984901189804077\n","Epoch 6, Loss: 1.0979965925216675\n","Epoch 7, Loss: 1.0976122617721558\n","Epoch 8, Loss: 1.0972579717636108\n","Epoch 9, Loss: 1.096907377243042\n","Epoch 10, Loss: 1.096556544303894\n","Epoch 11, Loss: 1.0961978435516357\n","Epoch 12, Loss: 1.0958251953125\n","Epoch 13, Loss: 1.095436692237854\n","Epoch 14, Loss: 1.0950311422348022\n","Epoch 15, Loss: 1.0946085453033447\n","Epoch 16, Loss: 1.0941686630249023\n","Epoch 17, Loss: 1.0937100648880005\n","Epoch 18, Loss: 1.0932316780090332\n","Epoch 19, Loss: 1.0927289724349976\n","Epoch 20, Loss: 1.0921945571899414\n","Epoch 21, Loss: 1.091617465019226\n","Epoch 22, Loss: 1.0909919738769531\n","Epoch 23, Loss: 1.0903249979019165\n","Epoch 24, Loss: 1.0896223783493042\n","Epoch 25, Loss: 1.0888864994049072\n","Epoch 26, Loss: 1.0881174802780151\n","Epoch 27, Loss: 1.0873159170150757\n","Epoch 28, Loss: 1.0864819288253784\n","Epoch 29, Loss: 1.0856133699417114\n","Epoch 30, Loss: 1.0847066640853882\n","Epoch 31, Loss: 1.0837582349777222\n","Epoch 32, Loss: 1.0827637910842896\n","Epoch 33, Loss: 1.0817205905914307\n","Epoch 34, Loss: 1.0806289911270142\n","Epoch 35, Loss: 1.0794981718063354\n","Epoch 36, Loss: 1.0783394575119019\n","Epoch 37, Loss: 1.0771586894989014\n","Epoch 38, Loss: 1.075958251953125\n","Epoch 39, Loss: 1.0747413635253906\n","Epoch 40, Loss: 1.0735119581222534\n","Epoch 41, Loss: 1.0722734928131104\n","Epoch 42, Loss: 1.071028470993042\n","Epoch 43, Loss: 1.069778561592102\n","Epoch 44, Loss: 1.0685250759124756\n","Epoch 45, Loss: 1.067268967628479\n","Epoch 46, Loss: 1.066011667251587\n","Epoch 47, Loss: 1.0647531747817993\n","Epoch 48, Loss: 1.063494086265564\n","Epoch 49, Loss: 1.06223464012146\n","Epoch 50, Loss: 1.060976505279541\n","Epoch 51, Loss: 1.059721827507019\n","Epoch 52, Loss: 1.0584746599197388\n","Epoch 53, Loss: 1.0572389364242554\n","Epoch 54, Loss: 1.0560179948806763\n","Epoch 55, Loss: 1.054814338684082\n","Epoch 56, Loss: 1.0536298751831055\n","Epoch 57, Loss: 1.0524656772613525\n","Epoch 58, Loss: 1.0513213872909546\n","Epoch 59, Loss: 1.0501960515975952\n","Epoch 60, Loss: 1.0490866899490356\n","Epoch 61, Loss: 1.0479897260665894\n","Epoch 62, Loss: 1.0468997955322266\n","Epoch 63, Loss: 1.0458120107650757\n","Epoch 64, Loss: 1.0447213649749756\n","Epoch 65, Loss: 1.0436230897903442\n","Epoch 66, Loss: 1.0425137281417847\n","Epoch 67, Loss: 1.0413883924484253\n","Epoch 68, Loss: 1.0402448177337646\n","Epoch 69, Loss: 1.0390803813934326\n","Epoch 70, Loss: 1.0378936529159546\n","Epoch 71, Loss: 1.0366871356964111\n","Epoch 72, Loss: 1.0354678630828857\n","Epoch 73, Loss: 1.034244179725647\n","Epoch 74, Loss: 1.0330179929733276\n","Epoch 75, Loss: 1.0317845344543457\n","Epoch 76, Loss: 1.0305367708206177\n","Epoch 77, Loss: 1.0292725563049316\n","Epoch 78, Loss: 1.0279940366744995\n","Epoch 79, Loss: 1.0267062187194824\n","Epoch 80, Loss: 1.0254132747650146\n","Epoch 81, Loss: 1.0241161584854126\n","Epoch 82, Loss: 1.0228123664855957\n","Epoch 83, Loss: 1.021501898765564\n","Epoch 84, Loss: 1.020186424255371\n","Epoch 85, Loss: 1.0188673734664917\n","Epoch 86, Loss: 1.017545223236084\n","Epoch 87, Loss: 1.0162205696105957\n","Epoch 88, Loss: 1.0148934125900269\n","Epoch 89, Loss: 1.0135656595230103\n","Epoch 90, Loss: 1.0122400522232056\n","Epoch 91, Loss: 1.0109195709228516\n","Epoch 92, Loss: 1.0096052885055542\n","Epoch 93, Loss: 1.0082976818084717\n","Epoch 94, Loss: 1.00699782371521\n","Epoch 95, Loss: 1.005707859992981\n","Epoch 96, Loss: 1.0044291019439697\n","Epoch 97, Loss: 1.0031623840332031\n","Epoch 98, Loss: 1.0019077062606812\n","Epoch 99, Loss: 1.000665307044983\n","Epoch 100, Loss: 0.9994359612464905\n"]}],"source":["#Training Loop\n","num_epochs = 100\n","for epoch in range(num_epochs):\n","    model.train()\n","    optimizer.zero_grad()\n","    outputs = model(X_train_tensor)\n","    loss = criterion(outputs, y_train_tensor)\n","    loss.backward()\n","    optimizer.step()\n","    print(f\"Epoch {epoch+1}, Loss: {loss}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QsTRXlcfbzSu","outputId":"f9daf8a9-2478-4374-8303-04ca6bbca196"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy (PRE-TRAINED TERTIARY): 0.55782\n"]}],"source":["# Evaluate the model on the test set\n","model.eval()\n","with torch.no_grad():\n","    outputs = model(X_test_tensor)\n","    _, predicted = torch.max(outputs, 1)\n","    accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)\n","    print(\"Accuracy (PRE-TRAINED TERTIARY):\", accuracy)"]},{"cell_type":"markdown","metadata":{"id":"MgpCZss6bzSu"},"source":["# **Convolutional Neural Networks**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LsP6ZWZGbzSv"},"outputs":[],"source":["import torch.nn.functional as F\n","\n","class CNN(nn.Module):\n","    def __init__(self, num_h1_nodes: int, num_h2_nodes: int, d: int, num_output_classes: int, dropout_rate: float):\n","        super().__init__()\n","        # self.conv1 = nn.Conv2d(3, 6, 5)\n","        # self.pool = nn.MaxPool2d(2, 2)\n","        # self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.fc1 = nn.Linear(d, num_h1_nodes)\n","        self.fc2 = nn.Linear(num_h1_nodes, num_h2_nodes)\n","        self.fc3 = nn.Linear(num_h2_nodes, num_output_classes)\n","\n","    def forward(self, x):\n","        # x = self.pool(F.relu(self.conv1(x)))\n","        # x = self.pool(F.relu(self.conv2(x)))\n","        # x = torch.flatten(x, 1) # flatten all dimensions except batch\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"CUhcdh4xbzSv"},"source":["##### **WORD2VEC BINARY**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2XlR-WJUbzSv"},"outputs":[],"source":["# Convert the Word2Vec features to PyTorch tensors\n","X_train_tensor_binary = torch.tensor(X_train_word2vec_binary, dtype=torch.float32)\n","y_train_tensor_binary = torch.tensor(y_train_binary.values-1, dtype=torch.long)  # Assuming y_train_binary contains binary labels\n","X_test_tensor_binary = torch.tensor(X_test_word2vec_binary, dtype=torch.float32)\n","y_test_tensor_binary = torch.tensor(y_test_binary.values-1, dtype=torch.long)  # Convert Series to numpy array with .values attribute\n","\n","dataset = TensorDataset(X_train_tensor_binary, y_train_tensor_binary)\n","\n","# how many samples per batch to load\n","batch_size = 64\n","\n","num_workers = 0\n","\n","train_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n","\n","d_binary_embeddings = X_train_tensor_binary.shape[1]\n","\n","# Define hyperparameters\n","num_h1_nodes = 50\n","num_h2_nodes = 10\n","num_output_classes = 2\n","dropout_rate = 0.2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ORy8LtJNbzSv","outputId":"406a30d4-3324-492c-ffe9-83f7d0ef880b"},"outputs":[{"data":{"text/plain":["300"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["X_train_tensor_binary.shape[1]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R3vp0ZaObzSv"},"outputs":[],"source":["# Create the model and set it to training\n","model = CNN(num_h1_nodes, num_h2_nodes, d_binary_embeddings, num_output_classes, dropout_rate)\n","\n","# model = model.train()\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GjEQLe17bzSv","outputId":"a0e57de2-84bd-4108-a0ac-ac072235aeae"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1,   200] loss: 0.6830409\n","[1,   400] loss: 0.6467744\n","[1,   600] loss: 0.5842589\n","[1,   800] loss: 0.5120663\n","[1,  1000] loss: 0.4535369\n","[1,  1200] loss: 0.4243788\n","[1,  1400] loss: 0.3942891\n","[1,  1600] loss: 0.3839101\n","[1,  1800] loss: 0.3802211\n","[1,  2000] loss: 0.3696721\n","[1,  2200] loss: 0.3613170\n","[1,  2400] loss: 0.3487234\n","[2,   200] loss: 0.3491343\n","[2,   400] loss: 0.3435925\n","[2,   600] loss: 0.3453553\n","[2,   800] loss: 0.3452436\n","[2,  1000] loss: 0.3421762\n","[2,  1200] loss: 0.3426039\n","[2,  1400] loss: 0.3299195\n","[2,  1600] loss: 0.3347692\n","[2,  1800] loss: 0.3360429\n","[2,  2000] loss: 0.3321724\n","[2,  2200] loss: 0.3313562\n","[2,  2400] loss: 0.3227181\n","[3,   200] loss: 0.3263650\n","[3,   400] loss: 0.3235906\n","[3,   600] loss: 0.3280013\n","[3,   800] loss: 0.3279585\n","[3,  1000] loss: 0.3274927\n","[3,  1200] loss: 0.3281851\n","[3,  1400] loss: 0.3161734\n","[3,  1600] loss: 0.3222112\n","[3,  1800] loss: 0.3229213\n","[3,  2000] loss: 0.3209815\n","[3,  2200] loss: 0.3201460\n","[3,  2400] loss: 0.3124270\n","[4,   200] loss: 0.3168920\n","[4,   400] loss: 0.3136664\n","[4,   600] loss: 0.3185523\n","[4,   800] loss: 0.3182088\n","[4,  1000] loss: 0.3194547\n","[4,  1200] loss: 0.3199488\n","[4,  1400] loss: 0.3079251\n","[4,  1600] loss: 0.3146292\n","[4,  1800] loss: 0.3150833\n","[4,  2000] loss: 0.3145139\n","[4,  2200] loss: 0.3130599\n","[4,  2400] loss: 0.3061859\n","[5,   200] loss: 0.3107608\n","[5,   400] loss: 0.3067291\n","[5,   600] loss: 0.3119380\n","[5,   800] loss: 0.3116004\n","[5,  1000] loss: 0.3139869\n","[5,  1200] loss: 0.3141363\n","[5,  1400] loss: 0.3021380\n","[5,  1600] loss: 0.3094531\n","[5,  1800] loss: 0.3091441\n","[5,  2000] loss: 0.3097059\n","[5,  2200] loss: 0.3081868\n","[5,  2400] loss: 0.3012272\n","[6,   200] loss: 0.3059318\n","[6,   400] loss: 0.3017557\n","[6,   600] loss: 0.3065696\n","[6,   800] loss: 0.3065118\n","[6,  1000] loss: 0.3096939\n","[6,  1200] loss: 0.3092760\n","[6,  1400] loss: 0.2974321\n","[6,  1600] loss: 0.3051570\n","[6,  1800] loss: 0.3047017\n","[6,  2000] loss: 0.3056322\n","[6,  2200] loss: 0.3037332\n","[6,  2400] loss: 0.2968455\n","[7,   200] loss: 0.3017975\n","[7,   400] loss: 0.2975888\n","[7,   600] loss: 0.3018311\n","[7,   800] loss: 0.3021769\n","[7,  1000] loss: 0.3060279\n","[7,  1200] loss: 0.3048744\n","[7,  1400] loss: 0.2932943\n","[7,  1600] loss: 0.3008811\n","[7,  1800] loss: 0.3006874\n","[7,  2000] loss: 0.3020231\n","[7,  2200] loss: 0.2998445\n","[7,  2400] loss: 0.2926444\n","[8,   200] loss: 0.2979206\n","[8,   400] loss: 0.2935893\n","[8,   600] loss: 0.2978932\n","[8,   800] loss: 0.2983199\n","[8,  1000] loss: 0.3023889\n","[8,  1200] loss: 0.3010621\n","[8,  1400] loss: 0.2894898\n","[8,  1600] loss: 0.2970960\n","[8,  1800] loss: 0.2969225\n","[8,  2000] loss: 0.2988339\n","[8,  2200] loss: 0.2960632\n","[8,  2400] loss: 0.2890000\n","[9,   200] loss: 0.2947331\n","[9,   400] loss: 0.2900732\n","[9,   600] loss: 0.2942762\n","[9,   800] loss: 0.2947349\n","[9,  1000] loss: 0.2991519\n","[9,  1200] loss: 0.2973192\n","[9,  1400] loss: 0.2862053\n","[9,  1600] loss: 0.2938305\n","[9,  1800] loss: 0.2935226\n","[9,  2000] loss: 0.2959128\n","[9,  2200] loss: 0.2927670\n","[9,  2400] loss: 0.2856996\n","[10,   200] loss: 0.2917401\n","[10,   400] loss: 0.2868591\n","[10,   600] loss: 0.2909639\n","[10,   800] loss: 0.2916758\n","[10,  1000] loss: 0.2963125\n","[10,  1200] loss: 0.2940025\n","[10,  1400] loss: 0.2832342\n","[10,  1600] loss: 0.2908018\n","[10,  1800] loss: 0.2904218\n","[10,  2000] loss: 0.2931029\n","[10,  2200] loss: 0.2896784\n","[10,  2400] loss: 0.2827733\n","[11,   200] loss: 0.2890272\n","[11,   400] loss: 0.2841127\n","[11,   600] loss: 0.2880173\n","[11,   800] loss: 0.2887020\n","[11,  1000] loss: 0.2936153\n","[11,  1200] loss: 0.2911674\n","[11,  1400] loss: 0.2806487\n","[11,  1600] loss: 0.2882730\n","[11,  1800] loss: 0.2877328\n","[11,  2000] loss: 0.2906504\n","[11,  2200] loss: 0.2868835\n","[11,  2400] loss: 0.2802183\n","[12,   200] loss: 0.2864236\n","[12,   400] loss: 0.2817496\n","[12,   600] loss: 0.2855537\n","[12,   800] loss: 0.2861106\n","[12,  1000] loss: 0.2910465\n","[12,  1200] loss: 0.2884719\n","[12,  1400] loss: 0.2783428\n","[12,  1600] loss: 0.2861812\n","[12,  1800] loss: 0.2854065\n","[12,  2000] loss: 0.2883486\n","[12,  2200] loss: 0.2841317\n","[12,  2400] loss: 0.2780230\n","[13,   200] loss: 0.2842419\n","[13,   400] loss: 0.2795953\n","[13,   600] loss: 0.2835606\n","[13,   800] loss: 0.2838251\n","[13,  1000] loss: 0.2889745\n","[13,  1200] loss: 0.2860922\n","[13,  1400] loss: 0.2760631\n","[13,  1600] loss: 0.2841468\n","[13,  1800] loss: 0.2831796\n","[13,  2000] loss: 0.2862975\n","[13,  2200] loss: 0.2817644\n","[13,  2400] loss: 0.2759954\n","[14,   200] loss: 0.2821421\n","[14,   400] loss: 0.2775896\n","[14,   600] loss: 0.2816450\n","[14,   800] loss: 0.2815925\n","[14,  1000] loss: 0.2867833\n","[14,  1200] loss: 0.2837377\n","[14,  1400] loss: 0.2738969\n","[14,  1600] loss: 0.2821874\n","[14,  1800] loss: 0.2812178\n","[14,  2000] loss: 0.2845148\n","[14,  2200] loss: 0.2794995\n","[14,  2400] loss: 0.2742046\n","[15,   200] loss: 0.2801744\n","[15,   400] loss: 0.2757708\n","[15,   600] loss: 0.2796229\n","[15,   800] loss: 0.2794732\n","[15,  1000] loss: 0.2850029\n","[15,  1200] loss: 0.2814354\n","[15,  1400] loss: 0.2718316\n","[15,  1600] loss: 0.2805907\n","[15,  1800] loss: 0.2794081\n","[15,  2000] loss: 0.2826294\n","[15,  2200] loss: 0.2772901\n","[15,  2400] loss: 0.2723250\n","[16,   200] loss: 0.2783571\n","[16,   400] loss: 0.2743077\n","[16,   600] loss: 0.2777536\n","[16,   800] loss: 0.2778517\n","[16,  1000] loss: 0.2832954\n","[16,  1200] loss: 0.2794160\n","[16,  1400] loss: 0.2699676\n","[16,  1600] loss: 0.2791182\n","[16,  1800] loss: 0.2779433\n","[16,  2000] loss: 0.2809337\n","[16,  2200] loss: 0.2752051\n","[16,  2400] loss: 0.2704885\n","[17,   200] loss: 0.2765747\n","[17,   400] loss: 0.2726392\n","[17,   600] loss: 0.2760645\n","[17,   800] loss: 0.2759434\n","[17,  1000] loss: 0.2815153\n","[17,  1200] loss: 0.2776168\n","[17,  1400] loss: 0.2682641\n","[17,  1600] loss: 0.2775882\n","[17,  1800] loss: 0.2765451\n","[17,  2000] loss: 0.2794279\n","[17,  2200] loss: 0.2734601\n","[17,  2400] loss: 0.2687941\n","[18,   200] loss: 0.2751094\n","[18,   400] loss: 0.2713181\n","[18,   600] loss: 0.2743415\n","[18,   800] loss: 0.2741921\n","[18,  1000] loss: 0.2797751\n","[18,  1200] loss: 0.2756858\n","[18,  1400] loss: 0.2664752\n","[18,  1600] loss: 0.2760916\n","[18,  1800] loss: 0.2750713\n","[18,  2000] loss: 0.2778108\n","[18,  2200] loss: 0.2717425\n","[18,  2400] loss: 0.2673298\n","[19,   200] loss: 0.2735266\n","[19,   400] loss: 0.2701538\n","[19,   600] loss: 0.2726544\n","[19,   800] loss: 0.2727191\n","[19,  1000] loss: 0.2784025\n","[19,  1200] loss: 0.2739215\n","[19,  1400] loss: 0.2648205\n","[19,  1600] loss: 0.2747153\n","[19,  1800] loss: 0.2737143\n","[19,  2000] loss: 0.2762609\n","[19,  2200] loss: 0.2699605\n","[19,  2400] loss: 0.2657632\n","[20,   200] loss: 0.2721375\n","[20,   400] loss: 0.2689717\n","[20,   600] loss: 0.2712075\n","[20,   800] loss: 0.2710722\n","[20,  1000] loss: 0.2766880\n","[20,  1200] loss: 0.2723517\n","[20,  1400] loss: 0.2632494\n","[20,  1600] loss: 0.2736950\n","[20,  1800] loss: 0.2723708\n","[20,  2000] loss: 0.2747570\n","[20,  2200] loss: 0.2684793\n","[20,  2400] loss: 0.2643554\n"]}],"source":["# Training loop\n","for epoch in range(20):  # Adjust number of epochs\n","    running_loss = 0.0\n","    for i, data in enumerate(train_loader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data\n","\n","        # zero the parameter gradients\n","        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        if i % 200 == 199:    # print every 200 mini-batches\n","            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.7f}')\n","            running_loss = 0.0\n","    # print(f\"Binary Epoch {epoch+1}, Loss: {running_loss / 2000:.7f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UB5umpC0bzSw","outputId":"e3c45c36-7932-4ac4-ba94-6853504af860"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy (WORD2VEC BINARY): 0.881675\n"]}],"source":["# Evaluate the model on the test set\n","model.eval()\n","with torch.no_grad():\n","    outputs = model(X_test_tensor_binary)\n","    _, predicted = torch.max(outputs, 1)\n","    accuracy = (predicted == y_test_tensor_binary).sum().item() / len(y_test_tensor_binary)\n","    print(\"Accuracy (WORD2VEC BINARY):\", accuracy)"]},{"cell_type":"markdown","metadata":{"id":"Eh4xtdYzbzSw"},"source":["##### **WORD2VEC TERTIARY**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4AV5gFxrbzSw"},"outputs":[],"source":["# Convert the Word2Vec features to PyTorch tensors\n","X_train_tensor = torch.tensor(X_train_word2vec, dtype=torch.float32)\n","y_train_tensor = torch.tensor(y_train.values-1, dtype=torch.long)  # Assuming y_train_binary contains binary labels\n","X_test_tensor = torch.tensor(X_test_word2vec, dtype=torch.float32)\n","y_test_tensor = torch.tensor(y_test_binary.values-1, dtype=torch.long)  # Convert Series to numpy array with .values attribute\n","\n","dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","\n","# how many samples per batch to load\n","batch_size = 64\n","\n","num_workers = 0\n","\n","train_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n","\n","d_embeddings = X_train_tensor.shape[1]\n","\n","# Define hyperparameters\n","num_h1_nodes = 50\n","num_h2_nodes = 10\n","num_output_classes = 3\n","dropout_rate = 0.2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZRvWXgQybzSw"},"outputs":[],"source":["# Create the model and set it to training\n","model = CNN(num_h1_nodes, num_h2_nodes, d_binary_embeddings, num_output_classes, dropout_rate)\n","\n","# model = model.train()\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0OhYoyWIbzSw","outputId":"328cf413-11a7-4469-9a0f-4fe83afa4058"},"outputs":[{"name":"stdout","output_type":"stream","text":["Binary Epoch 1, Loss: 0.0467122\n","Binary Epoch 2, Loss: 0.0439033\n","Binary Epoch 3, Loss: 0.0427430\n","Binary Epoch 4, Loss: 0.0420776\n","Binary Epoch 5, Loss: 0.0416304\n","Binary Epoch 6, Loss: 0.0412907\n","Binary Epoch 7, Loss: 0.0410043\n","Binary Epoch 8, Loss: 0.0407908\n","Binary Epoch 9, Loss: 0.0405953\n","Binary Epoch 10, Loss: 0.0404205\n"]}],"source":["# Training loop\n","for epoch in range(10):  # Adjust number of epochs\n","    running_loss = 0.0\n","    for i, data in enumerate(train_loader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data\n","\n","        # zero the parameter gradients\n","        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        if i % 200 == 199:    # print every 200 mini-batches\n","        #     print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.7f}')\n","            running_loss = 0.0\n","    print(f\"Binary Epoch {epoch+1}, Loss: {running_loss / 2000:.7f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VsDvPiS4bzSx","outputId":"a5da1d48-a26e-462c-a827-9f23d3304a00"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy (WORD2VEC BINARY): 0.8243\n"]}],"source":["# Evaluate the model on the test set\n","model.eval()\n","with torch.no_grad():\n","    outputs = model(X_test_tensor_binary)\n","    _, predicted = torch.max(outputs, 1)\n","    accuracy = (predicted == y_test_tensor_binary).sum().item() / len(y_test_tensor_binary)\n","    print(\"Accuracy (WORD2VEC BINARY):\", accuracy)"]},{"cell_type":"markdown","metadata":{"id":"au1OEie5bzSx"},"source":["##### **PRE-TRAINED BINARY**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SHOS6ldkbzSx"},"outputs":[],"source":["# Convert the Word2Vec features to PyTorch tensors\n","X_train_tensor_binary = torch.tensor(X_train_pretrained_binary, dtype=torch.float32)\n","y_train_tensor_binary = torch.tensor(y_train_binary.values-1, dtype=torch.long)  # Assuming y_train_binary contains binary labels\n","X_test_tensor_binary = torch.tensor(X_test_pretrained_binary, dtype=torch.float32)\n","y_test_tensor_binary = torch.tensor(y_test_binary.values-1, dtype=torch.long)  # Convert Series to numpy array with .values attribute\n","\n","dataset = TensorDataset(X_train_tensor_binary, y_train_tensor_binary)\n","\n","# how many samples per batch to load\n","batch_size = 64\n","\n","num_workers = 0\n","\n","train_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n","\n","d_binary_embeddings = X_train_tensor_binary.shape[1]\n","\n","# Define hyperparameters\n","num_h1_nodes = 50\n","num_h2_nodes = 10\n","num_output_classes = 2\n","dropout_rate = 0.2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m0EP1Tz2bzSx"},"outputs":[],"source":["# Create the model and set it to training\n","model = CNN(num_h1_nodes, num_h2_nodes, d_binary_embeddings, num_output_classes, dropout_rate)\n","\n","# model = model.train()\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cupUTORLbzSx","outputId":"d51bc635-fb5c-47c8-feb7-e6fd456f00a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Binary Epoch 1, Loss: 0.0306439\n","Binary Epoch 2, Loss: 0.0233038\n","Binary Epoch 3, Loss: 0.0210494\n","Binary Epoch 4, Loss: 0.0199838\n","Binary Epoch 5, Loss: 0.0193276\n","Binary Epoch 6, Loss: 0.0189169\n","Binary Epoch 7, Loss: 0.0186409\n","Binary Epoch 8, Loss: 0.0184492\n","Binary Epoch 9, Loss: 0.0183016\n","Binary Epoch 10, Loss: 0.0181784\n","Binary Epoch 11, Loss: 0.0180851\n","Binary Epoch 12, Loss: 0.0180102\n","Binary Epoch 13, Loss: 0.0179401\n","Binary Epoch 14, Loss: 0.0178757\n","Binary Epoch 15, Loss: 0.0178080\n","Binary Epoch 16, Loss: 0.0177409\n","Binary Epoch 17, Loss: 0.0176814\n","Binary Epoch 18, Loss: 0.0176245\n","Binary Epoch 19, Loss: 0.0175601\n","Binary Epoch 20, Loss: 0.0174817\n"]}],"source":["# Training loop\n","for epoch in range(20):  # Adjust number of epochs\n","    running_loss = 0.0\n","    for i, data in enumerate(train_loader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data\n","\n","        # zero the parameter gradients\n","        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        if i % 200 == 199:    # print every 200 mini-batches\n","        #     print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.7f}')\n","            running_loss = 0.0\n","    print(f\"Binary Epoch {epoch+1}, Loss: {running_loss / 2000:.7f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xCfLpEX7bzSy","outputId":"d167ee3d-29e1-442a-c216-91005604eb9c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy (WORD2VEC BINARY): 0.8424\n"]}],"source":["# Evaluate the model on the test set\n","model.eval()\n","with torch.no_grad():\n","    outputs = model(X_test_tensor_binary)\n","    _, predicted = torch.max(outputs, 1)\n","    accuracy = (predicted == y_test_tensor_binary).sum().item() / len(y_test_tensor_binary)\n","    print(\"Accuracy (WORD2VEC BINARY):\", accuracy)"]},{"cell_type":"markdown","metadata":{"id":"zhnxjwODbzSy"},"source":["##### **PRE-TRAINED TERTIARY**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZJ4_J4lVbzSy"},"outputs":[],"source":["# Convert the Word2Vec features to PyTorch tensors\n","X_train_tensor = torch.tensor(X_train_pretrained, dtype=torch.float32)\n","y_train_tensor = torch.tensor(y_train.values-1, dtype=torch.long)  # Assuming y_train_binary contains binary labels\n","X_test_tensor = torch.tensor(X_test_pretrained, dtype=torch.float32)\n","y_test_tensor = torch.tensor(y_test_binary.values-1, dtype=torch.long)  # Convert Series to numpy array with .values attribute\n","\n","dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","\n","# how many samples per batch to load\n","batch_size = 64\n","\n","num_workers = 0\n","\n","train_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n","\n","d_embeddings = X_train_tensor.shape[1]\n","\n","# Define hyperparameters\n","num_h1_nodes = 50\n","num_h2_nodes = 10\n","num_output_classes = 3\n","dropout_rate = 0.2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gZvNaXp9bzSy"},"outputs":[],"source":["# Create the model and set it to training\n","model = CNN(num_h1_nodes, num_h2_nodes, d_binary_embeddings, num_output_classes, dropout_rate)\n","\n","# model = model.train()\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"deo6mi69bzSz","outputId":"2fc45482-4e2e-45d3-8528-4f4a22a4f53a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Binary Epoch 1, Loss: 0.0658773\n","Binary Epoch 2, Loss: 0.0657772\n","Binary Epoch 3, Loss: 0.0651549\n","Binary Epoch 4, Loss: 0.0605878\n","Binary Epoch 5, Loss: 0.0534047\n","Binary Epoch 6, Loss: 0.0509879\n","Binary Epoch 7, Loss: 0.0498970\n","Binary Epoch 8, Loss: 0.0492079\n","Binary Epoch 9, Loss: 0.0486316\n","Binary Epoch 10, Loss: 0.0482243\n"]}],"source":["# Training loop\n","for epoch in range(10):  # Adjust number of epochs\n","    running_loss = 0.0\n","    for i, data in enumerate(train_loader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data\n","\n","        # zero the parameter gradients\n","        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        if i % 200 == 199:    # print every 200 mini-batches\n","        #     print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.7f}')\n","            running_loss = 0.0\n","    print(f\"Binary Epoch {epoch+1}, Loss: {running_loss / 2000:.7f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R5_fRQxUbzSz","outputId":"20069d4a-280c-49c5-fb0d-acce4a7065b1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy (PRE-TRAINED TERTIARY): 0.833475\n"]}],"source":["# Evaluate the model on the test set\n","model.eval()\n","with torch.no_grad():\n","    outputs = model(X_test_tensor_binary)\n","    _, predicted = torch.max(outputs, 1)\n","    accuracy = (predicted == y_test_tensor_binary).sum().item() / len(y_test_tensor_binary)\n","    print(\"Accuracy (PRE-TRAINED TERTIARY):\", accuracy)"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":0}