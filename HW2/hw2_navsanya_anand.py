# -*- coding: utf-8 -*-
"""HW2_NavSanya_Anand.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1My8vGbruhDEqEr3CgqMRjonbyFWHwWBG
"""

# ! pip install gensim

# from google.colab import drive
# drive.mount('/content/drive')

import pandas as pd
import numpy as np
import nltk
import re
from bs4 import BeautifulSoup
import os
import contractions

# os.chdir('/content/drive/Shared drives/USC_CSCI544-Applied NLP/HWs/HW2') # where the files for this project are

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report
from sklearn.linear_model import Perceptron, LogisticRegression
from sklearn.svm import SVC
from sklearn.svm import LinearSVC
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import gensim
import gensim.downloader as api

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('omw-1.4')
nltk.download('averaged_perceptron_tagger')

"""# **READ DATA**"""

df=pd.read_table('amazon_reviews_us_Office_Products_v1_00.tsv', on_bad_lines='skip')

df = df[['review_body', 'star_rating']]


df['star_rating'] = pd.to_numeric(df['star_rating'], errors='coerce')

df = df.dropna(subset=['review_body'], how='all')

print(df)

"""# **CREATE BALANCED DATASET**"""

def create_balanced_dataset(df, samples_per_rating=50000):
    balanced_df = pd.DataFrame()
    for rating in range(1, 6):
        rating_df = df[df['star_rating'] == rating].sample(samples_per_rating)
        balanced_df = pd.concat([balanced_df, rating_df], ignore_index=True)
    return balanced_df

df = create_balanced_dataset(df)

print(df)

"""# **CREATE TERNARY LABELS**"""

df['sentiment'] = np.where(df['star_rating'] <= 2, 2,  # Negative: 2
                          np.where(df['star_rating'] > 3, 1, 3))  # Positive: 1, Neutral: 3

# df['sentiment'] = np.where(df['star_rating'] > 3, 1, 0)  # Positive: 1, Negative: 0

# Print review counts per class
print("Number of positive reviews:", df[df['sentiment'] == 1].shape[0])
print("Number of negative reviews:", df[df['sentiment'] == 2].shape[0])
print("Number of neutral reviews:", df[df['sentiment'] == 3].shape[0])

print(df)

"""# **CREATE BINARY LABELS**"""

# Data cleaning and preprocessing
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# Data Cleaning
def clean_text(text):
    # Remove HTML tags
    if isinstance(text, str) and text:
      soup = BeautifulSoup(text, 'html.parser')
      text = soup.get_text()

      # Expand contractions
      text = contractions.fix(text)

      # Remove special characters and digits
      text = re.sub(r'[^a-zA-Z\s]', '', text)

      # Remove URLs
      text = re.sub(r'https?://\S+', '', text)

      # Convert to lowercase
      text = text.lower()

      # Tokenize
      words = nltk.word_tokenize(text)

      # Remove stop words
      stop_words = set(nltk.corpus.stopwords.words('english'))
      words = [word for word in words if word not in stop_words]

      # Lemmatize
      lemmatizer = WordNetLemmatizer()
      words = [lemmatizer.lemmatize(word) for word in words]

      # Remove extra spaces
      text = ' '.join(words)
      text = re.sub(r'\s+', ' ', text).strip()  # Replace multiple spaces with single space

      return text
    else:
      return ''  # Return an empty string for empty or non-string inputs

df['clean_review'] = df['review_body'].apply(clean_text)
# Print the average length of the cleaned reviews
print(df)
print("Average length of cleaned reviews(TERTIARY):", df['clean_review'].str.len().mean())

df_binary = df[df['star_rating'] != 3]  # Discard neutral reviews (rating 3)

# Print review counts per class
print("Number of positive reviews:", df_binary[df_binary['sentiment'] == 1].shape[0])
print("Number of negative reviews:", df_binary[df_binary['sentiment'] == 2].shape[0])
print("Number of neutral reviews (discarded):", len(df_binary[df_binary['star_rating'] == 3]))
print(df_binary)
print("Average length of cleaned reviews(BINARY):", df_binary['clean_review'].str.len().mean())

"""# **SPLIT DATASET**"""

X_train, X_test, y_train, y_test = train_test_split(df['review_body'], df['sentiment'], test_size=0.2, random_state=42)
X_train_binary, X_test_binary, y_train_binary, y_test_binary = train_test_split(df_binary['review_body'], df_binary['sentiment'], test_size=0.2, random_state=42)

"""# **WORD EMBEDDING**"""

import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

# Load pretrained model
pretrained_model = api.load('word2vec-google-news-300')

print("PRETRAINED WORD2VEC")
king_minus_man_plus_woman = pretrained_model.most_similar(positive=['king', 'woman'], negative=['man', 'men'], topn=1)
excellent_similar_outstanding = pretrained_model.similarity('excellent', 'outstanding')

print("Semantic similarity between 'king - man + woman' and 'queen':", king_minus_man_plus_woman)
print("Semantic similarity between 'excellent' and 'outstanding':", excellent_similar_outstanding)

# Tokenize the reviews for training the Word2Vec model
tokenized_reviews = [nltk.word_tokenize(review) for review in X_train]
tokenized_reviews_binary = [nltk.word_tokenize(review) for review in X_train_binary]

# Train a Word2Vec model using the tokenized reviews
word2vec_model = gensim.models.Word2Vec(sentences=tokenized_reviews, vector_size=300, window=11, min_count=10)
word2vec_model_binary = gensim.models.Word2Vec(sentences=tokenized_reviews_binary, vector_size=300, window=11, min_count=10)

print("TRAINED WORD2VEC")

# Demonstrate semantic similarities for the trained Word2Vec model
king_minus_man_plus_woman_custom = word2vec_model.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)
excellent_similar_outstanding_custom = word2vec_model.wv.similarity('excellent', 'outstanding')

print("Semantic similarity between 'king - man + woman' and 'queen' (custom):", king_minus_man_plus_woman_custom)
print("Semantic similarity between 'excellent' and 'outstanding' (custom):", excellent_similar_outstanding_custom)

"""# **SIMPLE MODELS**

"""

# Define a function to represent each example with Word2Vec features
def get_word2vec_features(review_text):
    feature_vector = np.zeros((word2vec_model.vector_size,), dtype="float32")
    num_words = 0
    for word in review_text:
        if word in word2vec_model.wv:
            feature_vector = np.add(feature_vector, word2vec_model.wv[word])
            num_words += 1
    if num_words != 0:
        feature_vector = np.divide(feature_vector, num_words)
    return feature_vector

# Represent each example with Word2Vec features
X_train_word2vec = np.array([get_word2vec_features(review) for review in tokenized_reviews])
print("X_train:")
print(X_train)
print('X_train_word2vec:')
print(X_train_word2vec)

X_train_word2vec_binary = np.array([get_word2vec_features(review) for review in tokenized_reviews_binary])
print("X_train_binary:")
print(X_train_binary)
print('X_train_word2vec:')
print(X_train_word2vec_binary)

# Tokenize the reviews for training the Word2Vec model
tokenized_reviews_test = [nltk.word_tokenize(review) for review in X_test]
tokenized_reviews_test_binary = [nltk.word_tokenize(review) for review in X_test_binary]

# Represent each example with Word2Vec features
X_test_word2vec = np.array([get_word2vec_features(review) for review in tokenized_reviews_test])
print("X_test:")
print(X_test)
print('X_test_word2vec:')
print(X_test_word2vec)

X_test_word2vec_binary = np.array([get_word2vec_features(review) for review in tokenized_reviews_test_binary])
print("X_test_binary:")
print(X_test_binary)
print('X_test_word2vec:')
print(X_test_word2vec_binary)

def get_pretrained_features(review_text):
    feature_vector = np.zeros((pretrained_model.vector_size,), dtype="float32")
    num_words = 0
    for word in review_text:
        if word in pretrained_model:
            feature_vector += pretrained_model[word]
            num_words += 1
    if num_words != 0:
        feature_vector /= num_words
    return feature_vector

# Represent each example with Word2Vec features
X_train_pretrained = np.array([get_pretrained_features(review) for review in tokenized_reviews])
print("X_train:")
print(X_train)
print('X_train_pretrained:')
print(X_train_pretrained)

X_train_pretrained_binary = np.array([get_pretrained_features(review) for review in tokenized_reviews_binary])
print("X_train_binary:")
print(X_train_binary)
print('X_train_pretrained_binary:')
print(X_train_pretrained_binary)

# Represent each example with Pretrained features
X_test_pretrained = np.array([get_pretrained_features(review) for review in tokenized_reviews_test])
print("X_test:")
print(X_test)
print('X_test_word2vec:')
print(X_test_pretrained)

X_test_pretrained_binary = np.array([get_pretrained_features(review) for review in tokenized_reviews_test_binary])
print("X_train_binary:")
print(X_test_binary)
print('X_train_word2vec:')
print(X_test_pretrained_binary)

"""#### **TDIDF**"""

tfidf_vectorizer = TfidfVectorizer()
X = tfidf_vectorizer.fit_transform(df_binary['clean_review'])
y = df_binary['sentiment']

# shape of the feature matrix
print("Shape of feature matrix (X):", X.shape)

# Split the data into training and testing sets
X_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned = train_test_split(X, y, test_size=0.2, random_state=42)

perceptron_classifier = Perceptron(max_iter=1000)
perceptron_classifier.fit(X_train_cleaned, y_train_cleaned)
y_pred_perceptron = perceptron_classifier.predict(X_test_cleaned)
print("Perceptron Accuracy (TF-IDF):", accuracy_score(y_test_cleaned, y_pred_perceptron))

svm_classifier = LinearSVC()
svm_classifier.fit(X_train_cleaned, y_train_cleaned)
y_pred_svm = svm_classifier.predict(X_test_cleaned)
print("SVM Accuracy (TF-IDF):", accuracy_score(y_test_cleaned, y_pred_svm))

"""#### **PRE-TRAINED**"""

# Train Perceptron and SVM models using pretrained features
perceptron_classifier_pretrained = Perceptron(max_iter=1000)
perceptron_classifier_pretrained.fit(X_train_pretrained_binary, y_train_binary)
y_pred_perceptron_pretrained = perceptron_classifier_pretrained.predict(X_test_pretrained_binary)
print("Perceptron Accuracy (Word2Vec):", accuracy_score(y_test_binary, y_pred_perceptron_pretrained))

svm_classifier_pretrained =  LinearSVC()
svm_classifier_pretrained.fit(X_train_pretrained_binary, y_train_binary)
y_pred_svm_pretrained = svm_classifier_pretrained.predict(X_test_pretrained_binary)
print("SVM Accuracy (Word2Vec):", accuracy_score(y_test_binary, y_pred_svm_pretrained))

"""#### **WORD2VEC**"""

# Train Perceptron and SVM models using Word2Vec features
perceptron_classifier_word2vec = Perceptron(max_iter=1000)
perceptron_classifier_word2vec.fit(X_train_word2vec_binary, y_train_binary)
y_pred_perceptron_word2vec = perceptron_classifier_word2vec.predict(X_test_word2vec_binary)
print("Perceptron Accuracy (Word2Vec):", accuracy_score(y_test_binary, y_pred_perceptron_word2vec))

svm_classifier_word2vec =  LinearSVC()
svm_classifier_word2vec.fit(X_train_word2vec_binary, y_train_binary)
y_pred_svm_word2vec = svm_classifier_word2vec.predict(X_test_word2vec_binary)
print("SVM Accuracy (Word2Vec):", accuracy_score(y_test_binary, y_pred_svm_word2vec))

"""# **Feedforward Neural Networks**"""

# Define the MLP model
class MLP(nn.Module):
    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size1)
        self.fc2 = nn.Linear(hidden_size1, hidden_size2)
        self.fc3 = nn.Linear(hidden_size2, output_size)
        self.relu = nn.ReLU()
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return self.softmax(x)

"""### **AVG METHOD**

##### **WORD2VEC BINARY**
"""

# Convert the Word2Vec features to PyTorch tensors
X_train_tensor_binary = torch.tensor(X_train_word2vec_binary, dtype=torch.float32)
y_train_tensor_binary = torch.tensor(y_train_binary-1, dtype=torch.long)  # Assuming y_train_binary contains binary labels
X_test_tensor_binary = torch.tensor(X_test_word2vec_binary, dtype=torch.float32)
y_test_tensor_binary = torch.tensor(y_test_binary.values-1, dtype=torch.long)  # Convert Series to numpy array with .values attribute

# Define hyperparameters
input_size = X_train_tensor_binary.shape[1]
hidden_size1 = 50
hidden_size2 = 10
output_size = 2  # Binary classification

print(y_train_tensor_binary)

# Create DataLoader for training
train_dataset_binary = TensorDataset(X_train_tensor_binary, y_train_tensor_binary)
train_loader_binary = DataLoader(train_dataset_binary, batch_size=64, shuffle=True)

# Initialize the model, loss function, and optimizer
model = MLP(input_size, hidden_size1, hidden_size2, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
num_epochs = 100
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_train_tensor_binary)
    loss = criterion(outputs, y_train_tensor_binary)
    loss.backward()
    optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {loss}")

# Evaluate the model on the test set
model.eval()
with torch.no_grad():
    outputs = model(X_test_tensor_binary)
    _, predicted = torch.max(outputs, 1)
    accuracy = (predicted == y_test_tensor_binary).sum().item() / len(y_test_tensor_binary)
    print("Accuracy (WORD2VEC BINARY):", accuracy)

"""#####  **WORD2VEC TERTIARY**"""

# Convert the Word2Vec features to PyTorch tensors
X_train_tensor = torch.tensor(X_train_word2vec, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train-1, dtype=torch.long)  # Assuming y_train_binary contains binary labels
X_test_tensor = torch.tensor(X_test_word2vec, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values-1, dtype=torch.long)  # Convert Series to numpy array with .values attribute

# Define hyperparameters
input_size = X_train_tensor.shape[1]
hidden_size1 = 50
hidden_size2 = 10
output_size = 3  # Tertiary classification

print(y_train_tensor)

# Create DataLoader for training
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

# Initialize the model, loss function, and optimizer
model = MLP(input_size, hidden_size1, hidden_size2, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
num_epochs = 100
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_train_tensor)
    loss = criterion(outputs, y_train_tensor)
    loss.backward()
    optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {loss}")

# Evaluate the model on the test set
model.eval()
with torch.no_grad():
    outputs = model(X_test_tensor)
    _, predicted = torch.max(outputs, 1)
    accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)
    print("Accuracy (WORD2VEC TERTIARY):", accuracy)

"""#####  **PRE_TRAINED BINARY**"""

# Convert the Word2Vec features to PyTorch tensors
X_train_tensor_binary = torch.tensor(X_train_pretrained_binary, dtype=torch.float32)
y_train_tensor_binary = torch.tensor(y_train_binary-1, dtype=torch.long)  # Assuming y_train_binary contains binary labels
X_test_tensor_binary = torch.tensor(X_test_pretrained_binary, dtype=torch.float32)
y_test_tensor_binary = torch.tensor(y_test_binary.values-1, dtype=torch.long)  # Convert Series to numpy array with .values attribute

# Define hyperparameters
input_size = X_train_tensor_binary.shape[1]
hidden_size1 = 50
hidden_size2 = 10
output_size = 2  # Binary classification

print(y_train_tensor_binary)

# Create DataLoader for training
train_dataset_binary = TensorDataset(X_train_tensor_binary, y_train_tensor_binary)
train_loader_binary = DataLoader(train_dataset_binary, batch_size=64, shuffle=True)

# Initialize the model, loss function, and optimizer
model = MLP(input_size, hidden_size1, hidden_size2, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
num_epochs = 100
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_train_tensor_binary)
    loss = criterion(outputs, y_train_tensor_binary)
    loss.backward()
    optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {loss}")

# Evaluate the model on the test set
model.eval()
with torch.no_grad():
    outputs = model(X_test_tensor_binary)
    _, predicted = torch.max(outputs, 1)
    accuracy = (predicted == y_test_tensor_binary).sum().item() / len(y_test_tensor_binary)
    print("Accuracy (WORD2VEC BINARY):", accuracy)

"""#####  **PRE_TRAINED TERTIARY**"""

# Convert the Word2Vec features to PyTorch tensors
X_train_tensor = torch.tensor(X_train_pretrained, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train-1, dtype=torch.long)  # Assuming y_train_binary contains binary labels
X_test_tensor = torch.tensor(X_test_pretrained, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values-1, dtype=torch.long)  # Convert Series to numpy array with .values attribute

# Define hyperparameters
input_size = X_train_tensor.shape[1]
hidden_size1 = 50
hidden_size2 = 10
output_size = 3  # Binary classification

print(X_train_tensor)
print(y_train_tensor)

# Create DataLoader for training
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

# Initialize the model, loss function, and optimizer
model = MLP(input_size, hidden_size1, hidden_size2, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
num_epochs = 100
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_train_tensor)
    loss = criterion(outputs, y_train_tensor)
    loss.backward()
    optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {loss}")

# Evaluate the model on the test set
model.eval()
with torch.no_grad():
    outputs = model(X_test_tensor)
    _, predicted = torch.max(outputs, 1)
    accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)
    print("Accuracy (WORD2VEC TERTIARY):", accuracy)

"""### **CONCAT METHOD**

#### **WORD2VEC BINARY**
"""

x = np.array([X_train_word2vec_binary[:10]])
# x = X_train_word2vec_binary
print(x.shape)

def get_word2vec_features_concat(review_text):
  feature_vector = np.zeros((word2vec_model.vector_size,), dtype="float32")
  num_words = 0
  for i, word in enumerate(review_text):
    if word in word2vec_model.wv and i < 10:  # Limit to first 10 words
      feature_vector = np.add(feature_vector, word2vec_model.wv[word])
      num_words += 1
  if num_words != 0:
    feature_vector = np.divide(feature_vector, num_words)
  return feature_vector

# Represent each example with Word2Vec features
X_train_word2vec_concat = np.array([get_word2vec_features_concat(review) for review in tokenized_reviews])
print("X_train:")
print(X_train)
print('X_train_word2vec_concat:')
print(X_train_word2vec_concat)

X_train_word2vec_concat_binary = np.array([get_word2vec_features_concat(review) for review in tokenized_reviews_binary])
print("X_train_binary:")
print(X_train_binary)
print('X_train_word2vec_concat_binary:')
print(X_train_word2vec_concat_binary)

X_train_word2vec_concat.shape

tokenized_reviews_test = [nltk.word_tokenize(review) for review in X_test]
tokenized_reviews_test_binary = [nltk.word_tokenize(review) for review in X_test_binary]

# Represent each example with Word2Vec features
X_test_word2vec_concat = np.array([get_word2vec_features_concat(review) for review in tokenized_reviews_test])
print("X_test:")
print(X_test)
print('X_test_word2vec_concat:')
print(X_test_word2vec_concat)

X_test_word2vec_concat_binary = np.array([get_word2vec_features_concat(review) for review in tokenized_reviews_test_binary])
print("X_test_binary:")
print(X_test_binary)
print('X_test_word2vec_concat_binary:')
print(X_test_word2vec_concat_binary)

X_test_word2vec_concat_binary.shape

def get_pretrained_features_concat(review_text):
    feature_vector = np.zeros((pretrained_model.vector_size,), dtype="float32")
    num_words = 0
    for i, word in enumerate(review_text):
        if word in pretrained_model and i < 10: # limit to first 10
            feature_vector += pretrained_model[word]
            num_words += 1
    if num_words != 0:
        feature_vector /= num_words
    return feature_vector

# Represent each example with Word2Vec features
X_train_pretrained_concat = np.array([get_pretrained_features_concat(review) for review in tokenized_reviews])
print("X_train:")
print(X_train)
print('X_train_pretrained_concat:')
print(X_train_pretrained_concat)

X_train_pretrained_concat_binary = np.array([get_pretrained_features_concat(review) for review in tokenized_reviews_binary])
print("X_train_binary:")
print(X_train_binary)
print('X_train_pretrained_concat_binary:')
print(X_train_pretrained_concat_binary)

# Represent each example with Pretrained features
X_test_pretrained_concat = np.array([get_pretrained_features_concat(review) for review in tokenized_reviews_test])
print("X_test:")
print(X_test)
print('X_test_pretrained_concat:')
print(X_test_pretrained_concat)

X_test_pretrained_concat_binary = np.array([get_pretrained_features_concat(review) for review in tokenized_reviews_test_binary])
print("X_train_binary:")
print(X_test_binary)
print('X_test_pretrained_concat_binary:')
print(X_test_pretrained_concat_binary)

X_train_pretrained_concat_binary.shape

# Convert the Word2Vec features to PyTorch tensors
X_train_tensor_binary = torch.tensor(X_train_word2vec_concat_binary, dtype=torch.float32)
y_train_tensor_binary = torch.tensor(y_train_binary.values - 1, dtype=torch.long)
X_test_tensor_binary = torch.tensor(X_test_word2vec_concat_binary, dtype=torch.float32)
y_test_tensor_binary = torch.tensor(y_test_binary.values - 1, dtype=torch.long)

# Define hyperparameters
input_size_binary = X_train_tensor_binary.shape[1]
hidden_size1 = 50
hidden_size2 = 10
output_size = 2  # Binary classification

print(y_train_tensor_binary.shape)
print(X_train_tensor_binary.shape)

# Create DataLoader for training
train_dataset_binary = TensorDataset(X_train_tensor_binary, y_train_tensor_binary)
train_loader_binary = DataLoader(train_dataset_binary, batch_size=64, shuffle=True)

# Initialize the model, loss function, and optimizer
model = MLP(input_size_binary, hidden_size1, hidden_size2, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

#Training Loop
num_epochs = 100
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_train_tensor_binary)
    loss = criterion(outputs, y_train_tensor_binary)
    loss.backward()
    optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {loss}")

# Evaluate the model on the test set
model.eval()
with torch.no_grad():
    outputs = model(X_test_tensor_binary)
    _, predicted = torch.max(outputs, 1)
    accuracy = (predicted == y_test_tensor_binary).sum().item() / len(y_test_tensor_binary)
    print("Accuracy (WORD2VEC BINARY):", accuracy)

"""#### **PRE TRAINED BINARY**"""

# Convert the Word2Vec features to PyTorch tensors
X_train_tensor_binary = torch.tensor(X_train_pretrained_concat_binary, dtype=torch.float32)
y_train_tensor_binary = torch.tensor(y_train_binary.values - 1, dtype=torch.long)
X_test_tensor_binary = torch.tensor(X_test_pretrained_concat_binary, dtype=torch.float32)
y_test_tensor_binary = torch.tensor(y_test_binary.values - 1, dtype=torch.long)

# Define hyperparameters
input_size_binary = X_train_tensor_binary.shape[1]
hidden_size1 = 50
hidden_size2 = 10
output_size = 2  # Binary classification

# Create DataLoader for training
train_dataset_binary = TensorDataset(X_train_tensor_binary, y_train_tensor_binary)
train_loader_binary = DataLoader(train_dataset_binary, batch_size=64, shuffle=True)

# Initialize the model, loss function, and optimizer
model = MLP(input_size_binary, hidden_size1, hidden_size2, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

#Training Loop
num_epochs = 100
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_train_tensor_binary)
    loss = criterion(outputs, y_train_tensor_binary)
    loss.backward()
    optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {loss}")

# Evaluate the model on the test set
model.eval()
with torch.no_grad():
    outputs = model(X_test_tensor_binary)
    _, predicted = torch.max(outputs, 1)
    accuracy = (predicted == y_test_tensor_binary).sum().item() / len(y_test_tensor_binary)
    print("Accuracy (WORD2VEC BINARY):", accuracy)

"""#### **WORD2VEC TERTIARY**"""

# Convert the Word2Vec features to PyTorch tensors
X_train_tensor = torch.tensor(X_train_word2vec_concat, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values - 1, dtype=torch.long)
X_test_tensor = torch.tensor(X_test_word2vec_concat, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values - 1, dtype=torch.long)

# Define hyperparameters
input_size = X_train_tensor.shape[1]
hidden_size1 = 50
hidden_size2 = 10
output_size = 3  # Tertiary classification

print(y_train_tensor.shape)
print(X_train_tensor.shape)

# Create DataLoader for training
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

# Initialize the model, loss function, and optimizer
model = MLP(input_size, hidden_size1, hidden_size2, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

#Training Loop
num_epochs = 100
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_train_tensor)
    loss = criterion(outputs, y_train_tensor)
    loss.backward()
    optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {loss}")

# Evaluate the model on the test set
model.eval()
with torch.no_grad():
    outputs = model(X_test_tensor)
    _, predicted = torch.max(outputs, 1)
    accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)
    print("Accuracy (WORD2VEC TERTIARY):", accuracy)

"""#### **WORD2VEC TERTIARY**"""

# Convert the Word2Vec features to PyTorch tensors
X_train_tensor = torch.tensor(X_train_pretrained_concat, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values - 1, dtype=torch.long)
X_test_tensor = torch.tensor(X_test_pretrained_concat, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values - 1, dtype=torch.long)

# Define hyperparameters
input_size = X_train_tensor.shape[1]
hidden_size1 = 50
hidden_size2 = 10
output_size = 3  # Tertiary classification

# Create DataLoader for training
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

# Initialize the model, loss function, and optimizer
model = MLP(input_size, hidden_size1, hidden_size2, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

#Training Loop
num_epochs = 100
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_train_tensor)
    loss = criterion(outputs, y_train_tensor)
    loss.backward()
    optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {loss}")

# Evaluate the model on the test set
model.eval()
with torch.no_grad():
    outputs = model(X_test_tensor)
    _, predicted = torch.max(outputs, 1)
    accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)
    print("Accuracy (PRE-TRAINED TERTIARY):", accuracy)

"""# **Convolutional Neural Networks**"""

import torch.nn.functional as F

class CNN(nn.Module):
    def __init__(self, num_h1_nodes: int, num_h2_nodes: int, d: int, num_output_classes: int, dropout_rate: float):
        super().__init__()
        # self.conv1 = nn.Conv2d(3, 6, 5)
        # self.pool = nn.MaxPool2d(2, 2)
        # self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(d, num_h1_nodes)
        self.fc2 = nn.Linear(num_h1_nodes, num_h2_nodes)
        self.fc3 = nn.Linear(num_h2_nodes, num_output_classes)

    def forward(self, x):
        # x = self.pool(F.relu(self.conv1(x)))
        # x = self.pool(F.relu(self.conv2(x)))
        # x = torch.flatten(x, 1) # flatten all dimensions except batch
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

"""##### **WORD2VEC BINARY**"""

# Convert the Word2Vec features to PyTorch tensors
X_train_tensor_binary = torch.tensor(X_train_word2vec_binary, dtype=torch.float32)
y_train_tensor_binary = torch.tensor(y_train_binary.values-1, dtype=torch.long)  # Assuming y_train_binary contains binary labels
X_test_tensor_binary = torch.tensor(X_test_word2vec_binary, dtype=torch.float32)
y_test_tensor_binary = torch.tensor(y_test_binary.values-1, dtype=torch.long)  # Convert Series to numpy array with .values attribute

dataset = TensorDataset(X_train_tensor_binary, y_train_tensor_binary)

# how many samples per batch to load
batch_size = 64

num_workers = 0

train_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)

d_binary_embeddings = X_train_tensor_binary.shape[1]

# Define hyperparameters
num_h1_nodes = 50
num_h2_nodes = 10
num_output_classes = 2
dropout_rate = 0.2

X_train_tensor_binary.shape[1]

# Create the model and set it to training
model = CNN(num_h1_nodes, num_h2_nodes, d_binary_embeddings, num_output_classes, dropout_rate)

# model = model.train()

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

# Training loop
for epoch in range(20):  # Adjust number of epochs
    running_loss = 0.0
    for i, data in enumerate(train_loader, 0):
        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data

        # zero the parameter gradients
        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 200 == 199:    # print every 200 mini-batches
            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.7f}')
            running_loss = 0.0
    # print(f"Binary Epoch {epoch+1}, Loss: {running_loss / 2000:.7f}")

# Evaluate the model on the test set
model.eval()
with torch.no_grad():
    outputs = model(X_test_tensor_binary)
    _, predicted = torch.max(outputs, 1)
    accuracy = (predicted == y_test_tensor_binary).sum().item() / len(y_test_tensor_binary)
    print("Accuracy (WORD2VEC BINARY):", accuracy)

"""##### **WORD2VEC TERTIARY**"""

# Convert the Word2Vec features to PyTorch tensors
X_train_tensor = torch.tensor(X_train_word2vec, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values-1, dtype=torch.long)  # Assuming y_train_binary contains binary labels
X_test_tensor = torch.tensor(X_test_word2vec, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test_binary.values-1, dtype=torch.long)  # Convert Series to numpy array with .values attribute

dataset = TensorDataset(X_train_tensor, y_train_tensor)

# how many samples per batch to load
batch_size = 64

num_workers = 0

train_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)

d_embeddings = X_train_tensor.shape[1]

# Define hyperparameters
num_h1_nodes = 50
num_h2_nodes = 10
num_output_classes = 3
dropout_rate = 0.2

# Create the model and set it to training
model = CNN(num_h1_nodes, num_h2_nodes, d_binary_embeddings, num_output_classes, dropout_rate)

# model = model.train()

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

# Training loop
for epoch in range(10):  # Adjust number of epochs
    running_loss = 0.0
    for i, data in enumerate(train_loader, 0):
        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data

        # zero the parameter gradients
        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 200 == 199:    # print every 200 mini-batches
        #     print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.7f}')
            running_loss = 0.0
    print(f"Binary Epoch {epoch+1}, Loss: {running_loss / 2000:.7f}")

# Evaluate the model on the test set
model.eval()
with torch.no_grad():
    outputs = model(X_test_tensor_binary)
    _, predicted = torch.max(outputs, 1)
    accuracy = (predicted == y_test_tensor_binary).sum().item() / len(y_test_tensor_binary)
    print("Accuracy (WORD2VEC BINARY):", accuracy)

"""##### **PRE-TRAINED BINARY**"""

# Convert the Word2Vec features to PyTorch tensors
X_train_tensor_binary = torch.tensor(X_train_pretrained_binary, dtype=torch.float32)
y_train_tensor_binary = torch.tensor(y_train_binary.values-1, dtype=torch.long)  # Assuming y_train_binary contains binary labels
X_test_tensor_binary = torch.tensor(X_test_pretrained_binary, dtype=torch.float32)
y_test_tensor_binary = torch.tensor(y_test_binary.values-1, dtype=torch.long)  # Convert Series to numpy array with .values attribute

dataset = TensorDataset(X_train_tensor_binary, y_train_tensor_binary)

# how many samples per batch to load
batch_size = 64

num_workers = 0

train_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)

d_binary_embeddings = X_train_tensor_binary.shape[1]

# Define hyperparameters
num_h1_nodes = 50
num_h2_nodes = 10
num_output_classes = 2
dropout_rate = 0.2

# Create the model and set it to training
model = CNN(num_h1_nodes, num_h2_nodes, d_binary_embeddings, num_output_classes, dropout_rate)

# model = model.train()

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

# Training loop
for epoch in range(20):  # Adjust number of epochs
    running_loss = 0.0
    for i, data in enumerate(train_loader, 0):
        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data

        # zero the parameter gradients
        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 200 == 199:    # print every 200 mini-batches
        #     print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.7f}')
            running_loss = 0.0
    print(f"Binary Epoch {epoch+1}, Loss: {running_loss / 2000:.7f}")

# Evaluate the model on the test set
model.eval()
with torch.no_grad():
    outputs = model(X_test_tensor_binary)
    _, predicted = torch.max(outputs, 1)
    accuracy = (predicted == y_test_tensor_binary).sum().item() / len(y_test_tensor_binary)
    print("Accuracy (WORD2VEC BINARY):", accuracy)

"""##### **PRE-TRAINED TERTIARY**"""

# Convert the Word2Vec features to PyTorch tensors
X_train_tensor = torch.tensor(X_train_pretrained, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values-1, dtype=torch.long)  # Assuming y_train_binary contains binary labels
X_test_tensor = torch.tensor(X_test_pretrained, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test_binary.values-1, dtype=torch.long)  # Convert Series to numpy array with .values attribute

dataset = TensorDataset(X_train_tensor, y_train_tensor)

# how many samples per batch to load
batch_size = 64

num_workers = 0

train_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)

d_embeddings = X_train_tensor.shape[1]

# Define hyperparameters
num_h1_nodes = 50
num_h2_nodes = 10
num_output_classes = 3
dropout_rate = 0.2

# Create the model and set it to training
model = CNN(num_h1_nodes, num_h2_nodes, d_binary_embeddings, num_output_classes, dropout_rate)

# model = model.train()

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

# Training loop
for epoch in range(10):  # Adjust number of epochs
    running_loss = 0.0
    for i, data in enumerate(train_loader, 0):
        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data

        # zero the parameter gradients
        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 200 == 199:    # print every 200 mini-batches
        #     print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.7f}')
            running_loss = 0.0
    print(f"Binary Epoch {epoch+1}, Loss: {running_loss / 2000:.7f}")

# Evaluate the model on the test set
model.eval()
with torch.no_grad():
    outputs = model(X_test_tensor_binary)
    _, predicted = torch.max(outputs, 1)
    accuracy = (predicted == y_test_tensor_binary).sum().item() / len(y_test_tensor_binary)
    print("Accuracy (PRE-TRAINED TERTIARY):", accuracy)