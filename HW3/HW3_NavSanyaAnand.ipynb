{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1281,"status":"ok","timestamp":1708584887149,"user":{"displayName":"Nav Sanya Anand","userId":"02554823300121068065"},"user_tz":480},"id":"MxK6kFQ7g326","outputId":"a8f3e420-f36e-4088-d530-b133588fc918"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"_CRXN9K6gqy3"},"outputs":[],"source":["\n","from collections import defaultdict\n","import operator\n","import copy\n","import json\n","import toolz\n","import os\n","import subprocess\n","# os.chdir('/content/drive/Shared drives/USC_CSCI544-Applied NLP/HWs/HW3') # where the files for this project are\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Kjt4SHfCgqy-"},"outputs":[],"source":["# Open and read the training data\n","with open(\"./data/train\", \"r\") as f:\n","    count_dict = defaultdict(int)\n","    for line in f:\n","        get_words = line.split()\n","        if len(get_words) != 0:\n","            count_dict[get_words[1]] += 1\n","\n","# Count the number of rare words (words occurring less than 2 times)\n","unkw = sum(val for val in count_dict.values() if val < 2)\n","\n","# Sort the count dictionary by values in descending order\n","sorted_count_list = sorted(count_dict.items(), key=operator.itemgetter(1), reverse=True)"]},{"cell_type":"markdown","metadata":{"id":"W6CPJhsHgqy_"},"source":["# **TASK 1**"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"YSknihutgqzC"},"outputs":[],"source":["# Open the vocab.txt file for writing\n","with open(\"vocab.txt\", \"w\") as f:\n","    # Write the '<unk>' token line\n","    f.write('<unk>\\t0\\t' + str(unkw) + '\\n')\n","\n","    # Initialize variables\n","    i = 1\n","    vocab_count = 0\n","    vocab_list = []\n","\n","    # Iterate through sorted_count_list and write lines to the file\n","    for word, count in sorted_count_list:\n","        if count >= 2:\n","            vocab_count += 1\n","            vocab_list.append(word)\n","            f.write(word + '\\t' + str(i) + '\\t' + str(count) + '\\n')\n","            i += 1\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1708584888313,"user":{"displayName":"Nav Sanya Anand","userId":"02554823300121068065"},"user_tz":480},"id":"d0-goUsIgqzD","outputId":"56be52cd-9ca6-4aee-f845-8c64cf919eea"},"outputs":[{"name":"stdout","output_type":"stream","text":["The total size of the vocabulary is 23182.\n","This is excluding the '<unk>' token.\n","The total occurences of the special token '<unk>' after replacement is 20011.\n"]}],"source":["print(\"The total size of the vocabulary is \"+str(vocab_count)+\".\")\n","print(\"This is excluding the '<unk>' token.\")\n","print(\"The total occurences of the special token '<unk>' after replacement is \"+str(unkw)+\".\")"]},{"cell_type":"markdown","metadata":{"id":"dXAsvBdagqzG"},"source":["# **TASK 2**"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"9x4Xil4lgqzG"},"outputs":[],"source":["# Initialize defaultdicts and variables\n","s_counts = defaultdict(int)\n","e_counts = defaultdict(int)\n","t_counts = defaultdict(int)\n","prev_s = \"start\"\n","s_counts[\"start\"] += 1\n","\n","# Open and read the training data\n","with open(\"./data/train\", \"r\") as f:\n","    for line in f:\n","        get_indiv = line.split()\n","        if len(get_indiv) != 0:\n","            # Update transition counts\n","            t_counts[(prev_s, get_indiv[2])] += 1\n","\n","            # Update emission counts\n","            if get_indiv[1] in vocab_list:\n","                e_counts[(get_indiv[2], get_indiv[1])] += 1\n","            else:\n","                e_counts[(get_indiv[2], '<unk>')] += 1\n","\n","            # Update state counts\n","            s_counts[get_indiv[2]] += 1\n","            prev_s = get_indiv[2]\n","        else:\n","            # Reset state counts at the end of each sentence\n","            prev_s = \"start\"\n","            s_counts[\"start\"] += 1\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"D06LFUfXgqzH"},"outputs":[],"source":["# Calculate transition probabilities\n","transition = defaultdict(int)\n","for key, val in t_counts.items():\n","    transition[key] = t_counts[key] / s_counts[key[0]]\n","\n","# Calculate emission probabilities\n","emission = defaultdict(int)\n","for key, val in e_counts.items():\n","    emission[key] = e_counts[key] / s_counts[key[0]]\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42,"status":"ok","timestamp":1708584936120,"user":{"displayName":"Nav Sanya Anand","userId":"02554823300121068065"},"user_tz":480},"id":"bxnpYsA_gqzI","outputId":"53faa3f3-5e4f-4e16-aa0f-30aad3717314"},"outputs":[{"name":"stdout","output_type":"stream","text":["The number of transition parameters in HMM: 1392\n","The number of emission parameters in HMM: 30303\n"]}],"source":["print(\"The number of transition parameters in HMM:\",str(len(transition.keys())))\n","print(\"The number of emission parameters in HMM:\",str(len(emission.keys())))"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"2xRFM677gqzK"},"outputs":[],"source":["tags = copy.deepcopy(list(s_counts.keys()))\n","tags.remove('start') #this is done because start is not an actual tag. It was only taken in s counts to help compute probabilities when a word was at the start of the sentence - prior prob\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"1lWJ6VUHgqzL"},"outputs":[],"source":["def tup_to_str(x):\n","    return str(x)\n","\n","transition_json = copy.deepcopy(transition)\n","emission_json = copy.deepcopy(emission)\n","transition_json = toolz.keymap(tup_to_str, transition_json)\n","emission_json = toolz.keymap(tup_to_str, emission_json)\n","total_dict = {'transition':transition_json, 'emission':emission_json}\n","with open(\"hmm.json\",\"w\") as output_file:\n","    json.dump(total_dict, output_file, indent=4)"]},{"cell_type":"markdown","metadata":{"id":"py14bIRsgqzJ"},"source":["# **TASK 3**"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"IH6jf6B8gqzM"},"outputs":[],"source":["def greedyDecoding(data):\n","    # Check if the data file path is provided\n","    if data:\n","        # Initialize the previous tag as \"start\" since the first line of the file starts with a sentence\n","        prev_tag = \"start\"\n","\n","        # Open the input data file for reading\n","        with open(data, \"r\") as f_test:\n","            # Open the output file for writing the decoded tags\n","            with open(\"greedy.out\", \"w\") as model_out:\n","                i = 1  # Counter for line numbers\n","                # Iterate through each line in the input data file\n","                for line in f_test:\n","                    get_indiv = line.split()  # Split the line into individual components\n","                    # Check if the line is not empty\n","                    if len(get_indiv) > 0:\n","                        max_pred_tag = [-1, None]  # Initialize the maximum predicted tag probability\n","                        # Iterate through each possible state (tag)\n","                        for state in tags:\n","                            # Get the emission probability for the word given the state (tag)\n","                            if get_indiv[1] in vocab_list:\n","                                em_prob = emission[(state, get_indiv[1])]\n","                            else:\n","                                em_prob = emission[(state, '<unk>')]\n","                            # Get the transition probability from the previous tag to the current state (tag)\n","                            trans_prob = transition[(prev_tag, state)]\n","                            # Calculate the combined probability\n","                            prob = em_prob * trans_prob\n","                            # Update the maximum predicted tag and its probability if a higher probability is found\n","                            if prob > max_pred_tag[0]:\n","                                max_pred_tag = [prob, state]\n","                        prev_tag = max_pred_tag[1]  # Update the previous tag with the maximum predicted tag\n","                        # Write the line number, word, and predicted tag to the output file\n","                        model_out.write(str(i) + \"\\t\" + get_indiv[1] + \"\\t\" + max_pred_tag[1] + \"\\n\")\n","                        i += 1  # Increment the line number counter\n","                    else:\n","                        prev_tag = \"start\"  # Reset the previous tag to \"start\" at the end of each sentence\n","                        model_out.write(\"\\n\")  # Write an empty line for sentence separation\n","                        i = 1  # Reset the line number counter at the beginning of a new sentence\n","\n","        # Close the input and output files\n","        f_test.close()\n","        model_out.close()\n","        print(\"greedy.out file created in data folder.\")  # Print a message indicating file creation\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":317678,"status":"ok","timestamp":1708585962930,"user":{"displayName":"Nav Sanya Anand","userId":"02554823300121068065"},"user_tz":480},"id":"LteYz8ScgqzN","outputId":"7ae3b43c-b573-4f6c-a19c-45b87c02884d"},"outputs":[{"name":"stdout","output_type":"stream","text":["greedy.out file created in data folder.\n"]}],"source":["greedyDecoding('data/dev')\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":311,"status":"ok","timestamp":1708585963228,"user":{"displayName":"Nav Sanya Anand","userId":"02554823300121068065"},"user_tz":480},"id":"fft_pldggqzO","outputId":"c3d1197e-37ac-4985-c488-112c743dbe66"},"outputs":[{"name":"stdout","output_type":"stream","text":["STDOUT:\n","\n","STDERR:\n","python: can't open file 'c:\\\\Users\\\\navsa\\\\OneDrive\\\\Desktop\\\\CSCI544_AppliedNLP\\\\HW3\\\\eval.py': [Errno 2] No such file or directory\n","\n"]}],"source":["command = \"python eval.py -p greedy.out -g data/dev\"\n","\n","# Run the command and capture the output\n","completed_process = subprocess.run(command, shell=True, capture_output=True, text=True)\n","\n","# Print the output\n","print(\"STDOUT:\")\n","print(completed_process.stdout)\n","\n","# Print the error if there is one\n","if completed_process.stderr:\n","    print(\"STDERR:\")\n","    print(completed_process.stderr)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":281313,"status":"ok","timestamp":1708586244534,"user":{"displayName":"Nav Sanya Anand","userId":"02554823300121068065"},"user_tz":480},"id":"t_Sx_aWVgqzO","outputId":"3ce91c30-bd5d-4fec-f90e-c49bdf52b658"},"outputs":[{"name":"stdout","output_type":"stream","text":["greedy.out file created in data folder.\n"]}],"source":["greedyDecoding('data/test')"]},{"cell_type":"markdown","metadata":{"id":"fbYh7fzxgqzP"},"source":["# **TASK 4**"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"Fo1kafqmgqzP"},"outputs":[],"source":["def viterbiDecoding(data):\n","    if data:\n","        f = open(data, 'r')\n","        predicted_tags = []\n","        prev_tag=\"start\" #since the first line of file starts with a sentence, we need to mention \"start\" so prior probability is computed.\n","        for line in f:\n","            get_indiv = line.split()\n","            if len(get_indiv)>0:\n","                if prev_tag==\"start\":\n","                    viterbi=[]\n","                    first_dict = {}\n","                    for state in tags:\n","                        if get_indiv[1] in vocab_list:\n","                            em_prob = emission[(state,get_indiv[1])]\n","                        else:\n","                            em_prob = emission[(state,'<unk>')]\n","                        trans_prob = transition[(prev_tag,state)]\n","                        prob = em_prob*trans_prob\n","                        first_dict[state] = (prob, prev_tag)\n","                    viterbi.append(copy.deepcopy(first_dict))\n","                    prev_tag=\"not start\"\n","                else:\n","                    curr_dict = {}\n","                    for state in tags:\n","                        if get_indiv[1] in vocab_list:\n","                            em_prob = emission[(state,get_indiv[1])]\n","                        else:\n","                            em_prob = emission[(state,'<unk>')]\n","                        max_state_prob = [-1,None]\n","                        for prev_state_key, prev_state_val in viterbi[-1].items():\n","                            trans_prob = transition[(prev_state_key,state)]\n","                            prev_state_prob_val = prev_state_val[0]\n","                            final_prob = em_prob*trans_prob*prev_state_prob_val\n","                            if final_prob>max_state_prob[0]:\n","                                max_state_prob = [final_prob, prev_state_key]\n","                        curr_dict[state] = (max_state_prob[0], max_state_prob[1])\n","                    viterbi.append(copy.deepcopy(curr_dict))\n","            else:\n","                preds = []\n","                max_val = max(viterbi[len(viterbi)-1].values(), key = lambda x: x[0])\n","                preds.append(next(key for key,val in viterbi[len(viterbi)-1].items() if val==max_val))\n","                prev_state=max_val[1]\n","                for i in range(len(viterbi)-2, -1, -1):\n","                    preds.append(prev_state)\n","                    prev_state = viterbi[i][prev_state][1]\n","                preds.reverse()\n","                predicted_tags.extend(preds)\n","                prev_tag = \"start\"\n","        f.close()\n","        preds = []\n","        max_val = max(viterbi[len(viterbi)-1].values(), key = lambda x: x[0])\n","        preds.append(next(key for key,val in viterbi[len(viterbi)-1].items() if val==max_val))\n","        prev_state=max_val[1]\n","        for i in range(len(viterbi)-2, -1, -1):\n","            preds.append(prev_state)\n","            prev_state = viterbi[i][prev_state][1]\n","        preds.reverse()\n","        predicted_tags.extend(preds)\n","\n","\n","    model_out = open('viterbi.out','w')\n","    f = open(data,'r')\n","    i=0\n","    for line in f:\n","        get_indiv = line.split()\n","        if len(get_indiv)>0:\n","            model_out.write(str(get_indiv[0])+\"\\t\"+get_indiv[1]+\"\\t\"+predicted_tags[i]+\"\\n\")\n","            i+=1\n","        else:\n","            model_out.write(\"\\n\")\n","    f.close()\n","    model_out.close()\n","    print(\"viterbi.out file created in data folder.\")\n","\n","\n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":483747,"status":"ok","timestamp":1708587271375,"user":{"displayName":"Nav Sanya Anand","userId":"02554823300121068065"},"user_tz":480},"id":"5ZaoRPCogqzQ","outputId":"075dcd3f-b75e-4730-f083-6ebc1afb1b83"},"outputs":[{"name":"stdout","output_type":"stream","text":["viterbi.out file created in data folder.\n"]}],"source":["viterbiDecoding('data/dev')"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":445,"status":"ok","timestamp":1708587271814,"user":{"displayName":"Nav Sanya Anand","userId":"02554823300121068065"},"user_tz":480},"id":"wxSJedCVgqzQ","outputId":"52ce551a-f062-4ef5-e5cf-4a68b3226ba7"},"outputs":[{"name":"stdout","output_type":"stream","text":["STDOUT:\n","\n","STDERR:\n","python: can't open file 'c:\\\\Users\\\\navsa\\\\OneDrive\\\\Desktop\\\\CSCI544_AppliedNLP\\\\HW3\\\\eval.py': [Errno 2] No such file or directory\n","\n"]}],"source":["command = \"python eval.py -p viterbi.out -g data/dev\"\n","\n","# Run the command and capture the output\n","completed_process = subprocess.run(command, shell=True, capture_output=True, text=True)\n","\n","# Print the output\n","print(\"STDOUT:\")\n","print(completed_process.stdout)\n","\n","# Print the error if there is one\n","if completed_process.stderr:\n","    print(\"STDERR:\")\n","    print(completed_process.stderr)"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":430258,"status":"ok","timestamp":1708589330439,"user":{"displayName":"Nav Sanya Anand","userId":"02554823300121068065"},"user_tz":480},"id":"viJXRY98gqzR","outputId":"b9e79147-5779-444f-a96e-b74d136b130d"},"outputs":[{"name":"stdout","output_type":"stream","text":["viterbi.out file created in data folder.\n"]}],"source":["viterbiDecoding('data/test')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":0}
